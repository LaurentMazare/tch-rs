/* THIS FILE IS AUTOMATICALLY GENERATED, DO NOT EDIT BY HAND! */
#[allow(clippy::all)]
use crate::{Device, Kind, Scalar, Tensor};
use std::borrow::Borrow;
use std::convert::Into;

impl Tensor {
    pub fn internal_and_<S: Into<Scalar>>(&mut self, other: S) -> Tensor {
        self.f_internal_and_(other).unwrap()
    }

    pub fn internal_and_1(&mut self, other: &Tensor) -> Tensor {
        self.f_internal_and_1(other).unwrap()
    }

    pub fn internal_iand_<S: Into<Scalar>>(&mut self, other: S) -> Tensor {
        self.f_internal_iand_(other).unwrap()
    }

    pub fn internal_iand_1(&mut self, other: &Tensor) -> Tensor {
        self.f_internal_iand_1(other).unwrap()
    }

    pub fn internal_ilshift_<S: Into<Scalar>>(&mut self, other: S) -> Tensor {
        self.f_internal_ilshift_(other).unwrap()
    }

    pub fn internal_ilshift_1(&mut self, other: &Tensor) -> Tensor {
        self.f_internal_ilshift_1(other).unwrap()
    }

    pub fn internal_ior_<S: Into<Scalar>>(&mut self, other: S) -> Tensor {
        self.f_internal_ior_(other).unwrap()
    }

    pub fn internal_ior_1(&mut self, other: &Tensor) -> Tensor {
        self.f_internal_ior_1(other).unwrap()
    }

    pub fn internal_irshift_<S: Into<Scalar>>(&mut self, other: S) -> Tensor {
        self.f_internal_irshift_(other).unwrap()
    }

    pub fn internal_irshift_1(&mut self, other: &Tensor) -> Tensor {
        self.f_internal_irshift_1(other).unwrap()
    }

    pub fn internal_ixor_<S: Into<Scalar>>(&mut self, other: S) -> Tensor {
        self.f_internal_ixor_(other).unwrap()
    }

    pub fn internal_ixor_1(&mut self, other: &Tensor) -> Tensor {
        self.f_internal_ixor_1(other).unwrap()
    }

    pub fn internal_lshift_<S: Into<Scalar>>(&mut self, other: S) -> Tensor {
        self.f_internal_lshift_(other).unwrap()
    }

    pub fn internal_lshift_1(&mut self, other: &Tensor) -> Tensor {
        self.f_internal_lshift_1(other).unwrap()
    }

    pub fn internal_or_<S: Into<Scalar>>(&mut self, other: S) -> Tensor {
        self.f_internal_or_(other).unwrap()
    }

    pub fn internal_or_1(&mut self, other: &Tensor) -> Tensor {
        self.f_internal_or_1(other).unwrap()
    }

    pub fn internal_rshift_<S: Into<Scalar>>(&mut self, other: S) -> Tensor {
        self.f_internal_rshift_(other).unwrap()
    }

    pub fn internal_rshift_1(&mut self, other: &Tensor) -> Tensor {
        self.f_internal_rshift_1(other).unwrap()
    }

    pub fn internal_xor_<S: Into<Scalar>>(&mut self, other: S) -> Tensor {
        self.f_internal_xor_(other).unwrap()
    }

    pub fn internal_xor_1(&mut self, other: &Tensor) -> Tensor {
        self.f_internal_xor_1(other).unwrap()
    }

    pub fn internal_adaptive_avg_pool2d(&self, output_size: &[i64]) -> Tensor {
        self.f_internal_adaptive_avg_pool2d(output_size).unwrap()
    }

    pub fn internal_adaptive_avg_pool2d_backward(&self, grad_output: &Tensor) -> Tensor {
        self.f_internal_adaptive_avg_pool2d_backward(grad_output)
            .unwrap()
    }

    pub fn internal_addr(&self, vec1: &Tensor, vec2: &Tensor) -> Tensor {
        self.f_internal_addr(vec1, vec2).unwrap()
    }

    pub fn internal_addr_(&mut self, vec1: &Tensor, vec2: &Tensor) -> Tensor {
        self.f_internal_addr_(vec1, vec2).unwrap()
    }

    pub fn internal_addr_out(&self, out: &Tensor, vec1: &Tensor, vec2: &Tensor) -> Tensor {
        self.f_internal_addr_out(out, vec1, vec2).unwrap()
    }

    pub fn internal_baddbmm_mkl_(&mut self, batch1: &Tensor, batch2: &Tensor) -> Tensor {
        self.f_internal_baddbmm_mkl_(batch1, batch2).unwrap()
    }

    pub fn internal_cast_byte(&self, non_blocking: bool) -> Tensor {
        self.f_internal_cast_byte(non_blocking).unwrap()
    }

    pub fn internal_cast_char(&self, non_blocking: bool) -> Tensor {
        self.f_internal_cast_char(non_blocking).unwrap()
    }

    pub fn internal_cast_double(&self, non_blocking: bool) -> Tensor {
        self.f_internal_cast_double(non_blocking).unwrap()
    }

    pub fn internal_cast_float(&self, non_blocking: bool) -> Tensor {
        self.f_internal_cast_float(non_blocking).unwrap()
    }

    pub fn internal_cast_half(&self, non_blocking: bool) -> Tensor {
        self.f_internal_cast_half(non_blocking).unwrap()
    }

    pub fn internal_cast_int(&self, non_blocking: bool) -> Tensor {
        self.f_internal_cast_int(non_blocking).unwrap()
    }

    pub fn internal_cast_long(&self, non_blocking: bool) -> Tensor {
        self.f_internal_cast_long(non_blocking).unwrap()
    }

    pub fn internal_cast_short(&self, non_blocking: bool) -> Tensor {
        self.f_internal_cast_short(non_blocking).unwrap()
    }

    pub fn internal_cat<T: Borrow<Tensor>>(tensors: &[T], dim: i64) -> Tensor {
        Tensor::f_internal_cat(tensors, dim).unwrap()
    }

    pub fn internal_cat_out<T: Borrow<Tensor>>(out: &Tensor, tensors: &[T], dim: i64) -> Tensor {
        Tensor::f_internal_cat_out(out, tensors, dim).unwrap()
    }

    pub fn internal_cdist_backward(
        grad: &Tensor,
        x1: &Tensor,
        x2: &Tensor,
        p: f64,
        cdist: &Tensor,
    ) -> Tensor {
        Tensor::f_internal_cdist_backward(grad, x1, x2, p, cdist).unwrap()
    }

    pub fn internal_cholesky_helper(&self, upper: bool) -> Tensor {
        self.f_internal_cholesky_helper(upper).unwrap()
    }

    pub fn internal_cholesky_solve_helper(&self, a: &Tensor, upper: bool) -> Tensor {
        self.f_internal_cholesky_solve_helper(a, upper).unwrap()
    }

    pub fn internal_coalesced_(&mut self, coalesced: bool) -> Tensor {
        self.f_internal_coalesced_(coalesced).unwrap()
    }

    pub fn internal_convolution<T: Borrow<Tensor>>(
        &self,
        weight: &Tensor,
        bias: Option<T>,
        stride: &[i64],
        padding: &[i64],
        dilation: &[i64],
        transposed: bool,
        output_padding: &[i64],
        groups: i64,
        benchmark: bool,
        deterministic: bool,
        cudnn_enabled: bool,
    ) -> Tensor {
        self.f_internal_convolution(
            weight,
            bias,
            stride,
            padding,
            dilation,
            transposed,
            output_padding,
            groups,
            benchmark,
            deterministic,
            cudnn_enabled,
        )
        .unwrap()
    }

    pub fn internal_convolution_nogroup<T: Borrow<Tensor>>(
        &self,
        weight: &Tensor,
        bias: Option<T>,
        stride: &[i64],
        padding: &[i64],
        dilation: &[i64],
        transposed: bool,
        output_padding: &[i64],
    ) -> Tensor {
        self.f_internal_convolution_nogroup(
            weight,
            bias,
            stride,
            padding,
            dilation,
            transposed,
            output_padding,
        )
        .unwrap()
    }

    pub fn internal_copy_from(&self, dst: &Tensor, non_blocking: bool) -> Tensor {
        self.f_internal_copy_from(dst, non_blocking).unwrap()
    }

    pub fn internal_ctc_loss(
        log_probs: &Tensor,
        targets: &Tensor,
        input_lengths: &[i64],
        target_lengths: &[i64],
        blank: i64,
        zero_infinity: bool,
    ) -> (Tensor, Tensor) {
        Tensor::f_internal_ctc_loss(
            log_probs,
            targets,
            input_lengths,
            target_lengths,
            blank,
            zero_infinity,
        )
        .unwrap()
    }

    pub fn internal_ctc_loss_backward(
        grad: &Tensor,
        log_probs: &Tensor,
        targets: &Tensor,
        input_lengths: &[i64],
        target_lengths: &[i64],
        neg_log_likelihood: &Tensor,
        log_alpha: &Tensor,
        blank: i64,
        zero_infinity: bool,
    ) -> Tensor {
        Tensor::f_internal_ctc_loss_backward(
            grad,
            log_probs,
            targets,
            input_lengths,
            target_lengths,
            neg_log_likelihood,
            log_alpha,
            blank,
            zero_infinity,
        )
        .unwrap()
    }

    pub fn internal_cudnn_ctc_loss(
        log_probs: &Tensor,
        targets: &Tensor,
        input_lengths: &[i64],
        target_lengths: &[i64],
        blank: i64,
        deterministic: bool,
        zero_infinity: bool,
    ) -> (Tensor, Tensor) {
        Tensor::f_internal_cudnn_ctc_loss(
            log_probs,
            targets,
            input_lengths,
            target_lengths,
            blank,
            deterministic,
            zero_infinity,
        )
        .unwrap()
    }

    pub fn internal_cudnn_init_dropout_state(
        dropout: f64,
        train: bool,
        dropout_seed: i64,
        options: (Kind, Device),
    ) -> Tensor {
        Tensor::f_internal_cudnn_init_dropout_state(dropout, train, dropout_seed, options).unwrap()
    }

    pub fn internal_cudnn_rnn<T: Borrow<Tensor>>(
        &self,
        weight: &[T],
        weight_stride0: i64,
        weight_buf: Option<T>,
        hx: &Tensor,
        cx: Option<T>,
        mode: i64,
        hidden_size: i64,
        num_layers: i64,
        batch_first: bool,
        dropout: f64,
        train: bool,
        bidirectional: bool,
        batch_sizes: &[i64],
        dropout_state: Option<T>,
    ) -> (Tensor, Tensor, Tensor, Tensor, Tensor) {
        self.f_internal_cudnn_rnn(
            weight,
            weight_stride0,
            weight_buf,
            hx,
            cx,
            mode,
            hidden_size,
            num_layers,
            batch_first,
            dropout,
            train,
            bidirectional,
            batch_sizes,
            dropout_state,
        )
        .unwrap()
    }

    pub fn internal_cudnn_rnn_flatten_weight<T: Borrow<Tensor>>(
        weight_arr: &[T],
        weight_stride0: i64,
        input_size: i64,
        mode: i64,
        hidden_size: i64,
        num_layers: i64,
        batch_first: bool,
        bidirectional: bool,
    ) -> Tensor {
        Tensor::f_internal_cudnn_rnn_flatten_weight(
            weight_arr,
            weight_stride0,
            input_size,
            mode,
            hidden_size,
            num_layers,
            batch_first,
            bidirectional,
        )
        .unwrap()
    }

    pub fn internal_cumprod(&self, dim: i64) -> Tensor {
        self.f_internal_cumprod(dim).unwrap()
    }

    pub fn internal_cumprod_out(&self, out: &Tensor, dim: i64) -> Tensor {
        self.f_internal_cumprod_out(out, dim).unwrap()
    }

    pub fn internal_cumsum(&self, dim: i64) -> Tensor {
        self.f_internal_cumsum(dim).unwrap()
    }

    pub fn internal_cumsum_out(&self, out: &Tensor, dim: i64) -> Tensor {
        self.f_internal_cumsum_out(out, dim).unwrap()
    }

    pub fn internal_dim_arange(like: &Tensor, dim: i64) -> Tensor {
        Tensor::f_internal_dim_arange(like, dim).unwrap()
    }

    pub fn internal_dirichlet_grad(x: &Tensor, alpha: &Tensor, total: &Tensor) -> Tensor {
        Tensor::f_internal_dirichlet_grad(x, alpha, total).unwrap()
    }

    pub fn internal_embedding_bag<T: Borrow<Tensor>>(
        weight: &Tensor,
        indices: &Tensor,
        offsets: &Tensor,
        scale_grad_by_freq: bool,
        mode: i64,
        sparse: bool,
        per_sample_weights: Option<T>,
    ) -> (Tensor, Tensor, Tensor, Tensor) {
        Tensor::f_internal_embedding_bag(
            weight,
            indices,
            offsets,
            scale_grad_by_freq,
            mode,
            sparse,
            per_sample_weights,
        )
        .unwrap()
    }

    pub fn internal_embedding_bag_backward<T: Borrow<Tensor>>(
        grad: &Tensor,
        indices: &Tensor,
        offsets: &Tensor,
        offset2bag: &Tensor,
        bag_size: &Tensor,
        maximum_indices: &Tensor,
        num_weights: i64,
        scale_grad_by_freq: bool,
        mode: i64,
        sparse: bool,
        per_sample_weights: Option<T>,
    ) -> Tensor {
        Tensor::f_internal_embedding_bag_backward(
            grad,
            indices,
            offsets,
            offset2bag,
            bag_size,
            maximum_indices,
            num_weights,
            scale_grad_by_freq,
            mode,
            sparse,
            per_sample_weights,
        )
        .unwrap()
    }

    pub fn internal_embedding_bag_dense_backward<T: Borrow<Tensor>>(
        grad: &Tensor,
        indices: &Tensor,
        offsets: &Tensor,
        offset2bag: &Tensor,
        bag_size: &Tensor,
        maximum_indices: &Tensor,
        num_weights: i64,
        scale_grad_by_freq: bool,
        mode: i64,
        per_sample_weights: Option<T>,
    ) -> Tensor {
        Tensor::f_internal_embedding_bag_dense_backward(
            grad,
            indices,
            offsets,
            offset2bag,
            bag_size,
            maximum_indices,
            num_weights,
            scale_grad_by_freq,
            mode,
            per_sample_weights,
        )
        .unwrap()
    }

    pub fn internal_embedding_bag_per_sample_weights_backward(
        grad: &Tensor,
        weight: &Tensor,
        indices: &Tensor,
        offsets: &Tensor,
        offset2bag: &Tensor,
        mode: i64,
    ) -> Tensor {
        Tensor::f_internal_embedding_bag_per_sample_weights_backward(
            grad, weight, indices, offsets, offset2bag, mode,
        )
        .unwrap()
    }

    pub fn internal_embedding_bag_sparse_backward<T: Borrow<Tensor>>(
        grad: &Tensor,
        indices: &Tensor,
        offsets: &Tensor,
        offset2bag: &Tensor,
        bag_size: &Tensor,
        num_weights: i64,
        scale_grad_by_freq: bool,
        mode: i64,
        per_sample_weights: Option<T>,
    ) -> Tensor {
        Tensor::f_internal_embedding_bag_sparse_backward(
            grad,
            indices,
            offsets,
            offset2bag,
            bag_size,
            num_weights,
            scale_grad_by_freq,
            mode,
            per_sample_weights,
        )
        .unwrap()
    }

    pub fn internal_empty_affine_quantized(
        size: &[i64],
        options: (Kind, Device),
        scale: f64,
        zero_point: i64,
    ) -> Tensor {
        Tensor::f_internal_empty_affine_quantized(size, options, scale, zero_point).unwrap()
    }

    pub fn internal_empty_per_channel_affine_quantized(
        size: &[i64],
        scales: &Tensor,
        zero_points: &Tensor,
        axis: i64,
        options: (Kind, Device),
    ) -> Tensor {
        Tensor::f_internal_empty_per_channel_affine_quantized(
            size,
            scales,
            zero_points,
            axis,
            options,
        )
        .unwrap()
    }

    pub fn internal_fft_with_size(
        &self,
        signal_ndim: i64,
        complex_input: bool,
        complex_output: bool,
        inverse: bool,
        checked_signal_sizes: &[i64],
        normalized: bool,
        onesided: bool,
        output_sizes: &[i64],
    ) -> Tensor {
        self.f_internal_fft_with_size(
            signal_ndim,
            complex_input,
            complex_output,
            inverse,
            checked_signal_sizes,
            normalized,
            onesided,
            output_sizes,
        )
        .unwrap()
    }

    pub fn internal_fused_dropout(&self, p: f64) -> (Tensor, Tensor) {
        self.f_internal_fused_dropout(p).unwrap()
    }

    pub fn internal_gather_sparse_backward(
        &self,
        dim: i64,
        index: &Tensor,
        grad: &Tensor,
    ) -> Tensor {
        self.f_internal_gather_sparse_backward(dim, index, grad)
            .unwrap()
    }

    pub fn internal_index_copy_(&mut self, dim: i64, index: &Tensor, source: &Tensor) -> Tensor {
        self.f_internal_index_copy_(dim, index, source).unwrap()
    }

    pub fn internal_index_put_impl_<T: Borrow<Tensor>>(
        &mut self,
        indices: &[T],
        values: &Tensor,
        accumulate: bool,
        unsafe_: bool,
    ) -> Tensor {
        self.f_internal_index_put_impl_(indices, values, accumulate, unsafe_)
            .unwrap()
    }

    pub fn internal_indices(&self) -> Tensor {
        self.f_internal_indices().unwrap()
    }

    pub fn internal_inverse_helper(&self) -> Tensor {
        self.f_internal_inverse_helper().unwrap()
    }

    pub fn internal_log_softmax(&self, dim: i64, half_to_float: bool) -> Tensor {
        self.f_internal_log_softmax(dim, half_to_float).unwrap()
    }

    pub fn internal_log_softmax_backward_data(
        &self,
        grad_output: &Tensor,
        output: &Tensor,
        dim: i64,
    ) -> Tensor {
        self.f_internal_log_softmax_backward_data(grad_output, output, dim)
            .unwrap()
    }

    pub fn internal_lu_solve_helper(&self, lu_data: &Tensor, lu_pivots: &Tensor) -> Tensor {
        self.f_internal_lu_solve_helper(lu_data, lu_pivots).unwrap()
    }

    pub fn internal_lu_with_info(
        &self,
        pivot: bool,
        check_errors: bool,
    ) -> (Tensor, Tensor, Tensor) {
        self.f_internal_lu_with_info(pivot, check_errors).unwrap()
    }

    pub fn internal_make_per_channel_quantized_tensor(
        &self,
        scale: &Tensor,
        zero_point: &Tensor,
        axis: i64,
    ) -> Tensor {
        self.f_internal_make_per_channel_quantized_tensor(scale, zero_point, axis)
            .unwrap()
    }

    pub fn internal_make_per_tensor_quantized_tensor(&self, scale: f64, zero_point: i64) -> Tensor {
        self.f_internal_make_per_tensor_quantized_tensor(scale, zero_point)
            .unwrap()
    }

    pub fn internal_masked_scale(&self, mask: &Tensor, scale: f64) -> Tensor {
        self.f_internal_masked_scale(mask, scale).unwrap()
    }

    pub fn internal_max(&self, dim: i64, keepdim: bool) -> (Tensor, Tensor) {
        self.f_internal_max(dim, keepdim).unwrap()
    }

    pub fn internal_max_out(
        &self,
        max: &Tensor,
        max_indices: &Tensor,
        dim: i64,
        keepdim: bool,
    ) -> (Tensor, Tensor) {
        self.f_internal_max_out(max, max_indices, dim, keepdim)
            .unwrap()
    }

    pub fn internal_min(&self, dim: i64, keepdim: bool) -> (Tensor, Tensor) {
        self.f_internal_min(dim, keepdim).unwrap()
    }

    pub fn internal_min_out(
        &self,
        min: &Tensor,
        min_indices: &Tensor,
        dim: i64,
        keepdim: bool,
    ) -> (Tensor, Tensor) {
        self.f_internal_min_out(min, min_indices, dim, keepdim)
            .unwrap()
    }

    pub fn internal_mkldnn_reshape(&self, shape: &[i64]) -> Tensor {
        self.f_internal_mkldnn_reshape(shape).unwrap()
    }

    pub fn internal_mkldnn_transpose(&self, dim0: i64, dim1: i64) -> Tensor {
        self.f_internal_mkldnn_transpose(dim0, dim1).unwrap()
    }

    pub fn internal_mkldnn_transpose_(&mut self, dim0: i64, dim1: i64) -> Tensor {
        self.f_internal_mkldnn_transpose_(dim0, dim1).unwrap()
    }

    pub fn internal_mode(&self, dim: i64, keepdim: bool) -> (Tensor, Tensor) {
        self.f_internal_mode(dim, keepdim).unwrap()
    }

    pub fn internal_mode_out(
        &self,
        values: &Tensor,
        indices: &Tensor,
        dim: i64,
        keepdim: bool,
    ) -> (Tensor, Tensor) {
        self.f_internal_mode_out(values, indices, dim, keepdim)
            .unwrap()
    }

    pub fn internal_multinomial_alias_draw(j: &Tensor, q: &Tensor, num_samples: i64) -> Tensor {
        Tensor::f_internal_multinomial_alias_draw(j, q, num_samples).unwrap()
    }

    pub fn internal_multinomial_alias_setup(probs: &Tensor) -> (Tensor, Tensor) {
        Tensor::f_internal_multinomial_alias_setup(probs).unwrap()
    }

    pub fn internal_nnpack_spatial_convolution<T: Borrow<Tensor>>(
        &self,
        weight: &Tensor,
        bias: Option<T>,
        padding: &[i64],
        stride: &[i64],
    ) -> Tensor {
        self.f_internal_nnpack_spatial_convolution(weight, bias, padding, stride)
            .unwrap()
    }

    pub fn internal_nnpack_spatial_convolution_backward_input(
        &self,
        grad_output: &Tensor,
        weight: &Tensor,
        padding: &[i64],
    ) -> Tensor {
        self.f_internal_nnpack_spatial_convolution_backward_input(grad_output, weight, padding)
            .unwrap()
    }

    pub fn internal_nnpack_spatial_convolution_backward_weight(
        &self,
        weightsize: &[i64],
        grad_output: &Tensor,
        padding: &[i64],
    ) -> Tensor {
        self.f_internal_nnpack_spatial_convolution_backward_weight(weightsize, grad_output, padding)
            .unwrap()
    }

    pub fn internal_pack_padded_sequence(
        &self,
        lengths: &Tensor,
        batch_first: bool,
    ) -> (Tensor, Tensor) {
        self.f_internal_pack_padded_sequence(lengths, batch_first)
            .unwrap()
    }

    pub fn internal_pack_padded_sequence_backward(
        grad: &Tensor,
        input_size: &[i64],
        batch_sizes: &Tensor,
        batch_first: bool,
    ) -> Tensor {
        Tensor::f_internal_pack_padded_sequence_backward(grad, input_size, batch_sizes, batch_first)
            .unwrap()
    }

    pub fn internal_pad_packed_sequence<S: Into<Scalar>>(
        data: &Tensor,
        batch_sizes: &Tensor,
        batch_first: bool,
        padding_value: S,
        total_length: i64,
    ) -> (Tensor, Tensor) {
        Tensor::f_internal_pad_packed_sequence(
            data,
            batch_sizes,
            batch_first,
            padding_value,
            total_length,
        )
        .unwrap()
    }

    pub fn internal_pdist_backward(&self, grad: &Tensor, p: f64, pdist: &Tensor) -> Tensor {
        self.f_internal_pdist_backward(grad, p, pdist).unwrap()
    }

    pub fn internal_qr_helper(&self, some: bool) -> (Tensor, Tensor) {
        self.f_internal_qr_helper(some).unwrap()
    }

    pub fn internal_reshape_from_tensor(&self, shape: &Tensor) -> Tensor {
        self.f_internal_reshape_from_tensor(shape).unwrap()
    }

    pub fn internal_s_where(&self, condition: &Tensor, other: &Tensor) -> Tensor {
        self.f_internal_s_where(condition, other).unwrap()
    }

    pub fn internal_sample_dirichlet(&self) -> Tensor {
        self.f_internal_sample_dirichlet().unwrap()
    }

    pub fn internal_shape_as_tensor(&self) -> Tensor {
        self.f_internal_shape_as_tensor().unwrap()
    }

    pub fn internal_sobol_engine_draw(
        quasi: &Tensor,
        n: i64,
        sobolstate: &Tensor,
        dimension: i64,
        num_generated: i64,
        dtype: Kind,
    ) -> (Tensor, Tensor) {
        Tensor::f_internal_sobol_engine_draw(quasi, n, sobolstate, dimension, num_generated, dtype)
            .unwrap()
    }

    pub fn internal_sobol_engine_ff_(
        &mut self,
        n: i64,
        sobolstate: &Tensor,
        dimension: i64,
        num_generated: i64,
    ) -> Tensor {
        self.f_internal_sobol_engine_ff_(n, sobolstate, dimension, num_generated)
            .unwrap()
    }

    pub fn internal_sobol_engine_initialize_state_(&mut self, dimension: i64) -> Tensor {
        self.f_internal_sobol_engine_initialize_state_(dimension)
            .unwrap()
    }

    pub fn internal_sobol_engine_scramble_(&mut self, ltm: &Tensor, dimension: i64) -> Tensor {
        self.f_internal_sobol_engine_scramble_(ltm, dimension)
            .unwrap()
    }

    pub fn internal_softmax(&self, dim: i64, half_to_float: bool) -> Tensor {
        self.f_internal_softmax(dim, half_to_float).unwrap()
    }

    pub fn internal_softmax_backward_data(
        &self,
        grad_output: &Tensor,
        output: &Tensor,
        dim: i64,
    ) -> Tensor {
        self.f_internal_softmax_backward_data(grad_output, output, dim)
            .unwrap()
    }

    pub fn internal_solve_helper(&self, a: &Tensor) -> (Tensor, Tensor) {
        self.f_internal_solve_helper(a).unwrap()
    }

    pub fn internal_sparse_addmm(&self, sparse: &Tensor, dense: &Tensor) -> Tensor {
        self.f_internal_sparse_addmm(sparse, dense).unwrap()
    }

    pub fn internal_sparse_coo_tensor_unsafe(
        indices: &Tensor,
        values: &Tensor,
        size: &[i64],
        options: (Kind, Device),
    ) -> Tensor {
        Tensor::f_internal_sparse_coo_tensor_unsafe(indices, values, size, options).unwrap()
    }

    pub fn internal_sparse_coo_tensor_with_dims(
        sparse_dim: i64,
        dense_dim: i64,
        size: &[i64],
        options: (Kind, Device),
    ) -> Tensor {
        Tensor::f_internal_sparse_coo_tensor_with_dims(sparse_dim, dense_dim, size, options)
            .unwrap()
    }

    pub fn internal_sparse_coo_tensor_with_dims_and_tensors(
        sparse_dim: i64,
        dense_dim: i64,
        size: &[i64],
        indices: &Tensor,
        values: &Tensor,
        options: (Kind, Device),
    ) -> Tensor {
        Tensor::f_internal_sparse_coo_tensor_with_dims_and_tensors(
            sparse_dim, dense_dim, size, indices, values, options,
        )
        .unwrap()
    }

    pub fn internal_sparse_mm(sparse: &Tensor, dense: &Tensor) -> Tensor {
        Tensor::f_internal_sparse_mm(sparse, dense).unwrap()
    }

    pub fn internal_sparse_sum(&self) -> Tensor {
        self.f_internal_sparse_sum().unwrap()
    }

    pub fn internal_sparse_sum1(&self, dtype: Kind) -> Tensor {
        self.f_internal_sparse_sum1(dtype).unwrap()
    }

    pub fn internal_sparse_sum2(&self, dim: &[i64]) -> Tensor {
        self.f_internal_sparse_sum2(dim).unwrap()
    }

    pub fn internal_sparse_sum3(&self, dim: &[i64], dtype: Kind) -> Tensor {
        self.f_internal_sparse_sum3(dim, dtype).unwrap()
    }

    pub fn internal_sparse_sum_backward(&self, grad: &Tensor, dim: &[i64]) -> Tensor {
        self.f_internal_sparse_sum_backward(grad, dim).unwrap()
    }

    pub fn internal_standard_gamma(&self) -> Tensor {
        self.f_internal_standard_gamma().unwrap()
    }

    pub fn internal_standard_gamma_grad(&self, output: &Tensor) -> Tensor {
        self.f_internal_standard_gamma_grad(output).unwrap()
    }

    pub fn internal_std(&self, unbiased: bool) -> Tensor {
        self.f_internal_std(unbiased).unwrap()
    }

    pub fn internal_svd_helper(&self, some: bool, compute_uv: bool) -> (Tensor, Tensor, Tensor) {
        self.f_internal_svd_helper(some, compute_uv).unwrap()
    }

    pub fn internal_symeig_helper(&self, eigenvectors: bool, upper: bool) -> (Tensor, Tensor) {
        self.f_internal_symeig_helper(eigenvectors, upper).unwrap()
    }

    pub fn internal_test_optional_float(&self, scale: f64) -> Tensor {
        self.f_internal_test_optional_float(scale).unwrap()
    }

    pub fn internal_triangular_solve_helper(
        &self,
        a: &Tensor,
        upper: bool,
        transpose: bool,
        unitriangular: bool,
    ) -> (Tensor, Tensor) {
        self.f_internal_triangular_solve_helper(a, upper, transpose, unitriangular)
            .unwrap()
    }

    pub fn internal_trilinear(
        i1: &Tensor,
        i2: &Tensor,
        i3: &Tensor,
        expand1: &[i64],
        expand2: &[i64],
        expand3: &[i64],
        sumdim: &[i64],
        unroll_dim: i64,
    ) -> Tensor {
        Tensor::f_internal_trilinear(i1, i2, i3, expand1, expand2, expand3, sumdim, unroll_dim)
            .unwrap()
    }

    pub fn internal_unique(&self, sorted: bool, return_inverse: bool) -> (Tensor, Tensor) {
        self.f_internal_unique(sorted, return_inverse).unwrap()
    }

    pub fn internal_unique2(
        &self,
        sorted: bool,
        return_inverse: bool,
        return_counts: bool,
    ) -> (Tensor, Tensor, Tensor) {
        self.f_internal_unique2(sorted, return_inverse, return_counts)
            .unwrap()
    }

    pub fn internal_unsafe_view(&self, size: &[i64]) -> Tensor {
        self.f_internal_unsafe_view(size).unwrap()
    }

    pub fn internal_values(&self) -> Tensor {
        self.f_internal_values().unwrap()
    }

    pub fn internal_var(&self, unbiased: bool) -> Tensor {
        self.f_internal_var(unbiased).unwrap()
    }

    pub fn internal_weight_norm(v: &Tensor, g: &Tensor, dim: i64) -> Tensor {
        Tensor::f_internal_weight_norm(v, g, dim).unwrap()
    }

    pub fn internal_weight_norm_cuda_interface(
        v: &Tensor,
        g: &Tensor,
        dim: i64,
    ) -> (Tensor, Tensor) {
        Tensor::f_internal_weight_norm_cuda_interface(v, g, dim).unwrap()
    }

    pub fn internal_weight_norm_cuda_interface_backward(
        grad_w: &Tensor,
        saved_v: &Tensor,
        saved_g: &Tensor,
        saved_norms: &Tensor,
        dim: i64,
    ) -> (Tensor, Tensor) {
        Tensor::f_internal_weight_norm_cuda_interface_backward(
            grad_w,
            saved_v,
            saved_g,
            saved_norms,
            dim,
        )
        .unwrap()
    }

    pub fn internal_weight_norm_differentiable_backward(
        grad_w: &Tensor,
        saved_v: &Tensor,
        saved_g: &Tensor,
        saved_norms: &Tensor,
        dim: i64,
    ) -> (Tensor, Tensor) {
        Tensor::f_internal_weight_norm_differentiable_backward(
            grad_w,
            saved_v,
            saved_g,
            saved_norms,
            dim,
        )
        .unwrap()
    }

    pub fn abs(&self) -> Tensor {
        self.f_abs().unwrap()
    }

    pub fn abs_(&mut self) -> Tensor {
        self.f_abs_().unwrap()
    }

    pub fn abs_out(&self, out: &Tensor) -> Tensor {
        self.f_abs_out(out).unwrap()
    }

    pub fn acos(&self) -> Tensor {
        self.f_acos().unwrap()
    }

    pub fn acos_(&mut self) -> Tensor {
        self.f_acos_().unwrap()
    }

    pub fn acos_out(&self, out: &Tensor) -> Tensor {
        self.f_acos_out(out).unwrap()
    }

    pub fn adaptive_avg_pool1d(&self, output_size: &[i64]) -> Tensor {
        self.f_adaptive_avg_pool1d(output_size).unwrap()
    }

    pub fn adaptive_avg_pool2d(&self, output_size: &[i64]) -> Tensor {
        self.f_adaptive_avg_pool2d(output_size).unwrap()
    }

    pub fn adaptive_avg_pool2d_out(&self, out: &Tensor, output_size: &[i64]) -> Tensor {
        self.f_adaptive_avg_pool2d_out(out, output_size).unwrap()
    }

    pub fn adaptive_avg_pool3d(&self, output_size: &[i64]) -> Tensor {
        self.f_adaptive_avg_pool3d(output_size).unwrap()
    }

    pub fn adaptive_avg_pool3d_backward(&self, grad_output: &Tensor) -> Tensor {
        self.f_adaptive_avg_pool3d_backward(grad_output).unwrap()
    }

    pub fn adaptive_avg_pool3d_backward_out(
        &self,
        grad_input: &Tensor,
        grad_output: &Tensor,
    ) -> Tensor {
        self.f_adaptive_avg_pool3d_backward_out(grad_input, grad_output)
            .unwrap()
    }

    pub fn adaptive_avg_pool3d_out(&self, out: &Tensor, output_size: &[i64]) -> Tensor {
        self.f_adaptive_avg_pool3d_out(out, output_size).unwrap()
    }

    pub fn adaptive_max_pool1d(&self, output_size: &[i64]) -> (Tensor, Tensor) {
        self.f_adaptive_max_pool1d(output_size).unwrap()
    }

    pub fn adaptive_max_pool2d(&self, output_size: &[i64]) -> (Tensor, Tensor) {
        self.f_adaptive_max_pool2d(output_size).unwrap()
    }

    pub fn adaptive_max_pool2d_backward(&self, grad_output: &Tensor, indices: &Tensor) -> Tensor {
        self.f_adaptive_max_pool2d_backward(grad_output, indices)
            .unwrap()
    }

    pub fn adaptive_max_pool2d_backward_out(
        &self,
        grad_input: &Tensor,
        grad_output: &Tensor,
        indices: &Tensor,
    ) -> Tensor {
        self.f_adaptive_max_pool2d_backward_out(grad_input, grad_output, indices)
            .unwrap()
    }

    pub fn adaptive_max_pool2d_out(
        &self,
        out: &Tensor,
        indices: &Tensor,
        output_size: &[i64],
    ) -> (Tensor, Tensor) {
        self.f_adaptive_max_pool2d_out(out, indices, output_size)
            .unwrap()
    }

    pub fn adaptive_max_pool3d(&self, output_size: &[i64]) -> (Tensor, Tensor) {
        self.f_adaptive_max_pool3d(output_size).unwrap()
    }

    pub fn adaptive_max_pool3d_backward(&self, grad_output: &Tensor, indices: &Tensor) -> Tensor {
        self.f_adaptive_max_pool3d_backward(grad_output, indices)
            .unwrap()
    }

    pub fn adaptive_max_pool3d_backward_out(
        &self,
        grad_input: &Tensor,
        grad_output: &Tensor,
        indices: &Tensor,
    ) -> Tensor {
        self.f_adaptive_max_pool3d_backward_out(grad_input, grad_output, indices)
            .unwrap()
    }

    pub fn adaptive_max_pool3d_out(
        &self,
        out: &Tensor,
        indices: &Tensor,
        output_size: &[i64],
    ) -> (Tensor, Tensor) {
        self.f_adaptive_max_pool3d_out(out, indices, output_size)
            .unwrap()
    }

    pub fn g_add(&self, other: &Tensor) -> Tensor {
        self.f_add(other).unwrap()
    }

    pub fn g_add1<S: Into<Scalar>>(&self, other: S) -> Tensor {
        self.f_add1(other).unwrap()
    }

    pub fn g_add_(&mut self, other: &Tensor) -> Tensor {
        self.f_add_(other).unwrap()
    }

    pub fn g_add_1<S: Into<Scalar>>(&mut self, other: S) -> Tensor {
        self.f_add_1(other).unwrap()
    }

    pub fn add_out(&self, out: &Tensor, other: &Tensor) -> Tensor {
        self.f_add_out(out, other).unwrap()
    }

    pub fn addbmm(&self, batch1: &Tensor, batch2: &Tensor) -> Tensor {
        self.f_addbmm(batch1, batch2).unwrap()
    }

    pub fn addbmm_(&mut self, batch1: &Tensor, batch2: &Tensor) -> Tensor {
        self.f_addbmm_(batch1, batch2).unwrap()
    }

    pub fn addbmm_out(&self, out: &Tensor, batch1: &Tensor, batch2: &Tensor) -> Tensor {
        self.f_addbmm_out(out, batch1, batch2).unwrap()
    }

    pub fn addcdiv(&self, tensor1: &Tensor, tensor2: &Tensor) -> Tensor {
        self.f_addcdiv(tensor1, tensor2).unwrap()
    }

    pub fn addcdiv_(&mut self, tensor1: &Tensor, tensor2: &Tensor) -> Tensor {
        self.f_addcdiv_(tensor1, tensor2).unwrap()
    }

    pub fn addcdiv_out(&self, out: &Tensor, tensor1: &Tensor, tensor2: &Tensor) -> Tensor {
        self.f_addcdiv_out(out, tensor1, tensor2).unwrap()
    }

    pub fn addcmul(&self, tensor1: &Tensor, tensor2: &Tensor) -> Tensor {
        self.f_addcmul(tensor1, tensor2).unwrap()
    }

    pub fn addcmul_(&mut self, tensor1: &Tensor, tensor2: &Tensor) -> Tensor {
        self.f_addcmul_(tensor1, tensor2).unwrap()
    }

    pub fn addcmul_out(&self, out: &Tensor, tensor1: &Tensor, tensor2: &Tensor) -> Tensor {
        self.f_addcmul_out(out, tensor1, tensor2).unwrap()
    }

    pub fn addmm(&self, mat1: &Tensor, mat2: &Tensor) -> Tensor {
        self.f_addmm(mat1, mat2).unwrap()
    }

    pub fn addmm_(&mut self, mat1: &Tensor, mat2: &Tensor) -> Tensor {
        self.f_addmm_(mat1, mat2).unwrap()
    }

    pub fn addmm_out(&self, out: &Tensor, mat1: &Tensor, mat2: &Tensor) -> Tensor {
        self.f_addmm_out(out, mat1, mat2).unwrap()
    }

    pub fn addmv(&self, mat: &Tensor, vec: &Tensor) -> Tensor {
        self.f_addmv(mat, vec).unwrap()
    }

    pub fn addmv_(&mut self, mat: &Tensor, vec: &Tensor) -> Tensor {
        self.f_addmv_(mat, vec).unwrap()
    }

    pub fn addmv_out(&self, out: &Tensor, mat: &Tensor, vec: &Tensor) -> Tensor {
        self.f_addmv_out(out, mat, vec).unwrap()
    }

    pub fn addr(&self, vec1: &Tensor, vec2: &Tensor) -> Tensor {
        self.f_addr(vec1, vec2).unwrap()
    }

    pub fn addr_(&mut self, vec1: &Tensor, vec2: &Tensor) -> Tensor {
        self.f_addr_(vec1, vec2).unwrap()
    }

    pub fn addr_out(&self, out: &Tensor, vec1: &Tensor, vec2: &Tensor) -> Tensor {
        self.f_addr_out(out, vec1, vec2).unwrap()
    }

    pub fn affine_grid_generator(theta: &Tensor, size: &[i64], align_corners: bool) -> Tensor {
        Tensor::f_affine_grid_generator(theta, size, align_corners).unwrap()
    }

    pub fn affine_grid_generator_backward(
        grad: &Tensor,
        size: &[i64],
        align_corners: bool,
    ) -> Tensor {
        Tensor::f_affine_grid_generator_backward(grad, size, align_corners).unwrap()
    }

    pub fn alias(&self) -> Tensor {
        self.f_alias().unwrap()
    }

    pub fn align_as(&self, other: &Tensor) -> Tensor {
        self.f_align_as(other).unwrap()
    }

    pub fn align_tensors<T: Borrow<Tensor>>(tensors: &[T]) -> Vec<Tensor> {
        Tensor::f_align_tensors(tensors).unwrap()
    }

    pub fn all(&self) -> Tensor {
        self.f_all().unwrap()
    }

    pub fn all1(&self, dim: i64, keepdim: bool) -> Tensor {
        self.f_all1(dim, keepdim).unwrap()
    }

    pub fn all_out(&self, out: &Tensor, dim: i64, keepdim: bool) -> Tensor {
        self.f_all_out(out, dim, keepdim).unwrap()
    }

    pub fn alpha_dropout(&self, p: f64, train: bool) -> Tensor {
        self.f_alpha_dropout(p, train).unwrap()
    }

    pub fn alpha_dropout_(&mut self, p: f64, train: bool) -> Tensor {
        self.f_alpha_dropout_(p, train).unwrap()
    }

    pub fn angle(&self) -> Tensor {
        self.f_angle().unwrap()
    }

    pub fn angle_out(&self, out: &Tensor) -> Tensor {
        self.f_angle_out(out).unwrap()
    }

    pub fn any(&self) -> Tensor {
        self.f_any().unwrap()
    }

    pub fn any1(&self, dim: i64, keepdim: bool) -> Tensor {
        self.f_any1(dim, keepdim).unwrap()
    }

    pub fn any_out(&self, out: &Tensor, dim: i64, keepdim: bool) -> Tensor {
        self.f_any_out(out, dim, keepdim).unwrap()
    }

    pub fn arange<S: Into<Scalar>>(end: S, options: (Kind, Device)) -> Tensor {
        Tensor::f_arange(end, options).unwrap()
    }

    pub fn arange1<S: Into<Scalar>>(start: S, end: S, options: (Kind, Device)) -> Tensor {
        Tensor::f_arange1(start, end, options).unwrap()
    }

    pub fn arange2<S: Into<Scalar>>(start: S, end: S, step: S, options: (Kind, Device)) -> Tensor {
        Tensor::f_arange2(start, end, step, options).unwrap()
    }

    pub fn arange_out<S: Into<Scalar>>(out: &Tensor, end: S) -> Tensor {
        Tensor::f_arange_out(out, end).unwrap()
    }

    pub fn arange_out1<S: Into<Scalar>>(out: &Tensor, start: S, end: S) -> Tensor {
        Tensor::f_arange_out1(out, start, end).unwrap()
    }

    pub fn argmax(&self, dim: i64, keepdim: bool) -> Tensor {
        self.f_argmax(dim, keepdim).unwrap()
    }

    pub fn argmin(&self, dim: i64, keepdim: bool) -> Tensor {
        self.f_argmin(dim, keepdim).unwrap()
    }

    pub fn argsort(&self, dim: i64, descending: bool) -> Tensor {
        self.f_argsort(dim, descending).unwrap()
    }

    pub fn as_strided(&self, size: &[i64], stride: &[i64], storage_offset: i64) -> Tensor {
        self.f_as_strided(size, stride, storage_offset).unwrap()
    }

    pub fn as_strided_(&mut self, size: &[i64], stride: &[i64], storage_offset: i64) -> Tensor {
        self.f_as_strided_(size, stride, storage_offset).unwrap()
    }

    pub fn asin(&self) -> Tensor {
        self.f_asin().unwrap()
    }

    pub fn asin_(&mut self) -> Tensor {
        self.f_asin_().unwrap()
    }

    pub fn asin_out(&self, out: &Tensor) -> Tensor {
        self.f_asin_out(out).unwrap()
    }

    pub fn atan(&self) -> Tensor {
        self.f_atan().unwrap()
    }

    pub fn atan2(&self, other: &Tensor) -> Tensor {
        self.f_atan2(other).unwrap()
    }

    pub fn atan2_(&mut self, other: &Tensor) -> Tensor {
        self.f_atan2_(other).unwrap()
    }

    pub fn atan2_out(&self, out: &Tensor, other: &Tensor) -> Tensor {
        self.f_atan2_out(out, other).unwrap()
    }

    pub fn atan_(&mut self) -> Tensor {
        self.f_atan_().unwrap()
    }

    pub fn atan_out(&self, out: &Tensor) -> Tensor {
        self.f_atan_out(out).unwrap()
    }

    pub fn avg_pool1d(
        &self,
        kernel_size: &[i64],
        stride: &[i64],
        padding: &[i64],
        ceil_mode: bool,
        count_include_pad: bool,
    ) -> Tensor {
        self.f_avg_pool1d(kernel_size, stride, padding, ceil_mode, count_include_pad)
            .unwrap()
    }

    pub fn avg_pool2d(
        &self,
        kernel_size: &[i64],
        stride: &[i64],
        padding: &[i64],
        ceil_mode: bool,
        count_include_pad: bool,
        divisor_override: i64,
    ) -> Tensor {
        self.f_avg_pool2d(
            kernel_size,
            stride,
            padding,
            ceil_mode,
            count_include_pad,
            divisor_override,
        )
        .unwrap()
    }

    pub fn avg_pool2d_backward(
        &self,
        grad_output: &Tensor,
        kernel_size: &[i64],
        stride: &[i64],
        padding: &[i64],
        ceil_mode: bool,
        count_include_pad: bool,
        divisor_override: i64,
    ) -> Tensor {
        self.f_avg_pool2d_backward(
            grad_output,
            kernel_size,
            stride,
            padding,
            ceil_mode,
            count_include_pad,
            divisor_override,
        )
        .unwrap()
    }

    pub fn avg_pool2d_backward_out(
        &self,
        grad_input: &Tensor,
        grad_output: &Tensor,
        kernel_size: &[i64],
        stride: &[i64],
        padding: &[i64],
        ceil_mode: bool,
        count_include_pad: bool,
        divisor_override: i64,
    ) -> Tensor {
        self.f_avg_pool2d_backward_out(
            grad_input,
            grad_output,
            kernel_size,
            stride,
            padding,
            ceil_mode,
            count_include_pad,
            divisor_override,
        )
        .unwrap()
    }

    pub fn avg_pool2d_out(
        &self,
        out: &Tensor,
        kernel_size: &[i64],
        stride: &[i64],
        padding: &[i64],
        ceil_mode: bool,
        count_include_pad: bool,
        divisor_override: i64,
    ) -> Tensor {
        self.f_avg_pool2d_out(
            out,
            kernel_size,
            stride,
            padding,
            ceil_mode,
            count_include_pad,
            divisor_override,
        )
        .unwrap()
    }

    pub fn avg_pool3d(
        &self,
        kernel_size: &[i64],
        stride: &[i64],
        padding: &[i64],
        ceil_mode: bool,
        count_include_pad: bool,
        divisor_override: i64,
    ) -> Tensor {
        self.f_avg_pool3d(
            kernel_size,
            stride,
            padding,
            ceil_mode,
            count_include_pad,
            divisor_override,
        )
        .unwrap()
    }

    pub fn avg_pool3d_backward(
        &self,
        grad_output: &Tensor,
        kernel_size: &[i64],
        stride: &[i64],
        padding: &[i64],
        ceil_mode: bool,
        count_include_pad: bool,
        divisor_override: i64,
    ) -> Tensor {
        self.f_avg_pool3d_backward(
            grad_output,
            kernel_size,
            stride,
            padding,
            ceil_mode,
            count_include_pad,
            divisor_override,
        )
        .unwrap()
    }

    pub fn avg_pool3d_backward_out(
        &self,
        grad_input: &Tensor,
        grad_output: &Tensor,
        kernel_size: &[i64],
        stride: &[i64],
        padding: &[i64],
        ceil_mode: bool,
        count_include_pad: bool,
        divisor_override: i64,
    ) -> Tensor {
        self.f_avg_pool3d_backward_out(
            grad_input,
            grad_output,
            kernel_size,
            stride,
            padding,
            ceil_mode,
            count_include_pad,
            divisor_override,
        )
        .unwrap()
    }

    pub fn avg_pool3d_out(
        &self,
        out: &Tensor,
        kernel_size: &[i64],
        stride: &[i64],
        padding: &[i64],
        ceil_mode: bool,
        count_include_pad: bool,
        divisor_override: i64,
    ) -> Tensor {
        self.f_avg_pool3d_out(
            out,
            kernel_size,
            stride,
            padding,
            ceil_mode,
            count_include_pad,
            divisor_override,
        )
        .unwrap()
    }

    pub fn baddbmm(&self, batch1: &Tensor, batch2: &Tensor) -> Tensor {
        self.f_baddbmm(batch1, batch2).unwrap()
    }

    pub fn baddbmm_(&mut self, batch1: &Tensor, batch2: &Tensor) -> Tensor {
        self.f_baddbmm_(batch1, batch2).unwrap()
    }

    pub fn baddbmm_out(&self, out: &Tensor, batch1: &Tensor, batch2: &Tensor) -> Tensor {
        self.f_baddbmm_out(out, batch1, batch2).unwrap()
    }

    pub fn bartlett_window(window_length: i64, options: (Kind, Device)) -> Tensor {
        Tensor::f_bartlett_window(window_length, options).unwrap()
    }

    pub fn bartlett_window1(window_length: i64, periodic: bool, options: (Kind, Device)) -> Tensor {
        Tensor::f_bartlett_window1(window_length, periodic, options).unwrap()
    }

    pub fn batch_norm<T: Borrow<Tensor>>(
        &self,
        weight: Option<T>,
        bias: Option<T>,
        running_mean: Option<T>,
        running_var: Option<T>,
        training: bool,
        momentum: f64,
        eps: f64,
        cudnn_enabled: bool,
    ) -> Tensor {
        self.f_batch_norm(
            weight,
            bias,
            running_mean,
            running_var,
            training,
            momentum,
            eps,
            cudnn_enabled,
        )
        .unwrap()
    }

    pub fn batch_norm_backward_elemt<T: Borrow<Tensor>>(
        &self,
        grad_out: &Tensor,
        mean: &Tensor,
        invstd: &Tensor,
        weight: Option<T>,
        mean_dy: &Tensor,
        mean_dy_xmu: &Tensor,
    ) -> Tensor {
        self.f_batch_norm_backward_elemt(grad_out, mean, invstd, weight, mean_dy, mean_dy_xmu)
            .unwrap()
    }

    pub fn batch_norm_backward_reduce<T: Borrow<Tensor>>(
        &self,
        grad_out: &Tensor,
        mean: &Tensor,
        invstd: &Tensor,
        weight: Option<T>,
        input_g: bool,
        weight_g: bool,
        bias_g: bool,
    ) -> (Tensor, Tensor, Tensor, Tensor) {
        self.f_batch_norm_backward_reduce(grad_out, mean, invstd, weight, input_g, weight_g, bias_g)
            .unwrap()
    }

    pub fn batch_norm_elemt<T: Borrow<Tensor>>(
        &self,
        weight: Option<T>,
        bias: Option<T>,
        mean: &Tensor,
        invstd: &Tensor,
        eps: f64,
    ) -> Tensor {
        self.f_batch_norm_elemt(weight, bias, mean, invstd, eps)
            .unwrap()
    }

    pub fn batch_norm_elemt_out<T: Borrow<Tensor>>(
        &self,
        out: &Tensor,
        weight: Option<T>,
        bias: Option<T>,
        mean: &Tensor,
        invstd: &Tensor,
        eps: f64,
    ) -> Tensor {
        self.f_batch_norm_elemt_out(out, weight, bias, mean, invstd, eps)
            .unwrap()
    }

    pub fn batch_norm_gather_stats<T: Borrow<Tensor>>(
        &self,
        mean: &Tensor,
        invstd: &Tensor,
        running_mean: Option<T>,
        running_var: Option<T>,
        momentum: f64,
        eps: f64,
        count: i64,
    ) -> (Tensor, Tensor) {
        self.f_batch_norm_gather_stats(
            mean,
            invstd,
            running_mean,
            running_var,
            momentum,
            eps,
            count,
        )
        .unwrap()
    }

    pub fn batch_norm_gather_stats_with_counts<T: Borrow<Tensor>>(
        &self,
        mean: &Tensor,
        invstd: &Tensor,
        running_mean: Option<T>,
        running_var: Option<T>,
        momentum: f64,
        eps: f64,
        counts: &[i64],
    ) -> (Tensor, Tensor) {
        self.f_batch_norm_gather_stats_with_counts(
            mean,
            invstd,
            running_mean,
            running_var,
            momentum,
            eps,
            counts,
        )
        .unwrap()
    }

    pub fn batch_norm_stats(&self, eps: f64) -> (Tensor, Tensor) {
        self.f_batch_norm_stats(eps).unwrap()
    }

    pub fn batch_norm_update_stats<T: Borrow<Tensor>>(
        &self,
        running_mean: Option<T>,
        running_var: Option<T>,
        momentum: f64,
    ) -> (Tensor, Tensor) {
        self.f_batch_norm_update_stats(running_mean, running_var, momentum)
            .unwrap()
    }

    pub fn bernoulli(&self) -> Tensor {
        self.f_bernoulli().unwrap()
    }

    pub fn bernoulli1(&self, p: f64) -> Tensor {
        self.f_bernoulli1(p).unwrap()
    }

    pub fn bernoulli_(&mut self, p: &Tensor) -> Tensor {
        self.f_bernoulli_(p).unwrap()
    }

    pub fn bernoulli_1(&mut self, p: f64) -> Tensor {
        self.f_bernoulli_1(p).unwrap()
    }

    pub fn bernoulli_out(&self, out: &Tensor) -> Tensor {
        self.f_bernoulli_out(out).unwrap()
    }

    pub fn bilinear<T: Borrow<Tensor>>(
        input1: &Tensor,
        input2: &Tensor,
        weight: &Tensor,
        bias: Option<T>,
    ) -> Tensor {
        Tensor::f_bilinear(input1, input2, weight, bias).unwrap()
    }

    pub fn binary_cross_entropy<T: Borrow<Tensor>>(
        &self,
        target: &Tensor,
        weight: Option<T>,
        reduction: crate::Reduction,
    ) -> Tensor {
        self.f_binary_cross_entropy(target, weight, reduction)
            .unwrap()
    }

    pub fn binary_cross_entropy_backward<T: Borrow<Tensor>>(
        &self,
        grad_output: &Tensor,
        target: &Tensor,
        weight: Option<T>,
        reduction: crate::Reduction,
    ) -> Tensor {
        self.f_binary_cross_entropy_backward(grad_output, target, weight, reduction)
            .unwrap()
    }

    pub fn binary_cross_entropy_backward_out<T: Borrow<Tensor>>(
        &self,
        grad_input: &Tensor,
        grad_output: &Tensor,
        target: &Tensor,
        weight: Option<T>,
        reduction: crate::Reduction,
    ) -> Tensor {
        self.f_binary_cross_entropy_backward_out(grad_input, grad_output, target, weight, reduction)
            .unwrap()
    }

    pub fn binary_cross_entropy_out<T: Borrow<Tensor>>(
        &self,
        out: &Tensor,
        target: &Tensor,
        weight: Option<T>,
        reduction: crate::Reduction,
    ) -> Tensor {
        self.f_binary_cross_entropy_out(out, target, weight, reduction)
            .unwrap()
    }

    pub fn binary_cross_entropy_with_logits<T: Borrow<Tensor>>(
        &self,
        target: &Tensor,
        weight: Option<T>,
        pos_weight: Option<T>,
        reduction: crate::Reduction,
    ) -> Tensor {
        self.f_binary_cross_entropy_with_logits(target, weight, pos_weight, reduction)
            .unwrap()
    }

    pub fn binary_cross_entropy_with_logits_backward<T: Borrow<Tensor>>(
        &self,
        grad_output: &Tensor,
        target: &Tensor,
        weight: Option<T>,
        pos_weight: Option<T>,
        reduction: crate::Reduction,
    ) -> Tensor {
        self.f_binary_cross_entropy_with_logits_backward(
            grad_output,
            target,
            weight,
            pos_weight,
            reduction,
        )
        .unwrap()
    }

    pub fn bincount<T: Borrow<Tensor>>(&self, weights: Option<T>, minlength: i64) -> Tensor {
        self.f_bincount(weights, minlength).unwrap()
    }

    pub fn bitwise_not(&self) -> Tensor {
        self.f_bitwise_not().unwrap()
    }

    pub fn bitwise_not_(&mut self) -> Tensor {
        self.f_bitwise_not_().unwrap()
    }

    pub fn bitwise_not_out(&self, out: &Tensor) -> Tensor {
        self.f_bitwise_not_out(out).unwrap()
    }

    pub fn bitwise_xor<S: Into<Scalar>>(&self, other: S) -> Tensor {
        self.f_bitwise_xor(other).unwrap()
    }

    pub fn bitwise_xor1(&self, other: &Tensor) -> Tensor {
        self.f_bitwise_xor1(other).unwrap()
    }

    pub fn bitwise_xor_<S: Into<Scalar>>(&mut self, other: S) -> Tensor {
        self.f_bitwise_xor_(other).unwrap()
    }

    pub fn bitwise_xor_1(&mut self, other: &Tensor) -> Tensor {
        self.f_bitwise_xor_1(other).unwrap()
    }

    pub fn bitwise_xor_out(&self, out: &Tensor, other: &Tensor) -> Tensor {
        self.f_bitwise_xor_out(out, other).unwrap()
    }

    pub fn bitwise_xor_out1<S: Into<Scalar>>(&self, out: &Tensor, other: S) -> Tensor {
        self.f_bitwise_xor_out1(out, other).unwrap()
    }

    pub fn blackman_window(window_length: i64, options: (Kind, Device)) -> Tensor {
        Tensor::f_blackman_window(window_length, options).unwrap()
    }

    pub fn blackman_window1(window_length: i64, periodic: bool, options: (Kind, Device)) -> Tensor {
        Tensor::f_blackman_window1(window_length, periodic, options).unwrap()
    }

    pub fn bmm(&self, mat2: &Tensor) -> Tensor {
        self.f_bmm(mat2).unwrap()
    }

    pub fn bmm_out(&self, out: &Tensor, mat2: &Tensor) -> Tensor {
        self.f_bmm_out(out, mat2).unwrap()
    }

    pub fn broadcast_tensors<T: Borrow<Tensor>>(tensors: &[T]) -> Vec<Tensor> {
        Tensor::f_broadcast_tensors(tensors).unwrap()
    }

    pub fn cartesian_prod<T: Borrow<Tensor>>(tensors: &[T]) -> Tensor {
        Tensor::f_cartesian_prod(tensors).unwrap()
    }

    pub fn cat<T: Borrow<Tensor>>(tensors: &[T], dim: i64) -> Tensor {
        Tensor::f_cat(tensors, dim).unwrap()
    }

    pub fn cat_out<T: Borrow<Tensor>>(out: &Tensor, tensors: &[T], dim: i64) -> Tensor {
        Tensor::f_cat_out(out, tensors, dim).unwrap()
    }

    pub fn cauchy_(&mut self, median: f64, sigma: f64) -> Tensor {
        self.f_cauchy_(median, sigma).unwrap()
    }

    pub fn cdist(x1: &Tensor, x2: &Tensor, p: f64, compute_mode: i64) -> Tensor {
        Tensor::f_cdist(x1, x2, p, compute_mode).unwrap()
    }

    pub fn ceil(&self) -> Tensor {
        self.f_ceil().unwrap()
    }

    pub fn ceil_(&mut self) -> Tensor {
        self.f_ceil_().unwrap()
    }

    pub fn ceil_out(&self, out: &Tensor) -> Tensor {
        self.f_ceil_out(out).unwrap()
    }

    pub fn celu(&self) -> Tensor {
        self.f_celu().unwrap()
    }

    pub fn celu_(&mut self) -> Tensor {
        self.f_celu_().unwrap()
    }

    pub fn chain_matmul<T: Borrow<Tensor>>(matrices: &[T]) -> Tensor {
        Tensor::f_chain_matmul(matrices).unwrap()
    }

    pub fn cholesky(&self, upper: bool) -> Tensor {
        self.f_cholesky(upper).unwrap()
    }

    pub fn cholesky_inverse(&self, upper: bool) -> Tensor {
        self.f_cholesky_inverse(upper).unwrap()
    }

    pub fn cholesky_inverse_out(&self, out: &Tensor, upper: bool) -> Tensor {
        self.f_cholesky_inverse_out(out, upper).unwrap()
    }

    pub fn cholesky_out(&self, out: &Tensor, upper: bool) -> Tensor {
        self.f_cholesky_out(out, upper).unwrap()
    }

    pub fn cholesky_solve(&self, input2: &Tensor, upper: bool) -> Tensor {
        self.f_cholesky_solve(input2, upper).unwrap()
    }

    pub fn cholesky_solve_out(&self, out: &Tensor, input2: &Tensor, upper: bool) -> Tensor {
        self.f_cholesky_solve_out(out, input2, upper).unwrap()
    }

    pub fn chunk(&self, chunks: i64, dim: i64) -> Vec<Tensor> {
        self.f_chunk(chunks, dim).unwrap()
    }

    pub fn clamp<S: Into<Scalar>>(&self, min: S, max: S) -> Tensor {
        self.f_clamp(min, max).unwrap()
    }

    pub fn clamp_<S: Into<Scalar>>(&mut self, min: S, max: S) -> Tensor {
        self.f_clamp_(min, max).unwrap()
    }

    pub fn clamp_max<S: Into<Scalar>>(&self, max: S) -> Tensor {
        self.f_clamp_max(max).unwrap()
    }

    pub fn clamp_max_<S: Into<Scalar>>(&mut self, max: S) -> Tensor {
        self.f_clamp_max_(max).unwrap()
    }

    pub fn clamp_max_out<S: Into<Scalar>>(&self, out: &Tensor, max: S) -> Tensor {
        self.f_clamp_max_out(out, max).unwrap()
    }

    pub fn clamp_min<S: Into<Scalar>>(&self, min: S) -> Tensor {
        self.f_clamp_min(min).unwrap()
    }

    pub fn clamp_min_<S: Into<Scalar>>(&mut self, min: S) -> Tensor {
        self.f_clamp_min_(min).unwrap()
    }

    pub fn clamp_min_out<S: Into<Scalar>>(&self, out: &Tensor, min: S) -> Tensor {
        self.f_clamp_min_out(out, min).unwrap()
    }

    pub fn clamp_out<S: Into<Scalar>>(&self, out: &Tensor, min: S, max: S) -> Tensor {
        self.f_clamp_out(out, min, max).unwrap()
    }

    pub fn coalesce(&self) -> Tensor {
        self.f_coalesce().unwrap()
    }

    pub fn col2im(
        &self,
        output_size: &[i64],
        kernel_size: &[i64],
        dilation: &[i64],
        padding: &[i64],
        stride: &[i64],
    ) -> Tensor {
        self.f_col2im(output_size, kernel_size, dilation, padding, stride)
            .unwrap()
    }

    pub fn col2im_backward(
        grad_output: &Tensor,
        kernel_size: &[i64],
        dilation: &[i64],
        padding: &[i64],
        stride: &[i64],
    ) -> Tensor {
        Tensor::f_col2im_backward(grad_output, kernel_size, dilation, padding, stride).unwrap()
    }

    pub fn col2im_backward_out(
        grad_input: &Tensor,
        grad_output: &Tensor,
        kernel_size: &[i64],
        dilation: &[i64],
        padding: &[i64],
        stride: &[i64],
    ) -> Tensor {
        Tensor::f_col2im_backward_out(
            grad_input,
            grad_output,
            kernel_size,
            dilation,
            padding,
            stride,
        )
        .unwrap()
    }

    pub fn col2im_out(
        &self,
        out: &Tensor,
        output_size: &[i64],
        kernel_size: &[i64],
        dilation: &[i64],
        padding: &[i64],
        stride: &[i64],
    ) -> Tensor {
        self.f_col2im_out(out, output_size, kernel_size, dilation, padding, stride)
            .unwrap()
    }

    pub fn combinations(&self, r: i64, with_replacement: bool) -> Tensor {
        self.f_combinations(r, with_replacement).unwrap()
    }

    pub fn conj(&self) -> Tensor {
        self.f_conj().unwrap()
    }

    pub fn conj_out(&self, out: &Tensor) -> Tensor {
        self.f_conj_out(out).unwrap()
    }

    pub fn constant_pad_nd(&self, pad: &[i64]) -> Tensor {
        self.f_constant_pad_nd(pad).unwrap()
    }

    pub fn contiguous(&self) -> Tensor {
        self.f_contiguous().unwrap()
    }

    pub fn conv1d<T: Borrow<Tensor>>(
        &self,
        weight: &Tensor,
        bias: Option<T>,
        stride: &[i64],
        padding: &[i64],
        dilation: &[i64],
        groups: i64,
    ) -> Tensor {
        self.f_conv1d(weight, bias, stride, padding, dilation, groups)
            .unwrap()
    }

    pub fn conv2d<T: Borrow<Tensor>>(
        &self,
        weight: &Tensor,
        bias: Option<T>,
        stride: &[i64],
        padding: &[i64],
        dilation: &[i64],
        groups: i64,
    ) -> Tensor {
        self.f_conv2d(weight, bias, stride, padding, dilation, groups)
            .unwrap()
    }

    pub fn conv3d<T: Borrow<Tensor>>(
        &self,
        weight: &Tensor,
        bias: Option<T>,
        stride: &[i64],
        padding: &[i64],
        dilation: &[i64],
        groups: i64,
    ) -> Tensor {
        self.f_conv3d(weight, bias, stride, padding, dilation, groups)
            .unwrap()
    }

    pub fn conv_tbc(&self, weight: &Tensor, bias: &Tensor, pad: i64) -> Tensor {
        self.f_conv_tbc(weight, bias, pad).unwrap()
    }

    pub fn conv_tbc_backward(
        &self,
        input: &Tensor,
        weight: &Tensor,
        bias: &Tensor,
        pad: i64,
    ) -> (Tensor, Tensor, Tensor) {
        self.f_conv_tbc_backward(input, weight, bias, pad).unwrap()
    }

    pub fn conv_transpose1d<T: Borrow<Tensor>>(
        &self,
        weight: &Tensor,
        bias: Option<T>,
        stride: &[i64],
        padding: &[i64],
        output_padding: &[i64],
        groups: i64,
        dilation: &[i64],
    ) -> Tensor {
        self.f_conv_transpose1d(
            weight,
            bias,
            stride,
            padding,
            output_padding,
            groups,
            dilation,
        )
        .unwrap()
    }

    pub fn conv_transpose2d<T: Borrow<Tensor>>(
        &self,
        weight: &Tensor,
        bias: Option<T>,
        stride: &[i64],
        padding: &[i64],
        output_padding: &[i64],
        groups: i64,
        dilation: &[i64],
    ) -> Tensor {
        self.f_conv_transpose2d(
            weight,
            bias,
            stride,
            padding,
            output_padding,
            groups,
            dilation,
        )
        .unwrap()
    }

    pub fn conv_transpose3d<T: Borrow<Tensor>>(
        &self,
        weight: &Tensor,
        bias: Option<T>,
        stride: &[i64],
        padding: &[i64],
        output_padding: &[i64],
        groups: i64,
        dilation: &[i64],
    ) -> Tensor {
        self.f_conv_transpose3d(
            weight,
            bias,
            stride,
            padding,
            output_padding,
            groups,
            dilation,
        )
        .unwrap()
    }

    pub fn convolution<T: Borrow<Tensor>>(
        &self,
        weight: &Tensor,
        bias: Option<T>,
        stride: &[i64],
        padding: &[i64],
        dilation: &[i64],
        transposed: bool,
        output_padding: &[i64],
        groups: i64,
    ) -> Tensor {
        self.f_convolution(
            weight,
            bias,
            stride,
            padding,
            dilation,
            transposed,
            output_padding,
            groups,
        )
        .unwrap()
    }

    pub fn convolution_overrideable<T: Borrow<Tensor>>(
        &self,
        weight: &Tensor,
        bias: Option<T>,
        stride: &[i64],
        padding: &[i64],
        dilation: &[i64],
        transposed: bool,
        output_padding: &[i64],
        groups: i64,
    ) -> Tensor {
        self.f_convolution_overrideable(
            weight,
            bias,
            stride,
            padding,
            dilation,
            transposed,
            output_padding,
            groups,
        )
        .unwrap()
    }

    pub fn copy_sparse_to_sparse_(&mut self, src: &Tensor, non_blocking: bool) -> Tensor {
        self.f_copy_sparse_to_sparse_(src, non_blocking).unwrap()
    }

    pub fn cos(&self) -> Tensor {
        self.f_cos().unwrap()
    }

    pub fn cos_(&mut self) -> Tensor {
        self.f_cos_().unwrap()
    }

    pub fn cos_out(&self, out: &Tensor) -> Tensor {
        self.f_cos_out(out).unwrap()
    }

    pub fn cosh(&self) -> Tensor {
        self.f_cosh().unwrap()
    }

    pub fn cosh_(&mut self) -> Tensor {
        self.f_cosh_().unwrap()
    }

    pub fn cosh_out(&self, out: &Tensor) -> Tensor {
        self.f_cosh_out(out).unwrap()
    }

    pub fn cosine_embedding_loss(
        input1: &Tensor,
        input2: &Tensor,
        target: &Tensor,
        margin: f64,
        reduction: crate::Reduction,
    ) -> Tensor {
        Tensor::f_cosine_embedding_loss(input1, input2, target, margin, reduction).unwrap()
    }

    pub fn cosine_similarity(x1: &Tensor, x2: &Tensor, dim: i64, eps: f64) -> Tensor {
        Tensor::f_cosine_similarity(x1, x2, dim, eps).unwrap()
    }

    pub fn cross(&self, other: &Tensor, dim: i64) -> Tensor {
        self.f_cross(other, dim).unwrap()
    }

    pub fn cross_out(&self, out: &Tensor, other: &Tensor, dim: i64) -> Tensor {
        self.f_cross_out(out, other, dim).unwrap()
    }

    pub fn ctc_loss(
        log_probs: &Tensor,
        targets: &Tensor,
        input_lengths: &[i64],
        target_lengths: &[i64],
        blank: i64,
        reduction: crate::Reduction,
        zero_infinity: bool,
    ) -> Tensor {
        Tensor::f_ctc_loss(
            log_probs,
            targets,
            input_lengths,
            target_lengths,
            blank,
            reduction,
            zero_infinity,
        )
        .unwrap()
    }

    pub fn ctc_loss1(
        log_probs: &Tensor,
        targets: &Tensor,
        input_lengths: &Tensor,
        target_lengths: &Tensor,
        blank: i64,
        reduction: crate::Reduction,
        zero_infinity: bool,
    ) -> Tensor {
        Tensor::f_ctc_loss1(
            log_probs,
            targets,
            input_lengths,
            target_lengths,
            blank,
            reduction,
            zero_infinity,
        )
        .unwrap()
    }

    pub fn cudnn_affine_grid_generator(theta: &Tensor, n: i64, c: i64, h: i64, w: i64) -> Tensor {
        Tensor::f_cudnn_affine_grid_generator(theta, n, c, h, w).unwrap()
    }

    pub fn cudnn_affine_grid_generator_backward(
        grad: &Tensor,
        n: i64,
        c: i64,
        h: i64,
        w: i64,
    ) -> Tensor {
        Tensor::f_cudnn_affine_grid_generator_backward(grad, n, c, h, w).unwrap()
    }

    pub fn cudnn_batch_norm<T: Borrow<Tensor>>(
        &self,
        weight: &Tensor,
        bias: Option<T>,
        running_mean: Option<T>,
        running_var: Option<T>,
        training: bool,
        exponential_average_factor: f64,
        epsilon: f64,
    ) -> (Tensor, Tensor, Tensor, Tensor) {
        self.f_cudnn_batch_norm(
            weight,
            bias,
            running_mean,
            running_var,
            training,
            exponential_average_factor,
            epsilon,
        )
        .unwrap()
    }

    pub fn cudnn_batch_norm_backward<T: Borrow<Tensor>>(
        &self,
        grad_output: &Tensor,
        weight: &Tensor,
        running_mean: Option<T>,
        running_var: Option<T>,
        save_mean: Option<T>,
        save_var: Option<T>,
        epsilon: f64,
        reservespace: &Tensor,
    ) -> (Tensor, Tensor, Tensor) {
        self.f_cudnn_batch_norm_backward(
            grad_output,
            weight,
            running_mean,
            running_var,
            save_mean,
            save_var,
            epsilon,
            reservespace,
        )
        .unwrap()
    }

    pub fn cudnn_convolution<T: Borrow<Tensor>>(
        &self,
        weight: &Tensor,
        bias: Option<T>,
        padding: &[i64],
        stride: &[i64],
        dilation: &[i64],
        groups: i64,
        benchmark: bool,
        deterministic: bool,
    ) -> Tensor {
        self.f_cudnn_convolution(
            weight,
            bias,
            padding,
            stride,
            dilation,
            groups,
            benchmark,
            deterministic,
        )
        .unwrap()
    }

    pub fn cudnn_convolution_backward_bias(grad_output: &Tensor) -> Tensor {
        Tensor::f_cudnn_convolution_backward_bias(grad_output).unwrap()
    }

    pub fn cudnn_convolution_backward_input(
        self_size: &[i64],
        grad_output: &Tensor,
        weight: &Tensor,
        padding: &[i64],
        stride: &[i64],
        dilation: &[i64],
        groups: i64,
        benchmark: bool,
        deterministic: bool,
    ) -> Tensor {
        Tensor::f_cudnn_convolution_backward_input(
            self_size,
            grad_output,
            weight,
            padding,
            stride,
            dilation,
            groups,
            benchmark,
            deterministic,
        )
        .unwrap()
    }

    pub fn cudnn_convolution_backward_weight(
        &self,
        weight_size: &[i64],
        grad_output: &Tensor,
        padding: &[i64],
        stride: &[i64],
        dilation: &[i64],
        groups: i64,
        benchmark: bool,
        deterministic: bool,
    ) -> Tensor {
        self.f_cudnn_convolution_backward_weight(
            weight_size,
            grad_output,
            padding,
            stride,
            dilation,
            groups,
            benchmark,
            deterministic,
        )
        .unwrap()
    }

    pub fn cudnn_convolution_transpose<T: Borrow<Tensor>>(
        &self,
        weight: &Tensor,
        bias: Option<T>,
        padding: &[i64],
        output_padding: &[i64],
        stride: &[i64],
        dilation: &[i64],
        groups: i64,
        benchmark: bool,
        deterministic: bool,
    ) -> Tensor {
        self.f_cudnn_convolution_transpose(
            weight,
            bias,
            padding,
            output_padding,
            stride,
            dilation,
            groups,
            benchmark,
            deterministic,
        )
        .unwrap()
    }

    pub fn cudnn_convolution_transpose_backward_bias(grad_output: &Tensor) -> Tensor {
        Tensor::f_cudnn_convolution_transpose_backward_bias(grad_output).unwrap()
    }

    pub fn cudnn_convolution_transpose_backward_input(
        grad_output: &Tensor,
        weight: &Tensor,
        padding: &[i64],
        stride: &[i64],
        dilation: &[i64],
        groups: i64,
        benchmark: bool,
        deterministic: bool,
    ) -> Tensor {
        Tensor::f_cudnn_convolution_transpose_backward_input(
            grad_output,
            weight,
            padding,
            stride,
            dilation,
            groups,
            benchmark,
            deterministic,
        )
        .unwrap()
    }

    pub fn cudnn_convolution_transpose_backward_weight(
        &self,
        weight_size: &[i64],
        grad_output: &Tensor,
        padding: &[i64],
        stride: &[i64],
        dilation: &[i64],
        groups: i64,
        benchmark: bool,
        deterministic: bool,
    ) -> Tensor {
        self.f_cudnn_convolution_transpose_backward_weight(
            weight_size,
            grad_output,
            padding,
            stride,
            dilation,
            groups,
            benchmark,
            deterministic,
        )
        .unwrap()
    }

    pub fn cudnn_grid_sampler(&self, grid: &Tensor) -> Tensor {
        self.f_cudnn_grid_sampler(grid).unwrap()
    }

    pub fn cudnn_grid_sampler_backward(
        &self,
        grid: &Tensor,
        grad_output: &Tensor,
    ) -> (Tensor, Tensor) {
        self.f_cudnn_grid_sampler_backward(grid, grad_output)
            .unwrap()
    }

    pub fn cumprod(&self, dim: i64, dtype: Kind) -> Tensor {
        self.f_cumprod(dim, dtype).unwrap()
    }

    pub fn cumprod_out(&self, out: &Tensor, dim: i64, dtype: Kind) -> Tensor {
        self.f_cumprod_out(out, dim, dtype).unwrap()
    }

    pub fn cumsum(&self, dim: i64, dtype: Kind) -> Tensor {
        self.f_cumsum(dim, dtype).unwrap()
    }

    pub fn cumsum_out(&self, out: &Tensor, dim: i64, dtype: Kind) -> Tensor {
        self.f_cumsum_out(out, dim, dtype).unwrap()
    }

    pub fn data(&self) -> Tensor {
        self.f_data().unwrap()
    }

    pub fn dequantize(&self) -> Tensor {
        self.f_dequantize().unwrap()
    }

    pub fn det(&self) -> Tensor {
        self.f_det().unwrap()
    }

    pub fn detach(&self) -> Tensor {
        self.f_detach().unwrap()
    }

    pub fn detach_(&mut self) -> Tensor {
        self.f_detach_().unwrap()
    }

    pub fn diag(&self, diagonal: i64) -> Tensor {
        self.f_diag(diagonal).unwrap()
    }

    pub fn diag_embed(&self, offset: i64, dim1: i64, dim2: i64) -> Tensor {
        self.f_diag_embed(offset, dim1, dim2).unwrap()
    }

    pub fn diag_out(&self, out: &Tensor, diagonal: i64) -> Tensor {
        self.f_diag_out(out, diagonal).unwrap()
    }

    pub fn diagflat(&self, offset: i64) -> Tensor {
        self.f_diagflat(offset).unwrap()
    }

    pub fn diagonal(&self, offset: i64, dim1: i64, dim2: i64) -> Tensor {
        self.f_diagonal(offset, dim1, dim2).unwrap()
    }

    pub fn digamma(&self) -> Tensor {
        self.f_digamma().unwrap()
    }

    pub fn digamma_(&mut self) -> Tensor {
        self.f_digamma_().unwrap()
    }

    pub fn digamma_out(&self, out: &Tensor) -> Tensor {
        self.f_digamma_out(out).unwrap()
    }

    pub fn dist(&self, other: &Tensor) -> Tensor {
        self.f_dist(other).unwrap()
    }

    pub fn g_div(&self, other: &Tensor) -> Tensor {
        self.f_div(other).unwrap()
    }

    pub fn g_div1<S: Into<Scalar>>(&self, other: S) -> Tensor {
        self.f_div1(other).unwrap()
    }

    pub fn g_div_(&mut self, other: &Tensor) -> Tensor {
        self.f_div_(other).unwrap()
    }

    pub fn g_div_1<S: Into<Scalar>>(&mut self, other: S) -> Tensor {
        self.f_div_1(other).unwrap()
    }

    pub fn div_out(&self, out: &Tensor, other: &Tensor) -> Tensor {
        self.f_div_out(out, other).unwrap()
    }

    pub fn dot(&self, tensor: &Tensor) -> Tensor {
        self.f_dot(tensor).unwrap()
    }

    pub fn dot_out(&self, out: &Tensor, tensor: &Tensor) -> Tensor {
        self.f_dot_out(out, tensor).unwrap()
    }

    pub fn dropout(&self, p: f64, train: bool) -> Tensor {
        self.f_dropout(p, train).unwrap()
    }

    pub fn dropout_(&mut self, p: f64, train: bool) -> Tensor {
        self.f_dropout_(p, train).unwrap()
    }

    pub fn eig(&self, eigenvectors: bool) -> (Tensor, Tensor) {
        self.f_eig(eigenvectors).unwrap()
    }

    pub fn eig_out(&self, e: &Tensor, v: &Tensor, eigenvectors: bool) -> (Tensor, Tensor) {
        self.f_eig_out(e, v, eigenvectors).unwrap()
    }

    pub fn elu(&self) -> Tensor {
        self.f_elu().unwrap()
    }

    pub fn elu_(&mut self) -> Tensor {
        self.f_elu_().unwrap()
    }

    pub fn elu_backward<S: Into<Scalar>>(
        grad_output: &Tensor,
        alpha: S,
        scale: S,
        input_scale: S,
        output: &Tensor,
    ) -> Tensor {
        Tensor::f_elu_backward(grad_output, alpha, scale, input_scale, output).unwrap()
    }

    pub fn elu_backward_out<S: Into<Scalar>>(
        grad_input: &Tensor,
        grad_output: &Tensor,
        alpha: S,
        scale: S,
        input_scale: S,
        output: &Tensor,
    ) -> Tensor {
        Tensor::f_elu_backward_out(grad_input, grad_output, alpha, scale, input_scale, output)
            .unwrap()
    }

    pub fn elu_out(&self, out: &Tensor) -> Tensor {
        self.f_elu_out(out).unwrap()
    }

    pub fn embedding(
        weight: &Tensor,
        indices: &Tensor,
        padding_idx: i64,
        scale_grad_by_freq: bool,
        sparse: bool,
    ) -> Tensor {
        Tensor::f_embedding(weight, indices, padding_idx, scale_grad_by_freq, sparse).unwrap()
    }

    pub fn embedding_backward(
        grad: &Tensor,
        indices: &Tensor,
        num_weights: i64,
        padding_idx: i64,
        scale_grad_by_freq: bool,
        sparse: bool,
    ) -> Tensor {
        Tensor::f_embedding_backward(
            grad,
            indices,
            num_weights,
            padding_idx,
            scale_grad_by_freq,
            sparse,
        )
        .unwrap()
    }

    pub fn embedding_bag<T: Borrow<Tensor>>(
        weight: &Tensor,
        indices: &Tensor,
        offsets: &Tensor,
        scale_grad_by_freq: bool,
        mode: i64,
        sparse: bool,
        per_sample_weights: Option<T>,
    ) -> (Tensor, Tensor, Tensor, Tensor) {
        Tensor::f_embedding_bag(
            weight,
            indices,
            offsets,
            scale_grad_by_freq,
            mode,
            sparse,
            per_sample_weights,
        )
        .unwrap()
    }

    pub fn embedding_dense_backward(
        grad_output: &Tensor,
        indices: &Tensor,
        num_weights: i64,
        padding_idx: i64,
        scale_grad_by_freq: bool,
    ) -> Tensor {
        Tensor::f_embedding_dense_backward(
            grad_output,
            indices,
            num_weights,
            padding_idx,
            scale_grad_by_freq,
        )
        .unwrap()
    }

    pub fn embedding_renorm_(&mut self, indices: &Tensor, max_norm: f64, norm_type: f64) -> Tensor {
        self.f_embedding_renorm_(indices, max_norm, norm_type)
            .unwrap()
    }

    pub fn embedding_sparse_backward(
        grad: &Tensor,
        indices: &Tensor,
        num_weights: i64,
        padding_idx: i64,
        scale_grad_by_freq: bool,
    ) -> Tensor {
        Tensor::f_embedding_sparse_backward(
            grad,
            indices,
            num_weights,
            padding_idx,
            scale_grad_by_freq,
        )
        .unwrap()
    }

    pub fn empty(size: &[i64], options: (Kind, Device)) -> Tensor {
        Tensor::f_empty(size, options).unwrap()
    }

    pub fn empty_like(&self) -> Tensor {
        self.f_empty_like().unwrap()
    }

    pub fn empty_like1(&self, options: (Kind, Device)) -> Tensor {
        self.f_empty_like1(options).unwrap()
    }

    pub fn empty_out(out: &Tensor, size: &[i64]) -> Tensor {
        Tensor::f_empty_out(out, size).unwrap()
    }

    pub fn empty_strided(size: &[i64], stride: &[i64], options: (Kind, Device)) -> Tensor {
        Tensor::f_empty_strided(size, stride, options).unwrap()
    }

    pub fn eq<S: Into<Scalar>>(&self, other: S) -> Tensor {
        self.f_eq(other).unwrap()
    }

    pub fn eq1(&self, other: &Tensor) -> Tensor {
        self.f_eq1(other).unwrap()
    }

    pub fn eq_<S: Into<Scalar>>(&mut self, other: S) -> Tensor {
        self.f_eq_(other).unwrap()
    }

    pub fn eq_1(&mut self, other: &Tensor) -> Tensor {
        self.f_eq_1(other).unwrap()
    }

    pub fn eq_out<S: Into<Scalar>>(&self, out: &Tensor, other: S) -> Tensor {
        self.f_eq_out(out, other).unwrap()
    }

    pub fn eq_out1(&self, out: &Tensor, other: &Tensor) -> Tensor {
        self.f_eq_out1(out, other).unwrap()
    }

    pub fn erf(&self) -> Tensor {
        self.f_erf().unwrap()
    }

    pub fn erf_(&mut self) -> Tensor {
        self.f_erf_().unwrap()
    }

    pub fn erf_out(&self, out: &Tensor) -> Tensor {
        self.f_erf_out(out).unwrap()
    }

    pub fn erfc(&self) -> Tensor {
        self.f_erfc().unwrap()
    }

    pub fn erfc_(&mut self) -> Tensor {
        self.f_erfc_().unwrap()
    }

    pub fn erfc_out(&self, out: &Tensor) -> Tensor {
        self.f_erfc_out(out).unwrap()
    }

    pub fn erfinv(&self) -> Tensor {
        self.f_erfinv().unwrap()
    }

    pub fn erfinv_(&mut self) -> Tensor {
        self.f_erfinv_().unwrap()
    }

    pub fn erfinv_out(&self, out: &Tensor) -> Tensor {
        self.f_erfinv_out(out).unwrap()
    }

    pub fn exp(&self) -> Tensor {
        self.f_exp().unwrap()
    }

    pub fn exp_(&mut self) -> Tensor {
        self.f_exp_().unwrap()
    }

    pub fn exp_out(&self, out: &Tensor) -> Tensor {
        self.f_exp_out(out).unwrap()
    }

    pub fn expand(&self, size: &[i64], implicit: bool) -> Tensor {
        self.f_expand(size, implicit).unwrap()
    }

    pub fn expand_as(&self, other: &Tensor) -> Tensor {
        self.f_expand_as(other).unwrap()
    }

    pub fn expm1(&self) -> Tensor {
        self.f_expm1().unwrap()
    }

    pub fn expm1_(&mut self) -> Tensor {
        self.f_expm1_().unwrap()
    }

    pub fn expm1_out(&self, out: &Tensor) -> Tensor {
        self.f_expm1_out(out).unwrap()
    }

    pub fn exponential_(&mut self, lambd: f64) -> Tensor {
        self.f_exponential_(lambd).unwrap()
    }

    pub fn eye(n: i64, options: (Kind, Device)) -> Tensor {
        Tensor::f_eye(n, options).unwrap()
    }

    pub fn eye1(n: i64, m: i64, options: (Kind, Device)) -> Tensor {
        Tensor::f_eye1(n, m, options).unwrap()
    }

    pub fn eye_out(out: &Tensor, n: i64) -> Tensor {
        Tensor::f_eye_out(out, n).unwrap()
    }

    pub fn eye_out1(out: &Tensor, n: i64, m: i64) -> Tensor {
        Tensor::f_eye_out1(out, n, m).unwrap()
    }

    pub fn fake_quantize_per_channel_affine(
        &self,
        scale: &Tensor,
        zero_point: &Tensor,
        axis: i64,
        quant_min: i64,
        quant_max: i64,
    ) -> Tensor {
        self.f_fake_quantize_per_channel_affine(scale, zero_point, axis, quant_min, quant_max)
            .unwrap()
    }

    pub fn fake_quantize_per_channel_affine_backward(
        &self,
        grad: &Tensor,
        scale: &Tensor,
        zero_point: &Tensor,
        axis: i64,
        quant_min: i64,
        quant_max: i64,
    ) -> Tensor {
        self.f_fake_quantize_per_channel_affine_backward(
            grad, scale, zero_point, axis, quant_min, quant_max,
        )
        .unwrap()
    }

    pub fn fake_quantize_per_tensor_affine(
        &self,
        scale: f64,
        zero_point: i64,
        quant_min: i64,
        quant_max: i64,
    ) -> Tensor {
        self.f_fake_quantize_per_tensor_affine(scale, zero_point, quant_min, quant_max)
            .unwrap()
    }

    pub fn fake_quantize_per_tensor_affine_backward(
        &self,
        grad: &Tensor,
        scale: f64,
        zero_point: i64,
        quant_min: i64,
        quant_max: i64,
    ) -> Tensor {
        self.f_fake_quantize_per_tensor_affine_backward(
            grad, scale, zero_point, quant_min, quant_max,
        )
        .unwrap()
    }

    pub fn fbgemm_linear_fp16_weight(&self, packed_weight: &Tensor, bias: &Tensor) -> Tensor {
        self.f_fbgemm_linear_fp16_weight(packed_weight, bias)
            .unwrap()
    }

    pub fn fbgemm_linear_fp16_weight_fp32_activation(
        &self,
        packed_weight: &Tensor,
        bias: &Tensor,
    ) -> Tensor {
        self.f_fbgemm_linear_fp16_weight_fp32_activation(packed_weight, bias)
            .unwrap()
    }

    pub fn fbgemm_linear_int8_weight<S: Into<Scalar>>(
        &self,
        weight: &Tensor,
        packed: &Tensor,
        col_offsets: &Tensor,
        weight_scale: S,
        weight_zero_point: S,
        bias: &Tensor,
    ) -> Tensor {
        self.f_fbgemm_linear_int8_weight(
            weight,
            packed,
            col_offsets,
            weight_scale,
            weight_zero_point,
            bias,
        )
        .unwrap()
    }

    pub fn fbgemm_linear_int8_weight_fp32_activation<S: Into<Scalar>>(
        &self,
        weight: &Tensor,
        packed: &Tensor,
        col_offsets: &Tensor,
        weight_scale: S,
        weight_zero_point: S,
        bias: &Tensor,
    ) -> Tensor {
        self.f_fbgemm_linear_int8_weight_fp32_activation(
            weight,
            packed,
            col_offsets,
            weight_scale,
            weight_zero_point,
            bias,
        )
        .unwrap()
    }

    pub fn fbgemm_pack_gemm_matrix_fp16(&self) -> Tensor {
        self.f_fbgemm_pack_gemm_matrix_fp16().unwrap()
    }

    pub fn fbgemm_pack_quantized_matrix(&self) -> Tensor {
        self.f_fbgemm_pack_quantized_matrix().unwrap()
    }

    pub fn fbgemm_pack_quantized_matrix1(&self, k: i64, n: i64) -> Tensor {
        self.f_fbgemm_pack_quantized_matrix1(k, n).unwrap()
    }

    pub fn feature_alpha_dropout(&self, p: f64, train: bool) -> Tensor {
        self.f_feature_alpha_dropout(p, train).unwrap()
    }

    pub fn feature_alpha_dropout_(&mut self, p: f64, train: bool) -> Tensor {
        self.f_feature_alpha_dropout_(p, train).unwrap()
    }

    pub fn feature_dropout(&self, p: f64, train: bool) -> Tensor {
        self.f_feature_dropout(p, train).unwrap()
    }

    pub fn feature_dropout_(&mut self, p: f64, train: bool) -> Tensor {
        self.f_feature_dropout_(p, train).unwrap()
    }

    pub fn fft(&self, signal_ndim: i64, normalized: bool) -> Tensor {
        self.f_fft(signal_ndim, normalized).unwrap()
    }

    pub fn fill_<S: Into<Scalar>>(&mut self, value: S) -> Tensor {
        self.f_fill_(value).unwrap()
    }

    pub fn fill_1(&mut self, value: &Tensor) -> Tensor {
        self.f_fill_1(value).unwrap()
    }

    pub fn fill_diagonal_<S: Into<Scalar>>(&mut self, fill_value: S, wrap: bool) -> Tensor {
        self.f_fill_diagonal_(fill_value, wrap).unwrap()
    }

    pub fn flatten(&self, start_dim: i64, end_dim: i64) -> Tensor {
        self.f_flatten(start_dim, end_dim).unwrap()
    }

    pub fn flip(&self, dims: &[i64]) -> Tensor {
        self.f_flip(dims).unwrap()
    }

    pub fn floor(&self) -> Tensor {
        self.f_floor().unwrap()
    }

    pub fn floor_(&mut self) -> Tensor {
        self.f_floor_().unwrap()
    }

    pub fn floor_out(&self, out: &Tensor) -> Tensor {
        self.f_floor_out(out).unwrap()
    }

    pub fn fmod<S: Into<Scalar>>(&self, other: S) -> Tensor {
        self.f_fmod(other).unwrap()
    }

    pub fn fmod1(&self, other: &Tensor) -> Tensor {
        self.f_fmod1(other).unwrap()
    }

    pub fn fmod_<S: Into<Scalar>>(&mut self, other: S) -> Tensor {
        self.f_fmod_(other).unwrap()
    }

    pub fn fmod_1(&mut self, other: &Tensor) -> Tensor {
        self.f_fmod_1(other).unwrap()
    }

    pub fn fmod_out<S: Into<Scalar>>(&self, out: &Tensor, other: S) -> Tensor {
        self.f_fmod_out(out, other).unwrap()
    }

    pub fn fmod_out1(&self, out: &Tensor, other: &Tensor) -> Tensor {
        self.f_fmod_out1(out, other).unwrap()
    }

    pub fn frac(&self) -> Tensor {
        self.f_frac().unwrap()
    }

    pub fn frac_(&mut self) -> Tensor {
        self.f_frac_().unwrap()
    }

    pub fn frac_out(&self, out: &Tensor) -> Tensor {
        self.f_frac_out(out).unwrap()
    }

    pub fn fractional_max_pool2d(
        &self,
        kernel_size: &[i64],
        output_size: &[i64],
        random_samples: &Tensor,
    ) -> (Tensor, Tensor) {
        self.f_fractional_max_pool2d(kernel_size, output_size, random_samples)
            .unwrap()
    }

    pub fn fractional_max_pool2d_backward(
        &self,
        grad_output: &Tensor,
        kernel_size: &[i64],
        output_size: &[i64],
        indices: &Tensor,
    ) -> Tensor {
        self.f_fractional_max_pool2d_backward(grad_output, kernel_size, output_size, indices)
            .unwrap()
    }

    pub fn fractional_max_pool2d_backward_out(
        &self,
        grad_input: &Tensor,
        grad_output: &Tensor,
        kernel_size: &[i64],
        output_size: &[i64],
        indices: &Tensor,
    ) -> Tensor {
        self.f_fractional_max_pool2d_backward_out(
            grad_input,
            grad_output,
            kernel_size,
            output_size,
            indices,
        )
        .unwrap()
    }

    pub fn fractional_max_pool2d_out(
        &self,
        output: &Tensor,
        indices: &Tensor,
        kernel_size: &[i64],
        output_size: &[i64],
        random_samples: &Tensor,
    ) -> (Tensor, Tensor) {
        self.f_fractional_max_pool2d_out(output, indices, kernel_size, output_size, random_samples)
            .unwrap()
    }

    pub fn fractional_max_pool3d(
        &self,
        kernel_size: &[i64],
        output_size: &[i64],
        random_samples: &Tensor,
    ) -> (Tensor, Tensor) {
        self.f_fractional_max_pool3d(kernel_size, output_size, random_samples)
            .unwrap()
    }

    pub fn fractional_max_pool3d_backward(
        &self,
        grad_output: &Tensor,
        kernel_size: &[i64],
        output_size: &[i64],
        indices: &Tensor,
    ) -> Tensor {
        self.f_fractional_max_pool3d_backward(grad_output, kernel_size, output_size, indices)
            .unwrap()
    }

    pub fn fractional_max_pool3d_backward_out(
        &self,
        grad_input: &Tensor,
        grad_output: &Tensor,
        kernel_size: &[i64],
        output_size: &[i64],
        indices: &Tensor,
    ) -> Tensor {
        self.f_fractional_max_pool3d_backward_out(
            grad_input,
            grad_output,
            kernel_size,
            output_size,
            indices,
        )
        .unwrap()
    }

    pub fn fractional_max_pool3d_out(
        &self,
        output: &Tensor,
        indices: &Tensor,
        kernel_size: &[i64],
        output_size: &[i64],
        random_samples: &Tensor,
    ) -> (Tensor, Tensor) {
        self.f_fractional_max_pool3d_out(output, indices, kernel_size, output_size, random_samples)
            .unwrap()
    }

    pub fn frobenius_norm(&self) -> Tensor {
        self.f_frobenius_norm().unwrap()
    }

    pub fn frobenius_norm1(&self, dim: &[i64], keepdim: bool) -> Tensor {
        self.f_frobenius_norm1(dim, keepdim).unwrap()
    }

    pub fn frobenius_norm_out(&self, out: &Tensor, dim: &[i64], keepdim: bool) -> Tensor {
        self.f_frobenius_norm_out(out, dim, keepdim).unwrap()
    }

    pub fn full<S: Into<Scalar>>(size: &[i64], fill_value: S, options: (Kind, Device)) -> Tensor {
        Tensor::f_full(size, fill_value, options).unwrap()
    }

    pub fn full_like<S: Into<Scalar>>(&self, fill_value: S) -> Tensor {
        self.f_full_like(fill_value).unwrap()
    }

    pub fn full_like1<S: Into<Scalar>>(&self, fill_value: S, options: (Kind, Device)) -> Tensor {
        self.f_full_like1(fill_value, options).unwrap()
    }

    pub fn full_out<S: Into<Scalar>>(out: &Tensor, size: &[i64], fill_value: S) -> Tensor {
        Tensor::f_full_out(out, size, fill_value).unwrap()
    }

    pub fn gather(&self, dim: i64, index: &Tensor, sparse_grad: bool) -> Tensor {
        self.f_gather(dim, index, sparse_grad).unwrap()
    }

    pub fn gather_out(&self, out: &Tensor, dim: i64, index: &Tensor, sparse_grad: bool) -> Tensor {
        self.f_gather_out(out, dim, index, sparse_grad).unwrap()
    }

    pub fn ge<S: Into<Scalar>>(&self, other: S) -> Tensor {
        self.f_ge(other).unwrap()
    }

    pub fn ge1(&self, other: &Tensor) -> Tensor {
        self.f_ge1(other).unwrap()
    }

    pub fn ge_<S: Into<Scalar>>(&mut self, other: S) -> Tensor {
        self.f_ge_(other).unwrap()
    }

    pub fn ge_1(&mut self, other: &Tensor) -> Tensor {
        self.f_ge_1(other).unwrap()
    }

    pub fn ge_out<S: Into<Scalar>>(&self, out: &Tensor, other: S) -> Tensor {
        self.f_ge_out(out, other).unwrap()
    }

    pub fn ge_out1(&self, out: &Tensor, other: &Tensor) -> Tensor {
        self.f_ge_out1(out, other).unwrap()
    }

    pub fn gelu(&self) -> Tensor {
        self.f_gelu().unwrap()
    }

    pub fn gelu_backward(&self, grad: &Tensor) -> Tensor {
        self.f_gelu_backward(grad).unwrap()
    }

    pub fn geometric_(&mut self, p: f64) -> Tensor {
        self.f_geometric_(p).unwrap()
    }

    pub fn geqrf(&self) -> (Tensor, Tensor) {
        self.f_geqrf().unwrap()
    }

    pub fn geqrf_out(&self, a: &Tensor, tau: &Tensor) -> (Tensor, Tensor) {
        self.f_geqrf_out(a, tau).unwrap()
    }

    pub fn ger(&self, vec2: &Tensor) -> Tensor {
        self.f_ger(vec2).unwrap()
    }

    pub fn ger_out(&self, out: &Tensor, vec2: &Tensor) -> Tensor {
        self.f_ger_out(out, vec2).unwrap()
    }

    pub fn glu(&self, dim: i64) -> Tensor {
        self.f_glu(dim).unwrap()
    }

    pub fn glu_backward(&self, grad_output: &Tensor, dim: i64) -> Tensor {
        self.f_glu_backward(grad_output, dim).unwrap()
    }

    pub fn glu_backward_out(&self, grad_input: &Tensor, grad_output: &Tensor, dim: i64) -> Tensor {
        self.f_glu_backward_out(grad_input, grad_output, dim)
            .unwrap()
    }

    pub fn glu_out(&self, out: &Tensor, dim: i64) -> Tensor {
        self.f_glu_out(out, dim).unwrap()
    }

    pub fn grad(&self) -> Tensor {
        self.f_grad().unwrap()
    }

    pub fn grid_sampler(
        &self,
        grid: &Tensor,
        interpolation_mode: i64,
        padding_mode: i64,
        align_corners: bool,
    ) -> Tensor {
        self.f_grid_sampler(grid, interpolation_mode, padding_mode, align_corners)
            .unwrap()
    }

    pub fn grid_sampler_2d(
        &self,
        grid: &Tensor,
        interpolation_mode: i64,
        padding_mode: i64,
        align_corners: bool,
    ) -> Tensor {
        self.f_grid_sampler_2d(grid, interpolation_mode, padding_mode, align_corners)
            .unwrap()
    }

    pub fn grid_sampler_2d_backward(
        &self,
        grad_output: &Tensor,
        grid: &Tensor,
        interpolation_mode: i64,
        padding_mode: i64,
        align_corners: bool,
    ) -> (Tensor, Tensor) {
        self.f_grid_sampler_2d_backward(
            grad_output,
            grid,
            interpolation_mode,
            padding_mode,
            align_corners,
        )
        .unwrap()
    }

    pub fn grid_sampler_3d(
        &self,
        grid: &Tensor,
        interpolation_mode: i64,
        padding_mode: i64,
        align_corners: bool,
    ) -> Tensor {
        self.f_grid_sampler_3d(grid, interpolation_mode, padding_mode, align_corners)
            .unwrap()
    }

    pub fn grid_sampler_3d_backward(
        &self,
        grad_output: &Tensor,
        grid: &Tensor,
        interpolation_mode: i64,
        padding_mode: i64,
        align_corners: bool,
    ) -> (Tensor, Tensor) {
        self.f_grid_sampler_3d_backward(
            grad_output,
            grid,
            interpolation_mode,
            padding_mode,
            align_corners,
        )
        .unwrap()
    }

    pub fn group_norm<T: Borrow<Tensor>>(
        &self,
        num_groups: i64,
        weight: Option<T>,
        bias: Option<T>,
        eps: f64,
        cudnn_enabled: bool,
    ) -> Tensor {
        self.f_group_norm(num_groups, weight, bias, eps, cudnn_enabled)
            .unwrap()
    }

    pub fn gru<T: Borrow<Tensor>>(
        &self,
        hx: &Tensor,
        params: &[T],
        has_biases: bool,
        num_layers: i64,
        dropout: f64,
        train: bool,
        bidirectional: bool,
        batch_first: bool,
    ) -> (Tensor, Tensor) {
        self.f_gru(
            hx,
            params,
            has_biases,
            num_layers,
            dropout,
            train,
            bidirectional,
            batch_first,
        )
        .unwrap()
    }

    pub fn gru1<T: Borrow<Tensor>>(
        data: &Tensor,
        batch_sizes: &Tensor,
        hx: &Tensor,
        params: &[T],
        has_biases: bool,
        num_layers: i64,
        dropout: f64,
        train: bool,
        bidirectional: bool,
    ) -> (Tensor, Tensor) {
        Tensor::f_gru1(
            data,
            batch_sizes,
            hx,
            params,
            has_biases,
            num_layers,
            dropout,
            train,
            bidirectional,
        )
        .unwrap()
    }

    pub fn gru_cell<T: Borrow<Tensor>>(
        &self,
        hx: &Tensor,
        w_ih: &Tensor,
        w_hh: &Tensor,
        b_ih: Option<T>,
        b_hh: Option<T>,
    ) -> Tensor {
        self.f_gru_cell(hx, w_ih, w_hh, b_ih, b_hh).unwrap()
    }

    pub fn gt<S: Into<Scalar>>(&self, other: S) -> Tensor {
        self.f_gt(other).unwrap()
    }

    pub fn gt1(&self, other: &Tensor) -> Tensor {
        self.f_gt1(other).unwrap()
    }

    pub fn gt_<S: Into<Scalar>>(&mut self, other: S) -> Tensor {
        self.f_gt_(other).unwrap()
    }

    pub fn gt_1(&mut self, other: &Tensor) -> Tensor {
        self.f_gt_1(other).unwrap()
    }

    pub fn gt_out<S: Into<Scalar>>(&self, out: &Tensor, other: S) -> Tensor {
        self.f_gt_out(out, other).unwrap()
    }

    pub fn gt_out1(&self, out: &Tensor, other: &Tensor) -> Tensor {
        self.f_gt_out1(out, other).unwrap()
    }

    pub fn hamming_window(window_length: i64, options: (Kind, Device)) -> Tensor {
        Tensor::f_hamming_window(window_length, options).unwrap()
    }

    pub fn hamming_window1(window_length: i64, periodic: bool, options: (Kind, Device)) -> Tensor {
        Tensor::f_hamming_window1(window_length, periodic, options).unwrap()
    }

    pub fn hamming_window2(
        window_length: i64,
        periodic: bool,
        alpha: f64,
        options: (Kind, Device),
    ) -> Tensor {
        Tensor::f_hamming_window2(window_length, periodic, alpha, options).unwrap()
    }

    pub fn hamming_window3(
        window_length: i64,
        periodic: bool,
        alpha: f64,
        beta: f64,
        options: (Kind, Device),
    ) -> Tensor {
        Tensor::f_hamming_window3(window_length, periodic, alpha, beta, options).unwrap()
    }

    pub fn hann_window(window_length: i64, options: (Kind, Device)) -> Tensor {
        Tensor::f_hann_window(window_length, options).unwrap()
    }

    pub fn hann_window1(window_length: i64, periodic: bool, options: (Kind, Device)) -> Tensor {
        Tensor::f_hann_window1(window_length, periodic, options).unwrap()
    }

    pub fn hardshrink(&self) -> Tensor {
        self.f_hardshrink().unwrap()
    }

    pub fn hardshrink_backward<S: Into<Scalar>>(&self, grad_out: &Tensor, lambd: S) -> Tensor {
        self.f_hardshrink_backward(grad_out, lambd).unwrap()
    }

    pub fn hardtanh(&self) -> Tensor {
        self.f_hardtanh().unwrap()
    }

    pub fn hardtanh_(&mut self) -> Tensor {
        self.f_hardtanh_().unwrap()
    }

    pub fn hardtanh_backward<S: Into<Scalar>>(
        &self,
        grad_output: &Tensor,
        min_val: S,
        max_val: S,
    ) -> Tensor {
        self.f_hardtanh_backward(grad_output, min_val, max_val)
            .unwrap()
    }

    pub fn hardtanh_backward_out<S: Into<Scalar>>(
        &self,
        grad_input: &Tensor,
        grad_output: &Tensor,
        min_val: S,
        max_val: S,
    ) -> Tensor {
        self.f_hardtanh_backward_out(grad_input, grad_output, min_val, max_val)
            .unwrap()
    }

    pub fn hardtanh_out(&self, out: &Tensor) -> Tensor {
        self.f_hardtanh_out(out).unwrap()
    }

    pub fn hinge_embedding_loss(
        &self,
        target: &Tensor,
        margin: f64,
        reduction: crate::Reduction,
    ) -> Tensor {
        self.f_hinge_embedding_loss(target, margin, reduction)
            .unwrap()
    }

    pub fn histc(&self, bins: i64) -> Tensor {
        self.f_histc(bins).unwrap()
    }

    pub fn histc_out(&self, out: &Tensor, bins: i64) -> Tensor {
        self.f_histc_out(out, bins).unwrap()
    }

    pub fn hspmm(mat1: &Tensor, mat2: &Tensor) -> Tensor {
        Tensor::f_hspmm(mat1, mat2).unwrap()
    }

    pub fn hspmm_out(out: &Tensor, mat1: &Tensor, mat2: &Tensor) -> Tensor {
        Tensor::f_hspmm_out(out, mat1, mat2).unwrap()
    }

    pub fn ifft(&self, signal_ndim: i64, normalized: bool) -> Tensor {
        self.f_ifft(signal_ndim, normalized).unwrap()
    }

    pub fn im2col(
        &self,
        kernel_size: &[i64],
        dilation: &[i64],
        padding: &[i64],
        stride: &[i64],
    ) -> Tensor {
        self.f_im2col(kernel_size, dilation, padding, stride)
            .unwrap()
    }

    pub fn im2col_backward(
        grad_output: &Tensor,
        input_size: &[i64],
        kernel_size: &[i64],
        dilation: &[i64],
        padding: &[i64],
        stride: &[i64],
    ) -> Tensor {
        Tensor::f_im2col_backward(
            grad_output,
            input_size,
            kernel_size,
            dilation,
            padding,
            stride,
        )
        .unwrap()
    }

    pub fn im2col_backward_out(
        grad_input: &Tensor,
        grad_output: &Tensor,
        input_size: &[i64],
        kernel_size: &[i64],
        dilation: &[i64],
        padding: &[i64],
        stride: &[i64],
    ) -> Tensor {
        Tensor::f_im2col_backward_out(
            grad_input,
            grad_output,
            input_size,
            kernel_size,
            dilation,
            padding,
            stride,
        )
        .unwrap()
    }

    pub fn im2col_out(
        &self,
        out: &Tensor,
        kernel_size: &[i64],
        dilation: &[i64],
        padding: &[i64],
        stride: &[i64],
    ) -> Tensor {
        self.f_im2col_out(out, kernel_size, dilation, padding, stride)
            .unwrap()
    }

    pub fn imag(&self) -> Tensor {
        self.f_imag().unwrap()
    }

    pub fn imag_out(&self, out: &Tensor) -> Tensor {
        self.f_imag_out(out).unwrap()
    }

    pub fn index<T: Borrow<Tensor>>(&self, indices: &[T]) -> Tensor {
        self.f_index(indices).unwrap()
    }

    pub fn index_add(&self, dim: i64, index: &Tensor, source: &Tensor) -> Tensor {
        self.f_index_add(dim, index, source).unwrap()
    }

    pub fn index_add_(&mut self, dim: i64, index: &Tensor, source: &Tensor) -> Tensor {
        self.f_index_add_(dim, index, source).unwrap()
    }

    pub fn index_copy(&self, dim: i64, index: &Tensor, source: &Tensor) -> Tensor {
        self.f_index_copy(dim, index, source).unwrap()
    }

    pub fn index_copy_(&mut self, dim: i64, index: &Tensor, source: &Tensor) -> Tensor {
        self.f_index_copy_(dim, index, source).unwrap()
    }

    pub fn index_fill<S: Into<Scalar>>(&self, dim: i64, index: &Tensor, value: S) -> Tensor {
        self.f_index_fill(dim, index, value).unwrap()
    }

    pub fn index_fill1(&self, dim: i64, index: &Tensor, value: &Tensor) -> Tensor {
        self.f_index_fill1(dim, index, value).unwrap()
    }

    pub fn index_fill_<S: Into<Scalar>>(&mut self, dim: i64, index: &Tensor, value: S) -> Tensor {
        self.f_index_fill_(dim, index, value).unwrap()
    }

    pub fn index_fill_1(&mut self, dim: i64, index: &Tensor, value: &Tensor) -> Tensor {
        self.f_index_fill_1(dim, index, value).unwrap()
    }

    pub fn index_put<T: Borrow<Tensor>>(
        &self,
        indices: &[T],
        values: &Tensor,
        accumulate: bool,
    ) -> Tensor {
        self.f_index_put(indices, values, accumulate).unwrap()
    }

    pub fn index_put_<T: Borrow<Tensor>>(
        &mut self,
        indices: &[T],
        values: &Tensor,
        accumulate: bool,
    ) -> Tensor {
        self.f_index_put_(indices, values, accumulate).unwrap()
    }

    pub fn index_select(&self, dim: i64, index: &Tensor) -> Tensor {
        self.f_index_select(dim, index).unwrap()
    }

    pub fn index_select_out(&self, out: &Tensor, dim: i64, index: &Tensor) -> Tensor {
        self.f_index_select_out(out, dim, index).unwrap()
    }

    pub fn indices(&self) -> Tensor {
        self.f_indices().unwrap()
    }

    pub fn instance_norm<T: Borrow<Tensor>>(
        &self,
        weight: Option<T>,
        bias: Option<T>,
        running_mean: Option<T>,
        running_var: Option<T>,
        use_input_stats: bool,
        momentum: f64,
        eps: f64,
        cudnn_enabled: bool,
    ) -> Tensor {
        self.f_instance_norm(
            weight,
            bias,
            running_mean,
            running_var,
            use_input_stats,
            momentum,
            eps,
            cudnn_enabled,
        )
        .unwrap()
    }

    pub fn int_repr(&self) -> Tensor {
        self.f_int_repr().unwrap()
    }

    pub fn inverse(&self) -> Tensor {
        self.f_inverse().unwrap()
    }

    pub fn inverse_out(&self, out: &Tensor) -> Tensor {
        self.f_inverse_out(out).unwrap()
    }

    pub fn irfft(
        &self,
        signal_ndim: i64,
        normalized: bool,
        onesided: bool,
        signal_sizes: &[i64],
    ) -> Tensor {
        self.f_irfft(signal_ndim, normalized, onesided, signal_sizes)
            .unwrap()
    }

    pub fn isclose(&self, other: &Tensor, rtol: f64, atol: f64, equal_nan: bool) -> Tensor {
        self.f_isclose(other, rtol, atol, equal_nan).unwrap()
    }

    pub fn isfinite(&self) -> Tensor {
        self.f_isfinite().unwrap()
    }

    pub fn isnan(&self) -> Tensor {
        self.f_isnan().unwrap()
    }

    pub fn kl_div(&self, target: &Tensor, reduction: crate::Reduction) -> Tensor {
        self.f_kl_div(target, reduction).unwrap()
    }

    pub fn kl_div_backward(
        &self,
        grad_output: &Tensor,
        target: &Tensor,
        reduction: crate::Reduction,
    ) -> Tensor {
        self.f_kl_div_backward(grad_output, target, reduction)
            .unwrap()
    }

    pub fn kthvalue(&self, k: i64, dim: i64, keepdim: bool) -> (Tensor, Tensor) {
        self.f_kthvalue(k, dim, keepdim).unwrap()
    }

    pub fn kthvalue_out(
        &self,
        values: &Tensor,
        indices: &Tensor,
        k: i64,
        dim: i64,
        keepdim: bool,
    ) -> (Tensor, Tensor) {
        self.f_kthvalue_out(values, indices, k, dim, keepdim)
            .unwrap()
    }

    pub fn l1_loss(&self, target: &Tensor, reduction: crate::Reduction) -> Tensor {
        self.f_l1_loss(target, reduction).unwrap()
    }

    pub fn l1_loss_backward(
        &self,
        grad_output: &Tensor,
        target: &Tensor,
        reduction: crate::Reduction,
    ) -> Tensor {
        self.f_l1_loss_backward(grad_output, target, reduction)
            .unwrap()
    }

    pub fn l1_loss_backward_out(
        &self,
        grad_input: &Tensor,
        grad_output: &Tensor,
        target: &Tensor,
        reduction: crate::Reduction,
    ) -> Tensor {
        self.f_l1_loss_backward_out(grad_input, grad_output, target, reduction)
            .unwrap()
    }

    pub fn l1_loss_out(
        &self,
        out: &Tensor,
        target: &Tensor,
        reduction: crate::Reduction,
    ) -> Tensor {
        self.f_l1_loss_out(out, target, reduction).unwrap()
    }

    pub fn layer_norm<T: Borrow<Tensor>>(
        &self,
        normalized_shape: &[i64],
        weight: Option<T>,
        bias: Option<T>,
        eps: f64,
        cudnn_enable: bool,
    ) -> Tensor {
        self.f_layer_norm(normalized_shape, weight, bias, eps, cudnn_enable)
            .unwrap()
    }

    pub fn le<S: Into<Scalar>>(&self, other: S) -> Tensor {
        self.f_le(other).unwrap()
    }

    pub fn le1(&self, other: &Tensor) -> Tensor {
        self.f_le1(other).unwrap()
    }

    pub fn le_<S: Into<Scalar>>(&mut self, other: S) -> Tensor {
        self.f_le_(other).unwrap()
    }

    pub fn le_1(&mut self, other: &Tensor) -> Tensor {
        self.f_le_1(other).unwrap()
    }

    pub fn le_out<S: Into<Scalar>>(&self, out: &Tensor, other: S) -> Tensor {
        self.f_le_out(out, other).unwrap()
    }

    pub fn le_out1(&self, out: &Tensor, other: &Tensor) -> Tensor {
        self.f_le_out1(out, other).unwrap()
    }

    pub fn leaky_relu(&self) -> Tensor {
        self.f_leaky_relu().unwrap()
    }

    pub fn leaky_relu_(&mut self) -> Tensor {
        self.f_leaky_relu_().unwrap()
    }

    pub fn leaky_relu_backward<S: Into<Scalar>>(
        &self,
        grad_output: &Tensor,
        negative_slope: S,
    ) -> Tensor {
        self.f_leaky_relu_backward(grad_output, negative_slope)
            .unwrap()
    }

    pub fn leaky_relu_backward_out<S: Into<Scalar>>(
        &self,
        grad_input: &Tensor,
        grad_output: &Tensor,
        negative_slope: S,
    ) -> Tensor {
        self.f_leaky_relu_backward_out(grad_input, grad_output, negative_slope)
            .unwrap()
    }

    pub fn leaky_relu_out(&self, out: &Tensor) -> Tensor {
        self.f_leaky_relu_out(out).unwrap()
    }

    pub fn lerp<S: Into<Scalar>>(&self, end: &Tensor, weight: S) -> Tensor {
        self.f_lerp(end, weight).unwrap()
    }

    pub fn lerp1(&self, end: &Tensor, weight: &Tensor) -> Tensor {
        self.f_lerp1(end, weight).unwrap()
    }

    pub fn lerp_<S: Into<Scalar>>(&mut self, end: &Tensor, weight: S) -> Tensor {
        self.f_lerp_(end, weight).unwrap()
    }

    pub fn lerp_1(&mut self, end: &Tensor, weight: &Tensor) -> Tensor {
        self.f_lerp_1(end, weight).unwrap()
    }

    pub fn lerp_out<S: Into<Scalar>>(&self, out: &Tensor, end: &Tensor, weight: S) -> Tensor {
        self.f_lerp_out(out, end, weight).unwrap()
    }

    pub fn lerp_out1(&self, out: &Tensor, end: &Tensor, weight: &Tensor) -> Tensor {
        self.f_lerp_out1(out, end, weight).unwrap()
    }

    pub fn lgamma(&self) -> Tensor {
        self.f_lgamma().unwrap()
    }

    pub fn lgamma_(&mut self) -> Tensor {
        self.f_lgamma_().unwrap()
    }

    pub fn lgamma_out(&self, out: &Tensor) -> Tensor {
        self.f_lgamma_out(out).unwrap()
    }

    pub fn linear<T: Borrow<Tensor>>(&self, weight: &Tensor, bias: Option<T>) -> Tensor {
        self.f_linear(weight, bias).unwrap()
    }

    pub fn linspace<S: Into<Scalar>>(
        start: S,
        end: S,
        steps: i64,
        options: (Kind, Device),
    ) -> Tensor {
        Tensor::f_linspace(start, end, steps, options).unwrap()
    }

    pub fn linspace_out<S: Into<Scalar>>(out: &Tensor, start: S, end: S, steps: i64) -> Tensor {
        Tensor::f_linspace_out(out, start, end, steps).unwrap()
    }

    pub fn log(&self) -> Tensor {
        self.f_log().unwrap()
    }

    pub fn log10(&self) -> Tensor {
        self.f_log10().unwrap()
    }

    pub fn log10_(&mut self) -> Tensor {
        self.f_log10_().unwrap()
    }

    pub fn log10_out(&self, out: &Tensor) -> Tensor {
        self.f_log10_out(out).unwrap()
    }

    pub fn log1p(&self) -> Tensor {
        self.f_log1p().unwrap()
    }

    pub fn log1p_(&mut self) -> Tensor {
        self.f_log1p_().unwrap()
    }

    pub fn log1p_out(&self, out: &Tensor) -> Tensor {
        self.f_log1p_out(out).unwrap()
    }

    pub fn log2(&self) -> Tensor {
        self.f_log2().unwrap()
    }

    pub fn log2_(&mut self) -> Tensor {
        self.f_log2_().unwrap()
    }

    pub fn log2_out(&self, out: &Tensor) -> Tensor {
        self.f_log2_out(out).unwrap()
    }

    pub fn log_(&mut self) -> Tensor {
        self.f_log_().unwrap()
    }

    pub fn log_normal_(&mut self, mean: f64, std: f64) -> Tensor {
        self.f_log_normal_(mean, std).unwrap()
    }

    pub fn log_out(&self, out: &Tensor) -> Tensor {
        self.f_log_out(out).unwrap()
    }

    pub fn log_sigmoid(&self) -> Tensor {
        self.f_log_sigmoid().unwrap()
    }

    pub fn log_sigmoid_backward(&self, grad_output: &Tensor, buffer: &Tensor) -> Tensor {
        self.f_log_sigmoid_backward(grad_output, buffer).unwrap()
    }

    pub fn log_sigmoid_backward_out(
        &self,
        grad_input: &Tensor,
        grad_output: &Tensor,
        buffer: &Tensor,
    ) -> Tensor {
        self.f_log_sigmoid_backward_out(grad_input, grad_output, buffer)
            .unwrap()
    }

    pub fn log_sigmoid_out(&self, out: &Tensor) -> Tensor {
        self.f_log_sigmoid_out(out).unwrap()
    }

    pub fn log_softmax(&self, dim: i64, dtype: Kind) -> Tensor {
        self.f_log_softmax(dim, dtype).unwrap()
    }

    pub fn logdet(&self) -> Tensor {
        self.f_logdet().unwrap()
    }

    pub fn logical_not(&self) -> Tensor {
        self.f_logical_not().unwrap()
    }

    pub fn logical_not_(&mut self) -> Tensor {
        self.f_logical_not_().unwrap()
    }

    pub fn logical_not_out(&self, out: &Tensor) -> Tensor {
        self.f_logical_not_out(out).unwrap()
    }

    pub fn logical_xor(&self, other: &Tensor) -> Tensor {
        self.f_logical_xor(other).unwrap()
    }

    pub fn logical_xor_(&mut self, other: &Tensor) -> Tensor {
        self.f_logical_xor_(other).unwrap()
    }

    pub fn logical_xor_out(&self, out: &Tensor, other: &Tensor) -> Tensor {
        self.f_logical_xor_out(out, other).unwrap()
    }

    pub fn logspace<S: Into<Scalar>>(
        start: S,
        end: S,
        steps: i64,
        base: f64,
        options: (Kind, Device),
    ) -> Tensor {
        Tensor::f_logspace(start, end, steps, base, options).unwrap()
    }

    pub fn logspace_out<S: Into<Scalar>>(
        out: &Tensor,
        start: S,
        end: S,
        steps: i64,
        base: f64,
    ) -> Tensor {
        Tensor::f_logspace_out(out, start, end, steps, base).unwrap()
    }

    pub fn logsumexp(&self, dim: &[i64], keepdim: bool) -> Tensor {
        self.f_logsumexp(dim, keepdim).unwrap()
    }

    pub fn logsumexp_out(&self, out: &Tensor, dim: &[i64], keepdim: bool) -> Tensor {
        self.f_logsumexp_out(out, dim, keepdim).unwrap()
    }

    pub fn lstm<T: Borrow<Tensor>>(
        &self,
        hx: &[T],
        params: &[T],
        has_biases: bool,
        num_layers: i64,
        dropout: f64,
        train: bool,
        bidirectional: bool,
        batch_first: bool,
    ) -> (Tensor, Tensor, Tensor) {
        self.f_lstm(
            hx,
            params,
            has_biases,
            num_layers,
            dropout,
            train,
            bidirectional,
            batch_first,
        )
        .unwrap()
    }

    pub fn lstm1<T: Borrow<Tensor>>(
        data: &Tensor,
        batch_sizes: &Tensor,
        hx: &[T],
        params: &[T],
        has_biases: bool,
        num_layers: i64,
        dropout: f64,
        train: bool,
        bidirectional: bool,
    ) -> (Tensor, Tensor, Tensor) {
        Tensor::f_lstm1(
            data,
            batch_sizes,
            hx,
            params,
            has_biases,
            num_layers,
            dropout,
            train,
            bidirectional,
        )
        .unwrap()
    }

    pub fn lstm_cell<T: Borrow<Tensor>>(
        &self,
        hx: &[T],
        w_ih: &Tensor,
        w_hh: &Tensor,
        b_ih: Option<T>,
        b_hh: Option<T>,
    ) -> (Tensor, Tensor) {
        self.f_lstm_cell(hx, w_ih, w_hh, b_ih, b_hh).unwrap()
    }

    pub fn lstsq(&self, a: &Tensor) -> (Tensor, Tensor) {
        self.f_lstsq(a).unwrap()
    }

    pub fn lstsq_out(&self, x: &Tensor, qr: &Tensor, a: &Tensor) -> (Tensor, Tensor) {
        self.f_lstsq_out(x, qr, a).unwrap()
    }

    pub fn lt<S: Into<Scalar>>(&self, other: S) -> Tensor {
        self.f_lt(other).unwrap()
    }

    pub fn lt1(&self, other: &Tensor) -> Tensor {
        self.f_lt1(other).unwrap()
    }

    pub fn lt_<S: Into<Scalar>>(&mut self, other: S) -> Tensor {
        self.f_lt_(other).unwrap()
    }

    pub fn lt_1(&mut self, other: &Tensor) -> Tensor {
        self.f_lt_1(other).unwrap()
    }

    pub fn lt_out<S: Into<Scalar>>(&self, out: &Tensor, other: S) -> Tensor {
        self.f_lt_out(out, other).unwrap()
    }

    pub fn lt_out1(&self, out: &Tensor, other: &Tensor) -> Tensor {
        self.f_lt_out1(out, other).unwrap()
    }

    pub fn lu_solve(&self, lu_data: &Tensor, lu_pivots: &Tensor) -> Tensor {
        self.f_lu_solve(lu_data, lu_pivots).unwrap()
    }

    pub fn lu_solve_out(&self, out: &Tensor, lu_data: &Tensor, lu_pivots: &Tensor) -> Tensor {
        self.f_lu_solve_out(out, lu_data, lu_pivots).unwrap()
    }

    pub fn margin_ranking_loss(
        input1: &Tensor,
        input2: &Tensor,
        target: &Tensor,
        margin: f64,
        reduction: crate::Reduction,
    ) -> Tensor {
        Tensor::f_margin_ranking_loss(input1, input2, target, margin, reduction).unwrap()
    }

    pub fn masked_fill<S: Into<Scalar>>(&self, mask: &Tensor, value: S) -> Tensor {
        self.f_masked_fill(mask, value).unwrap()
    }

    pub fn masked_fill1(&self, mask: &Tensor, value: &Tensor) -> Tensor {
        self.f_masked_fill1(mask, value).unwrap()
    }

    pub fn masked_fill_<S: Into<Scalar>>(&mut self, mask: &Tensor, value: S) -> Tensor {
        self.f_masked_fill_(mask, value).unwrap()
    }

    pub fn masked_fill_1(&mut self, mask: &Tensor, value: &Tensor) -> Tensor {
        self.f_masked_fill_1(mask, value).unwrap()
    }

    pub fn masked_scatter(&self, mask: &Tensor, source: &Tensor) -> Tensor {
        self.f_masked_scatter(mask, source).unwrap()
    }

    pub fn masked_scatter_(&mut self, mask: &Tensor, source: &Tensor) -> Tensor {
        self.f_masked_scatter_(mask, source).unwrap()
    }

    pub fn masked_select(&self, mask: &Tensor) -> Tensor {
        self.f_masked_select(mask).unwrap()
    }

    pub fn masked_select_out(&self, out: &Tensor, mask: &Tensor) -> Tensor {
        self.f_masked_select_out(out, mask).unwrap()
    }

    pub fn matmul(&self, other: &Tensor) -> Tensor {
        self.f_matmul(other).unwrap()
    }

    pub fn matmul_out(&self, out: &Tensor, other: &Tensor) -> Tensor {
        self.f_matmul_out(out, other).unwrap()
    }

    pub fn matrix_power(&self, n: i64) -> Tensor {
        self.f_matrix_power(n).unwrap()
    }

    pub fn matrix_rank(&self, symmetric: bool) -> Tensor {
        self.f_matrix_rank(symmetric).unwrap()
    }

    pub fn matrix_rank1(&self, tol: f64, symmetric: bool) -> Tensor {
        self.f_matrix_rank1(tol, symmetric).unwrap()
    }

    pub fn max(&self) -> Tensor {
        self.f_max().unwrap()
    }

    pub fn max1(&self, other: &Tensor) -> Tensor {
        self.f_max1(other).unwrap()
    }

    pub fn max2(&self, dim: i64, keepdim: bool) -> (Tensor, Tensor) {
        self.f_max2(dim, keepdim).unwrap()
    }

    pub fn max_out(&self, out: &Tensor, other: &Tensor) -> Tensor {
        self.f_max_out(out, other).unwrap()
    }

    pub fn max_out1(
        &self,
        max: &Tensor,
        max_values: &Tensor,
        dim: i64,
        keepdim: bool,
    ) -> (Tensor, Tensor) {
        self.f_max_out1(max, max_values, dim, keepdim).unwrap()
    }

    pub fn max_pool1d(
        &self,
        kernel_size: &[i64],
        stride: &[i64],
        padding: &[i64],
        dilation: &[i64],
        ceil_mode: bool,
    ) -> Tensor {
        self.f_max_pool1d(kernel_size, stride, padding, dilation, ceil_mode)
            .unwrap()
    }

    pub fn max_pool1d_with_indices(
        &self,
        kernel_size: &[i64],
        stride: &[i64],
        padding: &[i64],
        dilation: &[i64],
        ceil_mode: bool,
    ) -> (Tensor, Tensor) {
        self.f_max_pool1d_with_indices(kernel_size, stride, padding, dilation, ceil_mode)
            .unwrap()
    }

    pub fn max_pool2d(
        &self,
        kernel_size: &[i64],
        stride: &[i64],
        padding: &[i64],
        dilation: &[i64],
        ceil_mode: bool,
    ) -> Tensor {
        self.f_max_pool2d(kernel_size, stride, padding, dilation, ceil_mode)
            .unwrap()
    }

    pub fn max_pool2d_with_indices(
        &self,
        kernel_size: &[i64],
        stride: &[i64],
        padding: &[i64],
        dilation: &[i64],
        ceil_mode: bool,
    ) -> (Tensor, Tensor) {
        self.f_max_pool2d_with_indices(kernel_size, stride, padding, dilation, ceil_mode)
            .unwrap()
    }

    pub fn max_pool2d_with_indices_backward(
        &self,
        grad_output: &Tensor,
        kernel_size: &[i64],
        stride: &[i64],
        padding: &[i64],
        dilation: &[i64],
        ceil_mode: bool,
        indices: &Tensor,
    ) -> Tensor {
        self.f_max_pool2d_with_indices_backward(
            grad_output,
            kernel_size,
            stride,
            padding,
            dilation,
            ceil_mode,
            indices,
        )
        .unwrap()
    }

    pub fn max_pool2d_with_indices_backward_out(
        &self,
        grad_input: &Tensor,
        grad_output: &Tensor,
        kernel_size: &[i64],
        stride: &[i64],
        padding: &[i64],
        dilation: &[i64],
        ceil_mode: bool,
        indices: &Tensor,
    ) -> Tensor {
        self.f_max_pool2d_with_indices_backward_out(
            grad_input,
            grad_output,
            kernel_size,
            stride,
            padding,
            dilation,
            ceil_mode,
            indices,
        )
        .unwrap()
    }

    pub fn max_pool2d_with_indices_out(
        &self,
        out: &Tensor,
        indices: &Tensor,
        kernel_size: &[i64],
        stride: &[i64],
        padding: &[i64],
        dilation: &[i64],
        ceil_mode: bool,
    ) -> (Tensor, Tensor) {
        self.f_max_pool2d_with_indices_out(
            out,
            indices,
            kernel_size,
            stride,
            padding,
            dilation,
            ceil_mode,
        )
        .unwrap()
    }

    pub fn max_pool3d(
        &self,
        kernel_size: &[i64],
        stride: &[i64],
        padding: &[i64],
        dilation: &[i64],
        ceil_mode: bool,
    ) -> Tensor {
        self.f_max_pool3d(kernel_size, stride, padding, dilation, ceil_mode)
            .unwrap()
    }

    pub fn max_pool3d_with_indices(
        &self,
        kernel_size: &[i64],
        stride: &[i64],
        padding: &[i64],
        dilation: &[i64],
        ceil_mode: bool,
    ) -> (Tensor, Tensor) {
        self.f_max_pool3d_with_indices(kernel_size, stride, padding, dilation, ceil_mode)
            .unwrap()
    }

    pub fn max_pool3d_with_indices_backward(
        &self,
        grad_output: &Tensor,
        kernel_size: &[i64],
        stride: &[i64],
        padding: &[i64],
        dilation: &[i64],
        ceil_mode: bool,
        indices: &Tensor,
    ) -> Tensor {
        self.f_max_pool3d_with_indices_backward(
            grad_output,
            kernel_size,
            stride,
            padding,
            dilation,
            ceil_mode,
            indices,
        )
        .unwrap()
    }

    pub fn max_pool3d_with_indices_backward_out(
        &self,
        grad_input: &Tensor,
        grad_output: &Tensor,
        kernel_size: &[i64],
        stride: &[i64],
        padding: &[i64],
        dilation: &[i64],
        ceil_mode: bool,
        indices: &Tensor,
    ) -> Tensor {
        self.f_max_pool3d_with_indices_backward_out(
            grad_input,
            grad_output,
            kernel_size,
            stride,
            padding,
            dilation,
            ceil_mode,
            indices,
        )
        .unwrap()
    }

    pub fn max_pool3d_with_indices_out(
        &self,
        out: &Tensor,
        indices: &Tensor,
        kernel_size: &[i64],
        stride: &[i64],
        padding: &[i64],
        dilation: &[i64],
        ceil_mode: bool,
    ) -> (Tensor, Tensor) {
        self.f_max_pool3d_with_indices_out(
            out,
            indices,
            kernel_size,
            stride,
            padding,
            dilation,
            ceil_mode,
        )
        .unwrap()
    }

    pub fn max_unpool2d(&self, indices: &Tensor, output_size: &[i64]) -> Tensor {
        self.f_max_unpool2d(indices, output_size).unwrap()
    }

    pub fn max_unpool2d_backward(
        &self,
        grad_output: &Tensor,
        indices: &Tensor,
        output_size: &[i64],
    ) -> Tensor {
        self.f_max_unpool2d_backward(grad_output, indices, output_size)
            .unwrap()
    }

    pub fn max_unpool2d_backward_out(
        &self,
        grad_input: &Tensor,
        grad_output: &Tensor,
        indices: &Tensor,
        output_size: &[i64],
    ) -> Tensor {
        self.f_max_unpool2d_backward_out(grad_input, grad_output, indices, output_size)
            .unwrap()
    }

    pub fn max_unpool2d_out(&self, out: &Tensor, indices: &Tensor, output_size: &[i64]) -> Tensor {
        self.f_max_unpool2d_out(out, indices, output_size).unwrap()
    }

    pub fn max_unpool3d(
        &self,
        indices: &Tensor,
        output_size: &[i64],
        stride: &[i64],
        padding: &[i64],
    ) -> Tensor {
        self.f_max_unpool3d(indices, output_size, stride, padding)
            .unwrap()
    }

    pub fn max_unpool3d_backward(
        &self,
        grad_output: &Tensor,
        indices: &Tensor,
        output_size: &[i64],
        stride: &[i64],
        padding: &[i64],
    ) -> Tensor {
        self.f_max_unpool3d_backward(grad_output, indices, output_size, stride, padding)
            .unwrap()
    }

    pub fn max_unpool3d_backward_out(
        &self,
        grad_input: &Tensor,
        grad_output: &Tensor,
        indices: &Tensor,
        output_size: &[i64],
        stride: &[i64],
        padding: &[i64],
    ) -> Tensor {
        self.f_max_unpool3d_backward_out(
            grad_input,
            grad_output,
            indices,
            output_size,
            stride,
            padding,
        )
        .unwrap()
    }

    pub fn max_unpool3d_out(
        &self,
        out: &Tensor,
        indices: &Tensor,
        output_size: &[i64],
        stride: &[i64],
        padding: &[i64],
    ) -> Tensor {
        self.f_max_unpool3d_out(out, indices, output_size, stride, padding)
            .unwrap()
    }

    pub fn max_values(&self, dim: &[i64], keepdim: bool) -> Tensor {
        self.f_max_values(dim, keepdim).unwrap()
    }

    pub fn mean(&self, dtype: Kind) -> Tensor {
        self.f_mean(dtype).unwrap()
    }

    pub fn mean1(&self, dim: &[i64], keepdim: bool, dtype: Kind) -> Tensor {
        self.f_mean1(dim, keepdim, dtype).unwrap()
    }

    pub fn mean_out(&self, out: &Tensor, dim: &[i64], keepdim: bool, dtype: Kind) -> Tensor {
        self.f_mean_out(out, dim, keepdim, dtype).unwrap()
    }

    pub fn median(&self) -> Tensor {
        self.f_median().unwrap()
    }

    pub fn median1(&self, dim: i64, keepdim: bool) -> (Tensor, Tensor) {
        self.f_median1(dim, keepdim).unwrap()
    }

    pub fn median_out(
        &self,
        values: &Tensor,
        indices: &Tensor,
        dim: i64,
        keepdim: bool,
    ) -> (Tensor, Tensor) {
        self.f_median_out(values, indices, dim, keepdim).unwrap()
    }

    pub fn meshgrid<T: Borrow<Tensor>>(tensors: &[T]) -> Vec<Tensor> {
        Tensor::f_meshgrid(tensors).unwrap()
    }

    pub fn min(&self) -> Tensor {
        self.f_min().unwrap()
    }

    pub fn min1(&self, other: &Tensor) -> Tensor {
        self.f_min1(other).unwrap()
    }

    pub fn min2(&self, dim: i64, keepdim: bool) -> (Tensor, Tensor) {
        self.f_min2(dim, keepdim).unwrap()
    }

    pub fn min_out(&self, out: &Tensor, other: &Tensor) -> Tensor {
        self.f_min_out(out, other).unwrap()
    }

    pub fn min_out1(
        &self,
        min: &Tensor,
        min_indices: &Tensor,
        dim: i64,
        keepdim: bool,
    ) -> (Tensor, Tensor) {
        self.f_min_out1(min, min_indices, dim, keepdim).unwrap()
    }

    pub fn min_values(&self, dim: &[i64], keepdim: bool) -> Tensor {
        self.f_min_values(dim, keepdim).unwrap()
    }

    pub fn miopen_batch_norm<T: Borrow<Tensor>>(
        &self,
        weight: &Tensor,
        bias: Option<T>,
        running_mean: Option<T>,
        running_var: Option<T>,
        training: bool,
        exponential_average_factor: f64,
        epsilon: f64,
    ) -> (Tensor, Tensor, Tensor) {
        self.f_miopen_batch_norm(
            weight,
            bias,
            running_mean,
            running_var,
            training,
            exponential_average_factor,
            epsilon,
        )
        .unwrap()
    }

    pub fn miopen_batch_norm_backward<T: Borrow<Tensor>>(
        &self,
        grad_output: &Tensor,
        weight: &Tensor,
        running_mean: Option<T>,
        running_var: Option<T>,
        save_mean: Option<T>,
        save_var: Option<T>,
        epsilon: f64,
    ) -> (Tensor, Tensor, Tensor) {
        self.f_miopen_batch_norm_backward(
            grad_output,
            weight,
            running_mean,
            running_var,
            save_mean,
            save_var,
            epsilon,
        )
        .unwrap()
    }

    pub fn miopen_convolution<T: Borrow<Tensor>>(
        &self,
        weight: &Tensor,
        bias: Option<T>,
        padding: &[i64],
        stride: &[i64],
        dilation: &[i64],
        groups: i64,
        benchmark: bool,
        deterministic: bool,
    ) -> Tensor {
        self.f_miopen_convolution(
            weight,
            bias,
            padding,
            stride,
            dilation,
            groups,
            benchmark,
            deterministic,
        )
        .unwrap()
    }

    pub fn miopen_convolution_backward_bias(grad_output: &Tensor) -> Tensor {
        Tensor::f_miopen_convolution_backward_bias(grad_output).unwrap()
    }

    pub fn miopen_convolution_backward_input(
        self_size: &[i64],
        grad_output: &Tensor,
        weight: &Tensor,
        padding: &[i64],
        stride: &[i64],
        dilation: &[i64],
        groups: i64,
        benchmark: bool,
        deterministic: bool,
    ) -> Tensor {
        Tensor::f_miopen_convolution_backward_input(
            self_size,
            grad_output,
            weight,
            padding,
            stride,
            dilation,
            groups,
            benchmark,
            deterministic,
        )
        .unwrap()
    }

    pub fn miopen_convolution_backward_weight(
        &self,
        weight_size: &[i64],
        grad_output: &Tensor,
        padding: &[i64],
        stride: &[i64],
        dilation: &[i64],
        groups: i64,
        benchmark: bool,
        deterministic: bool,
    ) -> Tensor {
        self.f_miopen_convolution_backward_weight(
            weight_size,
            grad_output,
            padding,
            stride,
            dilation,
            groups,
            benchmark,
            deterministic,
        )
        .unwrap()
    }

    pub fn miopen_convolution_transpose<T: Borrow<Tensor>>(
        &self,
        weight: &Tensor,
        bias: Option<T>,
        padding: &[i64],
        output_padding: &[i64],
        stride: &[i64],
        dilation: &[i64],
        groups: i64,
        benchmark: bool,
        deterministic: bool,
    ) -> Tensor {
        self.f_miopen_convolution_transpose(
            weight,
            bias,
            padding,
            output_padding,
            stride,
            dilation,
            groups,
            benchmark,
            deterministic,
        )
        .unwrap()
    }

    pub fn miopen_convolution_transpose_backward_input(
        grad_output: &Tensor,
        weight: &Tensor,
        padding: &[i64],
        stride: &[i64],
        dilation: &[i64],
        groups: i64,
        benchmark: bool,
        deterministic: bool,
    ) -> Tensor {
        Tensor::f_miopen_convolution_transpose_backward_input(
            grad_output,
            weight,
            padding,
            stride,
            dilation,
            groups,
            benchmark,
            deterministic,
        )
        .unwrap()
    }

    pub fn miopen_convolution_transpose_backward_weight(
        &self,
        weight_size: &[i64],
        grad_output: &Tensor,
        padding: &[i64],
        stride: &[i64],
        dilation: &[i64],
        groups: i64,
        benchmark: bool,
        deterministic: bool,
    ) -> Tensor {
        self.f_miopen_convolution_transpose_backward_weight(
            weight_size,
            grad_output,
            padding,
            stride,
            dilation,
            groups,
            benchmark,
            deterministic,
        )
        .unwrap()
    }

    pub fn miopen_depthwise_convolution<T: Borrow<Tensor>>(
        &self,
        weight: &Tensor,
        bias: Option<T>,
        padding: &[i64],
        stride: &[i64],
        dilation: &[i64],
        groups: i64,
        benchmark: bool,
        deterministic: bool,
    ) -> Tensor {
        self.f_miopen_depthwise_convolution(
            weight,
            bias,
            padding,
            stride,
            dilation,
            groups,
            benchmark,
            deterministic,
        )
        .unwrap()
    }

    pub fn miopen_depthwise_convolution_backward_input(
        self_size: &[i64],
        grad_output: &Tensor,
        weight: &Tensor,
        padding: &[i64],
        stride: &[i64],
        dilation: &[i64],
        groups: i64,
        benchmark: bool,
        deterministic: bool,
    ) -> Tensor {
        Tensor::f_miopen_depthwise_convolution_backward_input(
            self_size,
            grad_output,
            weight,
            padding,
            stride,
            dilation,
            groups,
            benchmark,
            deterministic,
        )
        .unwrap()
    }

    pub fn miopen_depthwise_convolution_backward_weight(
        &self,
        weight_size: &[i64],
        grad_output: &Tensor,
        padding: &[i64],
        stride: &[i64],
        dilation: &[i64],
        groups: i64,
        benchmark: bool,
        deterministic: bool,
    ) -> Tensor {
        self.f_miopen_depthwise_convolution_backward_weight(
            weight_size,
            grad_output,
            padding,
            stride,
            dilation,
            groups,
            benchmark,
            deterministic,
        )
        .unwrap()
    }

    pub fn miopen_rnn<T: Borrow<Tensor>>(
        &self,
        weight: &[T],
        weight_stride0: i64,
        hx: &Tensor,
        cx: Option<T>,
        mode: i64,
        hidden_size: i64,
        num_layers: i64,
        batch_first: bool,
        dropout: f64,
        train: bool,
        bidirectional: bool,
        batch_sizes: &[i64],
        dropout_state: Option<T>,
    ) -> (Tensor, Tensor, Tensor, Tensor, Tensor) {
        self.f_miopen_rnn(
            weight,
            weight_stride0,
            hx,
            cx,
            mode,
            hidden_size,
            num_layers,
            batch_first,
            dropout,
            train,
            bidirectional,
            batch_sizes,
            dropout_state,
        )
        .unwrap()
    }

    pub fn mkldnn_adaptive_avg_pool2d(&self, output_size: &[i64]) -> Tensor {
        self.f_mkldnn_adaptive_avg_pool2d(output_size).unwrap()
    }

    pub fn mkldnn_convolution<T: Borrow<Tensor>>(
        &self,
        weight: &Tensor,
        bias: Option<T>,
        padding: &[i64],
        stride: &[i64],
        dilation: &[i64],
        groups: i64,
    ) -> Tensor {
        self.f_mkldnn_convolution(weight, bias, padding, stride, dilation, groups)
            .unwrap()
    }

    pub fn mkldnn_convolution_backward_input(
        self_size: &[i64],
        grad_output: &Tensor,
        weight: &Tensor,
        padding: &[i64],
        stride: &[i64],
        dilation: &[i64],
        groups: i64,
        bias_defined: bool,
    ) -> Tensor {
        Tensor::f_mkldnn_convolution_backward_input(
            self_size,
            grad_output,
            weight,
            padding,
            stride,
            dilation,
            groups,
            bias_defined,
        )
        .unwrap()
    }

    pub fn mkldnn_convolution_backward_weights(
        &self,
        weight_size: &[i64],
        grad_output: &Tensor,
        padding: &[i64],
        stride: &[i64],
        dilation: &[i64],
        groups: i64,
        bias_defined: bool,
    ) -> (Tensor, Tensor) {
        self.f_mkldnn_convolution_backward_weights(
            weight_size,
            grad_output,
            padding,
            stride,
            dilation,
            groups,
            bias_defined,
        )
        .unwrap()
    }

    pub fn mkldnn_linear<T: Borrow<Tensor>>(&self, weight: &Tensor, bias: Option<T>) -> Tensor {
        self.f_mkldnn_linear(weight, bias).unwrap()
    }

    pub fn mkldnn_max_pool2d(
        &self,
        kernel_size: &[i64],
        stride: &[i64],
        padding: &[i64],
        dilation: &[i64],
        ceil_mode: bool,
    ) -> Tensor {
        self.f_mkldnn_max_pool2d(kernel_size, stride, padding, dilation, ceil_mode)
            .unwrap()
    }

    pub fn mkldnn_reorder_conv2d_weight(
        &self,
        padding: &[i64],
        stride: &[i64],
        dilation: &[i64],
        groups: i64,
    ) -> Tensor {
        self.f_mkldnn_reorder_conv2d_weight(padding, stride, dilation, groups)
            .unwrap()
    }

    pub fn mm(&self, mat2: &Tensor) -> Tensor {
        self.f_mm(mat2).unwrap()
    }

    pub fn mm_out(&self, out: &Tensor, mat2: &Tensor) -> Tensor {
        self.f_mm_out(out, mat2).unwrap()
    }

    pub fn mode(&self, dim: i64, keepdim: bool) -> (Tensor, Tensor) {
        self.f_mode(dim, keepdim).unwrap()
    }

    pub fn mode_out(
        &self,
        values: &Tensor,
        indices: &Tensor,
        dim: i64,
        keepdim: bool,
    ) -> (Tensor, Tensor) {
        self.f_mode_out(values, indices, dim, keepdim).unwrap()
    }

    pub fn mse_loss(&self, target: &Tensor, reduction: crate::Reduction) -> Tensor {
        self.f_mse_loss(target, reduction).unwrap()
    }

    pub fn mse_loss_backward(
        &self,
        grad_output: &Tensor,
        target: &Tensor,
        reduction: crate::Reduction,
    ) -> Tensor {
        self.f_mse_loss_backward(grad_output, target, reduction)
            .unwrap()
    }

    pub fn mse_loss_backward_out(
        &self,
        grad_input: &Tensor,
        grad_output: &Tensor,
        target: &Tensor,
        reduction: crate::Reduction,
    ) -> Tensor {
        self.f_mse_loss_backward_out(grad_input, grad_output, target, reduction)
            .unwrap()
    }

    pub fn mse_loss_out(
        &self,
        out: &Tensor,
        target: &Tensor,
        reduction: crate::Reduction,
    ) -> Tensor {
        self.f_mse_loss_out(out, target, reduction).unwrap()
    }

    pub fn g_mul(&self, other: &Tensor) -> Tensor {
        self.f_mul(other).unwrap()
    }

    pub fn g_mul1<S: Into<Scalar>>(&self, other: S) -> Tensor {
        self.f_mul1(other).unwrap()
    }

    pub fn g_mul_(&mut self, other: &Tensor) -> Tensor {
        self.f_mul_(other).unwrap()
    }

    pub fn g_mul_1<S: Into<Scalar>>(&mut self, other: S) -> Tensor {
        self.f_mul_1(other).unwrap()
    }

    pub fn mul_out(&self, out: &Tensor, other: &Tensor) -> Tensor {
        self.f_mul_out(out, other).unwrap()
    }

    pub fn multi_margin_loss_backward<T: Borrow<Tensor>, S: Into<Scalar>>(
        &self,
        grad_output: &Tensor,
        target: &Tensor,
        p: S,
        margin: S,
        weight: Option<T>,
        reduction: crate::Reduction,
    ) -> Tensor {
        self.f_multi_margin_loss_backward(grad_output, target, p, margin, weight, reduction)
            .unwrap()
    }

    pub fn multi_margin_loss_backward_out<T: Borrow<Tensor>, S: Into<Scalar>>(
        &self,
        grad_input: &Tensor,
        grad_output: &Tensor,
        target: &Tensor,
        p: S,
        margin: S,
        weight: Option<T>,
        reduction: crate::Reduction,
    ) -> Tensor {
        self.f_multi_margin_loss_backward_out(
            grad_input,
            grad_output,
            target,
            p,
            margin,
            weight,
            reduction,
        )
        .unwrap()
    }

    pub fn multilabel_margin_loss(&self, target: &Tensor, reduction: crate::Reduction) -> Tensor {
        self.f_multilabel_margin_loss(target, reduction).unwrap()
    }

    pub fn multilabel_margin_loss_backward(
        &self,
        grad_output: &Tensor,
        target: &Tensor,
        reduction: crate::Reduction,
        is_target: &Tensor,
    ) -> Tensor {
        self.f_multilabel_margin_loss_backward(grad_output, target, reduction, is_target)
            .unwrap()
    }

    pub fn multilabel_margin_loss_backward_out(
        &self,
        grad_input: &Tensor,
        grad_output: &Tensor,
        target: &Tensor,
        reduction: crate::Reduction,
        is_target: &Tensor,
    ) -> Tensor {
        self.f_multilabel_margin_loss_backward_out(
            grad_input,
            grad_output,
            target,
            reduction,
            is_target,
        )
        .unwrap()
    }

    pub fn multilabel_margin_loss_out(
        &self,
        out: &Tensor,
        target: &Tensor,
        reduction: crate::Reduction,
    ) -> Tensor {
        self.f_multilabel_margin_loss_out(out, target, reduction)
            .unwrap()
    }

    pub fn multinomial(&self, num_samples: i64, replacement: bool) -> Tensor {
        self.f_multinomial(num_samples, replacement).unwrap()
    }

    pub fn multinomial_out(&self, out: &Tensor, num_samples: i64, replacement: bool) -> Tensor {
        self.f_multinomial_out(out, num_samples, replacement)
            .unwrap()
    }

    pub fn mv(&self, vec: &Tensor) -> Tensor {
        self.f_mv(vec).unwrap()
    }

    pub fn mv_out(&self, out: &Tensor, vec: &Tensor) -> Tensor {
        self.f_mv_out(out, vec).unwrap()
    }

    pub fn mvlgamma(&self, p: i64) -> Tensor {
        self.f_mvlgamma(p).unwrap()
    }

    pub fn mvlgamma_(&mut self, p: i64) -> Tensor {
        self.f_mvlgamma_(p).unwrap()
    }

    pub fn narrow(&self, dim: i64, start: i64, length: i64) -> Tensor {
        self.f_narrow(dim, start, length).unwrap()
    }

    pub fn narrow_copy(&self, dim: i64, start: i64, length: i64) -> Tensor {
        self.f_narrow_copy(dim, start, length).unwrap()
    }

    pub fn native_batch_norm<T: Borrow<Tensor>>(
        &self,
        weight: Option<T>,
        bias: Option<T>,
        running_mean: Option<T>,
        running_var: Option<T>,
        training: bool,
        momentum: f64,
        eps: f64,
    ) -> (Tensor, Tensor, Tensor) {
        self.f_native_batch_norm(
            weight,
            bias,
            running_mean,
            running_var,
            training,
            momentum,
            eps,
        )
        .unwrap()
    }

    pub fn native_layer_norm<T: Borrow<Tensor>>(
        &self,
        weight: Option<T>,
        bias: Option<T>,
        m: i64,
        n: i64,
        eps: f64,
    ) -> (Tensor, Tensor, Tensor) {
        self.f_native_layer_norm(weight, bias, m, n, eps).unwrap()
    }

    pub fn native_norm(&self) -> Tensor {
        self.f_native_norm().unwrap()
    }

    pub fn ne<S: Into<Scalar>>(&self, other: S) -> Tensor {
        self.f_ne(other).unwrap()
    }

    pub fn ne1(&self, other: &Tensor) -> Tensor {
        self.f_ne1(other).unwrap()
    }

    pub fn ne_<S: Into<Scalar>>(&mut self, other: S) -> Tensor {
        self.f_ne_(other).unwrap()
    }

    pub fn ne_1(&mut self, other: &Tensor) -> Tensor {
        self.f_ne_1(other).unwrap()
    }

    pub fn ne_out<S: Into<Scalar>>(&self, out: &Tensor, other: S) -> Tensor {
        self.f_ne_out(out, other).unwrap()
    }

    pub fn ne_out1(&self, out: &Tensor, other: &Tensor) -> Tensor {
        self.f_ne_out1(out, other).unwrap()
    }

    pub fn neg(&self) -> Tensor {
        self.f_neg().unwrap()
    }

    pub fn neg_(&mut self) -> Tensor {
        self.f_neg_().unwrap()
    }

    pub fn neg_out(&self, out: &Tensor) -> Tensor {
        self.f_neg_out(out).unwrap()
    }

    pub fn new_empty(&self, size: &[i64], options: (Kind, Device)) -> Tensor {
        self.f_new_empty(size, options).unwrap()
    }

    pub fn new_full<S: Into<Scalar>>(
        &self,
        size: &[i64],
        fill_value: S,
        options: (Kind, Device),
    ) -> Tensor {
        self.f_new_full(size, fill_value, options).unwrap()
    }

    pub fn new_zeros(&self, size: &[i64], options: (Kind, Device)) -> Tensor {
        self.f_new_zeros(size, options).unwrap()
    }

    pub fn g_nll_loss<T: Borrow<Tensor>>(
        &self,
        target: &Tensor,
        weight: Option<T>,
        reduction: crate::Reduction,
        ignore_index: i64,
    ) -> Tensor {
        self.f_nll_loss(target, weight, reduction, ignore_index)
            .unwrap()
    }

    pub fn nll_loss2d<T: Borrow<Tensor>>(
        &self,
        target: &Tensor,
        weight: Option<T>,
        reduction: crate::Reduction,
        ignore_index: i64,
    ) -> Tensor {
        self.f_nll_loss2d(target, weight, reduction, ignore_index)
            .unwrap()
    }

    pub fn nll_loss2d_backward<T: Borrow<Tensor>>(
        &self,
        grad_output: &Tensor,
        target: &Tensor,
        weight: Option<T>,
        reduction: crate::Reduction,
        ignore_index: i64,
        total_weight: &Tensor,
    ) -> Tensor {
        self.f_nll_loss2d_backward(
            grad_output,
            target,
            weight,
            reduction,
            ignore_index,
            total_weight,
        )
        .unwrap()
    }

    pub fn nll_loss2d_backward_out<T: Borrow<Tensor>>(
        &self,
        grad_input: &Tensor,
        grad_output: &Tensor,
        target: &Tensor,
        weight: Option<T>,
        reduction: crate::Reduction,
        ignore_index: i64,
        total_weight: &Tensor,
    ) -> Tensor {
        self.f_nll_loss2d_backward_out(
            grad_input,
            grad_output,
            target,
            weight,
            reduction,
            ignore_index,
            total_weight,
        )
        .unwrap()
    }

    pub fn nll_loss2d_out<T: Borrow<Tensor>>(
        &self,
        out: &Tensor,
        target: &Tensor,
        weight: Option<T>,
        reduction: crate::Reduction,
        ignore_index: i64,
    ) -> Tensor {
        self.f_nll_loss2d_out(out, target, weight, reduction, ignore_index)
            .unwrap()
    }

    pub fn nll_loss_backward<T: Borrow<Tensor>>(
        &self,
        grad_output: &Tensor,
        target: &Tensor,
        weight: Option<T>,
        reduction: crate::Reduction,
        ignore_index: i64,
        total_weight: &Tensor,
    ) -> Tensor {
        self.f_nll_loss_backward(
            grad_output,
            target,
            weight,
            reduction,
            ignore_index,
            total_weight,
        )
        .unwrap()
    }

    pub fn nll_loss_backward_out<T: Borrow<Tensor>>(
        &self,
        grad_input: &Tensor,
        grad_output: &Tensor,
        target: &Tensor,
        weight: Option<T>,
        reduction: crate::Reduction,
        ignore_index: i64,
        total_weight: &Tensor,
    ) -> Tensor {
        self.f_nll_loss_backward_out(
            grad_input,
            grad_output,
            target,
            weight,
            reduction,
            ignore_index,
            total_weight,
        )
        .unwrap()
    }

    pub fn nll_loss_out<T: Borrow<Tensor>>(
        &self,
        out: &Tensor,
        target: &Tensor,
        weight: Option<T>,
        reduction: crate::Reduction,
        ignore_index: i64,
    ) -> Tensor {
        self.f_nll_loss_out(out, target, weight, reduction, ignore_index)
            .unwrap()
    }

    pub fn nonzero(&self) -> Tensor {
        self.f_nonzero().unwrap()
    }

    pub fn nonzero_numpy(&self) -> Vec<Tensor> {
        self.f_nonzero_numpy().unwrap()
    }

    pub fn nonzero_out(&self, out: &Tensor) -> Tensor {
        self.f_nonzero_out(out).unwrap()
    }

    pub fn norm(&self) -> Tensor {
        self.f_norm().unwrap()
    }

    pub fn norm1<S: Into<Scalar>>(&self, p: S, dtype: Kind) -> Tensor {
        self.f_norm1(p, dtype).unwrap()
    }

    pub fn norm2<S: Into<Scalar>>(&self, p: S, dim: &[i64], keepdim: bool) -> Tensor {
        self.f_norm2(p, dim, keepdim).unwrap()
    }

    pub fn norm3<S: Into<Scalar>>(&self, p: S, dim: &[i64], keepdim: bool, dtype: Kind) -> Tensor {
        self.f_norm3(p, dim, keepdim, dtype).unwrap()
    }

    pub fn norm_except_dim(v: &Tensor, pow: i64, dim: i64) -> Tensor {
        Tensor::f_norm_except_dim(v, pow, dim).unwrap()
    }

    pub fn norm_out<S: Into<Scalar>>(
        &self,
        out: &Tensor,
        p: S,
        dim: &[i64],
        keepdim: bool,
    ) -> Tensor {
        self.f_norm_out(out, p, dim, keepdim).unwrap()
    }

    pub fn norm_out1<S: Into<Scalar>>(
        &self,
        out: &Tensor,
        p: S,
        dim: &[i64],
        keepdim: bool,
        dtype: Kind,
    ) -> Tensor {
        self.f_norm_out1(out, p, dim, keepdim, dtype).unwrap()
    }

    pub fn normal_(&mut self, mean: f64, std: f64) -> Tensor {
        self.f_normal_(mean, std).unwrap()
    }

    pub fn normal_out(out: &Tensor, mean: &Tensor, std: f64) -> Tensor {
        Tensor::f_normal_out(out, mean, std).unwrap()
    }

    pub fn normal_out1(out: &Tensor, mean: f64, std: &Tensor) -> Tensor {
        Tensor::f_normal_out1(out, mean, std).unwrap()
    }

    pub fn normal_out2(out: &Tensor, mean: &Tensor, std: &Tensor) -> Tensor {
        Tensor::f_normal_out2(out, mean, std).unwrap()
    }

    pub fn normal_out3(out: &Tensor, mean: f64, std: f64, size: &[i64]) -> Tensor {
        Tensor::f_normal_out3(out, mean, std, size).unwrap()
    }

    pub fn nuclear_norm(&self, keepdim: bool) -> Tensor {
        self.f_nuclear_norm(keepdim).unwrap()
    }

    pub fn nuclear_norm1(&self, dim: &[i64], keepdim: bool) -> Tensor {
        self.f_nuclear_norm1(dim, keepdim).unwrap()
    }

    pub fn nuclear_norm_out(&self, out: &Tensor, keepdim: bool) -> Tensor {
        self.f_nuclear_norm_out(out, keepdim).unwrap()
    }

    pub fn nuclear_norm_out1(&self, out: &Tensor, dim: &[i64], keepdim: bool) -> Tensor {
        self.f_nuclear_norm_out1(out, dim, keepdim).unwrap()
    }

    pub fn numpy_t(&self) -> Tensor {
        self.f_numpy_t().unwrap()
    }

    pub fn one_hot(&self, num_classes: i64) -> Tensor {
        self.f_one_hot(num_classes).unwrap()
    }

    pub fn ones(size: &[i64], options: (Kind, Device)) -> Tensor {
        Tensor::f_ones(size, options).unwrap()
    }

    pub fn ones_like(&self) -> Tensor {
        self.f_ones_like().unwrap()
    }

    pub fn ones_like1(&self, options: (Kind, Device)) -> Tensor {
        self.f_ones_like1(options).unwrap()
    }

    pub fn ones_out(out: &Tensor, size: &[i64]) -> Tensor {
        Tensor::f_ones_out(out, size).unwrap()
    }

    pub fn orgqr(&self, input2: &Tensor) -> Tensor {
        self.f_orgqr(input2).unwrap()
    }

    pub fn orgqr_out(&self, out: &Tensor, input2: &Tensor) -> Tensor {
        self.f_orgqr_out(out, input2).unwrap()
    }

    pub fn ormqr(&self, input2: &Tensor, input3: &Tensor, left: bool, transpose: bool) -> Tensor {
        self.f_ormqr(input2, input3, left, transpose).unwrap()
    }

    pub fn ormqr_out(
        &self,
        out: &Tensor,
        input2: &Tensor,
        input3: &Tensor,
        left: bool,
        transpose: bool,
    ) -> Tensor {
        self.f_ormqr_out(out, input2, input3, left, transpose)
            .unwrap()
    }

    pub fn pairwise_distance(x1: &Tensor, x2: &Tensor, p: f64, eps: f64, keepdim: bool) -> Tensor {
        Tensor::f_pairwise_distance(x1, x2, p, eps, keepdim).unwrap()
    }

    pub fn pdist(&self, p: f64) -> Tensor {
        self.f_pdist(p).unwrap()
    }

    pub fn permute(&self, dims: &[i64]) -> Tensor {
        self.f_permute(dims).unwrap()
    }

    pub fn pin_memory(&self) -> Tensor {
        self.f_pin_memory().unwrap()
    }

    pub fn pinverse(&self, rcond: f64) -> Tensor {
        self.f_pinverse(rcond).unwrap()
    }

    pub fn pixel_shuffle(&self, upscale_factor: i64) -> Tensor {
        self.f_pixel_shuffle(upscale_factor).unwrap()
    }

    pub fn poisson(&self) -> Tensor {
        self.f_poisson().unwrap()
    }

    pub fn poisson_nll_loss(
        &self,
        target: &Tensor,
        log_input: bool,
        full: bool,
        eps: f64,
        reduction: crate::Reduction,
    ) -> Tensor {
        self.f_poisson_nll_loss(target, log_input, full, eps, reduction)
            .unwrap()
    }

    pub fn polygamma(&self, n: i64) -> Tensor {
        self.f_polygamma(n).unwrap()
    }

    pub fn polygamma_(&mut self, n: i64) -> Tensor {
        self.f_polygamma_(n).unwrap()
    }

    pub fn polygamma_out(&self, out: &Tensor, n: i64) -> Tensor {
        self.f_polygamma_out(out, n).unwrap()
    }

    pub fn pow<S: Into<Scalar>>(&self, exponent: S) -> Tensor {
        self.f_pow(exponent).unwrap()
    }

    pub fn pow1(&self, exponent: &Tensor) -> Tensor {
        self.f_pow1(exponent).unwrap()
    }

    pub fn pow2<S: Into<Scalar>>(self_scalar: S, exponent: &Tensor) -> Tensor {
        Tensor::f_pow2(self_scalar, exponent).unwrap()
    }

    pub fn pow_<S: Into<Scalar>>(&mut self, exponent: S) -> Tensor {
        self.f_pow_(exponent).unwrap()
    }

    pub fn pow_1(&mut self, exponent: &Tensor) -> Tensor {
        self.f_pow_1(exponent).unwrap()
    }

    pub fn pow_out<S: Into<Scalar>>(&self, out: &Tensor, exponent: S) -> Tensor {
        self.f_pow_out(out, exponent).unwrap()
    }

    pub fn pow_out1(&self, out: &Tensor, exponent: &Tensor) -> Tensor {
        self.f_pow_out1(out, exponent).unwrap()
    }

    pub fn pow_out2<S: Into<Scalar>>(out: &Tensor, self_scalar: S, exponent: &Tensor) -> Tensor {
        Tensor::f_pow_out2(out, self_scalar, exponent).unwrap()
    }

    pub fn prelu(&self, weight: &Tensor) -> Tensor {
        self.f_prelu(weight).unwrap()
    }

    pub fn prelu_backward(&self, grad_output: &Tensor, weight: &Tensor) -> (Tensor, Tensor) {
        self.f_prelu_backward(grad_output, weight).unwrap()
    }

    pub fn prod(&self, dtype: Kind) -> Tensor {
        self.f_prod(dtype).unwrap()
    }

    pub fn prod1(&self, dim: i64, keepdim: bool, dtype: Kind) -> Tensor {
        self.f_prod1(dim, keepdim, dtype).unwrap()
    }

    pub fn prod_out(&self, out: &Tensor, dim: i64, keepdim: bool, dtype: Kind) -> Tensor {
        self.f_prod_out(out, dim, keepdim, dtype).unwrap()
    }

    pub fn put_(&mut self, index: &Tensor, source: &Tensor, accumulate: bool) -> Tensor {
        self.f_put_(index, source, accumulate).unwrap()
    }

    pub fn q_per_channel_scales(&self) -> Tensor {
        self.f_q_per_channel_scales().unwrap()
    }

    pub fn q_per_channel_zero_points(&self) -> Tensor {
        self.f_q_per_channel_zero_points().unwrap()
    }

    pub fn qr(&self, some: bool) -> (Tensor, Tensor) {
        self.f_qr(some).unwrap()
    }

    pub fn qr_out(&self, q: &Tensor, r: &Tensor, some: bool) -> (Tensor, Tensor) {
        self.f_qr_out(q, r, some).unwrap()
    }

    pub fn quantize_per_channel(
        &self,
        scales: &Tensor,
        zero_points: &Tensor,
        axis: i64,
        dtype: Kind,
    ) -> Tensor {
        self.f_quantize_per_channel(scales, zero_points, axis, dtype)
            .unwrap()
    }

    pub fn quantize_per_tensor(&self, scale: f64, zero_point: i64, dtype: Kind) -> Tensor {
        self.f_quantize_per_tensor(scale, zero_point, dtype)
            .unwrap()
    }

    pub fn quantized_gru<T: Borrow<Tensor>>(
        &self,
        hx: &Tensor,
        params: &[T],
        has_biases: bool,
        num_layers: i64,
        dropout: f64,
        train: bool,
        bidirectional: bool,
        batch_first: bool,
    ) -> (Tensor, Tensor) {
        self.f_quantized_gru(
            hx,
            params,
            has_biases,
            num_layers,
            dropout,
            train,
            bidirectional,
            batch_first,
        )
        .unwrap()
    }

    pub fn quantized_gru1<T: Borrow<Tensor>>(
        data: &Tensor,
        batch_sizes: &Tensor,
        hx: &Tensor,
        params: &[T],
        has_biases: bool,
        num_layers: i64,
        dropout: f64,
        train: bool,
        bidirectional: bool,
    ) -> (Tensor, Tensor) {
        Tensor::f_quantized_gru1(
            data,
            batch_sizes,
            hx,
            params,
            has_biases,
            num_layers,
            dropout,
            train,
            bidirectional,
        )
        .unwrap()
    }

    pub fn quantized_gru_cell<S: Into<Scalar>>(
        &self,
        hx: &Tensor,
        w_ih: &Tensor,
        w_hh: &Tensor,
        b_ih: &Tensor,
        b_hh: &Tensor,
        packed_ih: &Tensor,
        packed_hh: &Tensor,
        col_offsets_ih: &Tensor,
        col_offsets_hh: &Tensor,
        scale_ih: S,
        scale_hh: S,
        zero_point_ih: S,
        zero_point_hh: S,
    ) -> Tensor {
        self.f_quantized_gru_cell(
            hx,
            w_ih,
            w_hh,
            b_ih,
            b_hh,
            packed_ih,
            packed_hh,
            col_offsets_ih,
            col_offsets_hh,
            scale_ih,
            scale_hh,
            zero_point_ih,
            zero_point_hh,
        )
        .unwrap()
    }

    pub fn quantized_lstm<T: Borrow<Tensor>>(
        &self,
        hx: &[T],
        params: &[T],
        has_biases: bool,
        num_layers: i64,
        dropout: f64,
        train: bool,
        bidirectional: bool,
        batch_first: bool,
        dtype: Kind,
        use_dynamic: bool,
    ) -> (Tensor, Tensor, Tensor) {
        self.f_quantized_lstm(
            hx,
            params,
            has_biases,
            num_layers,
            dropout,
            train,
            bidirectional,
            batch_first,
            dtype,
            use_dynamic,
        )
        .unwrap()
    }

    pub fn quantized_lstm1<T: Borrow<Tensor>>(
        data: &Tensor,
        batch_sizes: &Tensor,
        hx: &[T],
        params: &[T],
        has_biases: bool,
        num_layers: i64,
        dropout: f64,
        train: bool,
        bidirectional: bool,
        dtype: Kind,
        use_dynamic: bool,
    ) -> (Tensor, Tensor, Tensor) {
        Tensor::f_quantized_lstm1(
            data,
            batch_sizes,
            hx,
            params,
            has_biases,
            num_layers,
            dropout,
            train,
            bidirectional,
            dtype,
            use_dynamic,
        )
        .unwrap()
    }

    pub fn quantized_lstm_cell<T: Borrow<Tensor>, S: Into<Scalar>>(
        &self,
        hx: &[T],
        w_ih: &Tensor,
        w_hh: &Tensor,
        b_ih: &Tensor,
        b_hh: &Tensor,
        packed_ih: &Tensor,
        packed_hh: &Tensor,
        col_offsets_ih: &Tensor,
        col_offsets_hh: &Tensor,
        scale_ih: S,
        scale_hh: S,
        zero_point_ih: S,
        zero_point_hh: S,
    ) -> (Tensor, Tensor) {
        self.f_quantized_lstm_cell(
            hx,
            w_ih,
            w_hh,
            b_ih,
            b_hh,
            packed_ih,
            packed_hh,
            col_offsets_ih,
            col_offsets_hh,
            scale_ih,
            scale_hh,
            zero_point_ih,
            zero_point_hh,
        )
        .unwrap()
    }

    pub fn quantized_max_pool2d(
        &self,
        kernel_size: &[i64],
        stride: &[i64],
        padding: &[i64],
        dilation: &[i64],
        ceil_mode: bool,
    ) -> Tensor {
        self.f_quantized_max_pool2d(kernel_size, stride, padding, dilation, ceil_mode)
            .unwrap()
    }

    pub fn quantized_rnn_relu_cell<S: Into<Scalar>>(
        &self,
        hx: &Tensor,
        w_ih: &Tensor,
        w_hh: &Tensor,
        b_ih: &Tensor,
        b_hh: &Tensor,
        packed_ih: &Tensor,
        packed_hh: &Tensor,
        col_offsets_ih: &Tensor,
        col_offsets_hh: &Tensor,
        scale_ih: S,
        scale_hh: S,
        zero_point_ih: S,
        zero_point_hh: S,
    ) -> Tensor {
        self.f_quantized_rnn_relu_cell(
            hx,
            w_ih,
            w_hh,
            b_ih,
            b_hh,
            packed_ih,
            packed_hh,
            col_offsets_ih,
            col_offsets_hh,
            scale_ih,
            scale_hh,
            zero_point_ih,
            zero_point_hh,
        )
        .unwrap()
    }

    pub fn quantized_rnn_tanh_cell<S: Into<Scalar>>(
        &self,
        hx: &Tensor,
        w_ih: &Tensor,
        w_hh: &Tensor,
        b_ih: &Tensor,
        b_hh: &Tensor,
        packed_ih: &Tensor,
        packed_hh: &Tensor,
        col_offsets_ih: &Tensor,
        col_offsets_hh: &Tensor,
        scale_ih: S,
        scale_hh: S,
        zero_point_ih: S,
        zero_point_hh: S,
    ) -> Tensor {
        self.f_quantized_rnn_tanh_cell(
            hx,
            w_ih,
            w_hh,
            b_ih,
            b_hh,
            packed_ih,
            packed_hh,
            col_offsets_ih,
            col_offsets_hh,
            scale_ih,
            scale_hh,
            zero_point_ih,
            zero_point_hh,
        )
        .unwrap()
    }

    pub fn rand(size: &[i64], options: (Kind, Device)) -> Tensor {
        Tensor::f_rand(size, options).unwrap()
    }

    pub fn rand_like(&self) -> Tensor {
        self.f_rand_like().unwrap()
    }

    pub fn rand_like1(&self, options: (Kind, Device)) -> Tensor {
        self.f_rand_like1(options).unwrap()
    }

    pub fn rand_out(out: &Tensor, size: &[i64]) -> Tensor {
        Tensor::f_rand_out(out, size).unwrap()
    }

    pub fn randint(high: i64, size: &[i64], options: (Kind, Device)) -> Tensor {
        Tensor::f_randint(high, size, options).unwrap()
    }

    pub fn randint1(low: i64, high: i64, size: &[i64], options: (Kind, Device)) -> Tensor {
        Tensor::f_randint1(low, high, size, options).unwrap()
    }

    pub fn randint_like(&self, high: i64) -> Tensor {
        self.f_randint_like(high).unwrap()
    }

    pub fn randint_like1(&self, low: i64, high: i64) -> Tensor {
        self.f_randint_like1(low, high).unwrap()
    }

    pub fn randint_like2(&self, high: i64, options: (Kind, Device)) -> Tensor {
        self.f_randint_like2(high, options).unwrap()
    }

    pub fn randint_like3(&self, low: i64, high: i64, options: (Kind, Device)) -> Tensor {
        self.f_randint_like3(low, high, options).unwrap()
    }

    pub fn randint_out(out: &Tensor, high: i64, size: &[i64]) -> Tensor {
        Tensor::f_randint_out(out, high, size).unwrap()
    }

    pub fn randint_out1(out: &Tensor, low: i64, high: i64, size: &[i64]) -> Tensor {
        Tensor::f_randint_out1(out, low, high, size).unwrap()
    }

    pub fn randn(size: &[i64], options: (Kind, Device)) -> Tensor {
        Tensor::f_randn(size, options).unwrap()
    }

    pub fn randn_like(&self) -> Tensor {
        self.f_randn_like().unwrap()
    }

    pub fn randn_like1(&self, options: (Kind, Device)) -> Tensor {
        self.f_randn_like1(options).unwrap()
    }

    pub fn randn_out(out: &Tensor, size: &[i64]) -> Tensor {
        Tensor::f_randn_out(out, size).unwrap()
    }

    pub fn random_(&mut self) -> Tensor {
        self.f_random_().unwrap()
    }

    pub fn random_1(&mut self, to: i64) -> Tensor {
        self.f_random_1(to).unwrap()
    }

    pub fn random_2(&mut self, from: i64, to: i64) -> Tensor {
        self.f_random_2(from, to).unwrap()
    }

    pub fn randperm(n: i64, options: (Kind, Device)) -> Tensor {
        Tensor::f_randperm(n, options).unwrap()
    }

    pub fn randperm_out(out: &Tensor, n: i64) -> Tensor {
        Tensor::f_randperm_out(out, n).unwrap()
    }

    pub fn range<S: Into<Scalar>>(start: S, end: S, options: (Kind, Device)) -> Tensor {
        Tensor::f_range(start, end, options).unwrap()
    }

    pub fn range1<S: Into<Scalar>>(start: S, end: S, options: (Kind, Device)) -> Tensor {
        Tensor::f_range1(start, end, options).unwrap()
    }

    pub fn range_out<S: Into<Scalar>>(out: &Tensor, start: S, end: S) -> Tensor {
        Tensor::f_range_out(out, start, end).unwrap()
    }

    pub fn real(&self) -> Tensor {
        self.f_real().unwrap()
    }

    pub fn real_out(&self, out: &Tensor) -> Tensor {
        self.f_real_out(out).unwrap()
    }

    pub fn reciprocal(&self) -> Tensor {
        self.f_reciprocal().unwrap()
    }

    pub fn reciprocal_(&mut self) -> Tensor {
        self.f_reciprocal_().unwrap()
    }

    pub fn reciprocal_out(&self, out: &Tensor) -> Tensor {
        self.f_reciprocal_out(out).unwrap()
    }

    pub fn reflection_pad1d(&self, padding: &[i64]) -> Tensor {
        self.f_reflection_pad1d(padding).unwrap()
    }

    pub fn reflection_pad1d_backward(&self, grad_output: &Tensor, padding: &[i64]) -> Tensor {
        self.f_reflection_pad1d_backward(grad_output, padding)
            .unwrap()
    }

    pub fn reflection_pad1d_backward_out(
        &self,
        grad_input: &Tensor,
        grad_output: &Tensor,
        padding: &[i64],
    ) -> Tensor {
        self.f_reflection_pad1d_backward_out(grad_input, grad_output, padding)
            .unwrap()
    }

    pub fn reflection_pad1d_out(&self, out: &Tensor, padding: &[i64]) -> Tensor {
        self.f_reflection_pad1d_out(out, padding).unwrap()
    }

    pub fn reflection_pad2d(&self, padding: &[i64]) -> Tensor {
        self.f_reflection_pad2d(padding).unwrap()
    }

    pub fn reflection_pad2d_backward(&self, grad_output: &Tensor, padding: &[i64]) -> Tensor {
        self.f_reflection_pad2d_backward(grad_output, padding)
            .unwrap()
    }

    pub fn reflection_pad2d_backward_out(
        &self,
        grad_input: &Tensor,
        grad_output: &Tensor,
        padding: &[i64],
    ) -> Tensor {
        self.f_reflection_pad2d_backward_out(grad_input, grad_output, padding)
            .unwrap()
    }

    pub fn reflection_pad2d_out(&self, out: &Tensor, padding: &[i64]) -> Tensor {
        self.f_reflection_pad2d_out(out, padding).unwrap()
    }

    pub fn relu(&self) -> Tensor {
        self.f_relu().unwrap()
    }

    pub fn relu_(&mut self) -> Tensor {
        self.f_relu_().unwrap()
    }

    pub fn remainder<S: Into<Scalar>>(&self, other: S) -> Tensor {
        self.f_remainder(other).unwrap()
    }

    pub fn remainder1(&self, other: &Tensor) -> Tensor {
        self.f_remainder1(other).unwrap()
    }

    pub fn remainder_<S: Into<Scalar>>(&mut self, other: S) -> Tensor {
        self.f_remainder_(other).unwrap()
    }

    pub fn remainder_1(&mut self, other: &Tensor) -> Tensor {
        self.f_remainder_1(other).unwrap()
    }

    pub fn remainder_out<S: Into<Scalar>>(&self, out: &Tensor, other: S) -> Tensor {
        self.f_remainder_out(out, other).unwrap()
    }

    pub fn remainder_out1(&self, out: &Tensor, other: &Tensor) -> Tensor {
        self.f_remainder_out1(out, other).unwrap()
    }

    pub fn renorm<S: Into<Scalar>>(&self, p: S, dim: i64, maxnorm: S) -> Tensor {
        self.f_renorm(p, dim, maxnorm).unwrap()
    }

    pub fn renorm_<S: Into<Scalar>>(&mut self, p: S, dim: i64, maxnorm: S) -> Tensor {
        self.f_renorm_(p, dim, maxnorm).unwrap()
    }

    pub fn renorm_out<S: Into<Scalar>>(&self, out: &Tensor, p: S, dim: i64, maxnorm: S) -> Tensor {
        self.f_renorm_out(out, p, dim, maxnorm).unwrap()
    }

    pub fn repeat(&self, repeats: &[i64]) -> Tensor {
        self.f_repeat(repeats).unwrap()
    }

    pub fn repeat_interleave(repeats: &Tensor) -> Tensor {
        Tensor::f_repeat_interleave(repeats).unwrap()
    }

    pub fn repeat_interleave1(&self, repeats: &Tensor, dim: i64) -> Tensor {
        self.f_repeat_interleave1(repeats, dim).unwrap()
    }

    pub fn repeat_interleave2(&self, repeats: i64, dim: i64) -> Tensor {
        self.f_repeat_interleave2(repeats, dim).unwrap()
    }

    pub fn replication_pad1d(&self, padding: &[i64]) -> Tensor {
        self.f_replication_pad1d(padding).unwrap()
    }

    pub fn replication_pad1d_backward(&self, grad_output: &Tensor, padding: &[i64]) -> Tensor {
        self.f_replication_pad1d_backward(grad_output, padding)
            .unwrap()
    }

    pub fn replication_pad1d_backward_out(
        &self,
        grad_input: &Tensor,
        grad_output: &Tensor,
        padding: &[i64],
    ) -> Tensor {
        self.f_replication_pad1d_backward_out(grad_input, grad_output, padding)
            .unwrap()
    }

    pub fn replication_pad1d_out(&self, out: &Tensor, padding: &[i64]) -> Tensor {
        self.f_replication_pad1d_out(out, padding).unwrap()
    }

    pub fn replication_pad2d(&self, padding: &[i64]) -> Tensor {
        self.f_replication_pad2d(padding).unwrap()
    }

    pub fn replication_pad2d_backward(&self, grad_output: &Tensor, padding: &[i64]) -> Tensor {
        self.f_replication_pad2d_backward(grad_output, padding)
            .unwrap()
    }

    pub fn replication_pad2d_backward_out(
        &self,
        grad_input: &Tensor,
        grad_output: &Tensor,
        padding: &[i64],
    ) -> Tensor {
        self.f_replication_pad2d_backward_out(grad_input, grad_output, padding)
            .unwrap()
    }

    pub fn replication_pad2d_out(&self, out: &Tensor, padding: &[i64]) -> Tensor {
        self.f_replication_pad2d_out(out, padding).unwrap()
    }

    pub fn replication_pad3d(&self, padding: &[i64]) -> Tensor {
        self.f_replication_pad3d(padding).unwrap()
    }

    pub fn replication_pad3d_backward(&self, grad_output: &Tensor, padding: &[i64]) -> Tensor {
        self.f_replication_pad3d_backward(grad_output, padding)
            .unwrap()
    }

    pub fn replication_pad3d_backward_out(
        &self,
        grad_input: &Tensor,
        grad_output: &Tensor,
        padding: &[i64],
    ) -> Tensor {
        self.f_replication_pad3d_backward_out(grad_input, grad_output, padding)
            .unwrap()
    }

    pub fn replication_pad3d_out(&self, out: &Tensor, padding: &[i64]) -> Tensor {
        self.f_replication_pad3d_out(out, padding).unwrap()
    }

    pub fn requires_grad_(&mut self, internal_requires_grad: bool) -> Tensor {
        self.f_requires_grad_(internal_requires_grad).unwrap()
    }

    pub fn reshape(&self, shape: &[i64]) -> Tensor {
        self.f_reshape(shape).unwrap()
    }

    pub fn reshape_as(&self, other: &Tensor) -> Tensor {
        self.f_reshape_as(other).unwrap()
    }

    pub fn resize_(&mut self, size: &[i64]) -> Tensor {
        self.f_resize_(size).unwrap()
    }

    pub fn resize_as_(&mut self, the_template: &Tensor) -> Tensor {
        self.f_resize_as_(the_template).unwrap()
    }

    pub fn rfft(&self, signal_ndim: i64, normalized: bool, onesided: bool) -> Tensor {
        self.f_rfft(signal_ndim, normalized, onesided).unwrap()
    }

    pub fn rnn_relu<T: Borrow<Tensor>>(
        &self,
        hx: &Tensor,
        params: &[T],
        has_biases: bool,
        num_layers: i64,
        dropout: f64,
        train: bool,
        bidirectional: bool,
        batch_first: bool,
    ) -> (Tensor, Tensor) {
        self.f_rnn_relu(
            hx,
            params,
            has_biases,
            num_layers,
            dropout,
            train,
            bidirectional,
            batch_first,
        )
        .unwrap()
    }

    pub fn rnn_relu1<T: Borrow<Tensor>>(
        data: &Tensor,
        batch_sizes: &Tensor,
        hx: &Tensor,
        params: &[T],
        has_biases: bool,
        num_layers: i64,
        dropout: f64,
        train: bool,
        bidirectional: bool,
    ) -> (Tensor, Tensor) {
        Tensor::f_rnn_relu1(
            data,
            batch_sizes,
            hx,
            params,
            has_biases,
            num_layers,
            dropout,
            train,
            bidirectional,
        )
        .unwrap()
    }

    pub fn rnn_relu_cell<T: Borrow<Tensor>>(
        &self,
        hx: &Tensor,
        w_ih: &Tensor,
        w_hh: &Tensor,
        b_ih: Option<T>,
        b_hh: Option<T>,
    ) -> Tensor {
        self.f_rnn_relu_cell(hx, w_ih, w_hh, b_ih, b_hh).unwrap()
    }

    pub fn rnn_tanh<T: Borrow<Tensor>>(
        &self,
        hx: &Tensor,
        params: &[T],
        has_biases: bool,
        num_layers: i64,
        dropout: f64,
        train: bool,
        bidirectional: bool,
        batch_first: bool,
    ) -> (Tensor, Tensor) {
        self.f_rnn_tanh(
            hx,
            params,
            has_biases,
            num_layers,
            dropout,
            train,
            bidirectional,
            batch_first,
        )
        .unwrap()
    }

    pub fn rnn_tanh1<T: Borrow<Tensor>>(
        data: &Tensor,
        batch_sizes: &Tensor,
        hx: &Tensor,
        params: &[T],
        has_biases: bool,
        num_layers: i64,
        dropout: f64,
        train: bool,
        bidirectional: bool,
    ) -> (Tensor, Tensor) {
        Tensor::f_rnn_tanh1(
            data,
            batch_sizes,
            hx,
            params,
            has_biases,
            num_layers,
            dropout,
            train,
            bidirectional,
        )
        .unwrap()
    }

    pub fn rnn_tanh_cell<T: Borrow<Tensor>>(
        &self,
        hx: &Tensor,
        w_ih: &Tensor,
        w_hh: &Tensor,
        b_ih: Option<T>,
        b_hh: Option<T>,
    ) -> Tensor {
        self.f_rnn_tanh_cell(hx, w_ih, w_hh, b_ih, b_hh).unwrap()
    }

    pub fn roll(&self, shifts: &[i64], dims: &[i64]) -> Tensor {
        self.f_roll(shifts, dims).unwrap()
    }

    pub fn rot90(&self, k: i64, dims: &[i64]) -> Tensor {
        self.f_rot90(k, dims).unwrap()
    }

    pub fn round(&self) -> Tensor {
        self.f_round().unwrap()
    }

    pub fn round_(&mut self) -> Tensor {
        self.f_round_().unwrap()
    }

    pub fn round_out(&self, out: &Tensor) -> Tensor {
        self.f_round_out(out).unwrap()
    }

    pub fn rrelu(&self, training: bool) -> Tensor {
        self.f_rrelu(training).unwrap()
    }

    pub fn rrelu_(&mut self, training: bool) -> Tensor {
        self.f_rrelu_(training).unwrap()
    }

    pub fn rrelu_with_noise(&self, noise: &Tensor, training: bool) -> Tensor {
        self.f_rrelu_with_noise(noise, training).unwrap()
    }

    pub fn rrelu_with_noise_(&mut self, noise: &Tensor, training: bool) -> Tensor {
        self.f_rrelu_with_noise_(noise, training).unwrap()
    }

    pub fn rrelu_with_noise_backward<S: Into<Scalar>>(
        &self,
        grad_output: &Tensor,
        noise: &Tensor,
        lower: S,
        upper: S,
        training: bool,
    ) -> Tensor {
        self.f_rrelu_with_noise_backward(grad_output, noise, lower, upper, training)
            .unwrap()
    }

    pub fn rrelu_with_noise_backward_out<S: Into<Scalar>>(
        &self,
        grad_input: &Tensor,
        grad_output: &Tensor,
        noise: &Tensor,
        lower: S,
        upper: S,
        training: bool,
    ) -> Tensor {
        self.f_rrelu_with_noise_backward_out(grad_input, grad_output, noise, lower, upper, training)
            .unwrap()
    }

    pub fn rrelu_with_noise_out(&self, out: &Tensor, noise: &Tensor, training: bool) -> Tensor {
        self.f_rrelu_with_noise_out(out, noise, training).unwrap()
    }

    pub fn rsqrt(&self) -> Tensor {
        self.f_rsqrt().unwrap()
    }

    pub fn rsqrt_(&mut self) -> Tensor {
        self.f_rsqrt_().unwrap()
    }

    pub fn rsqrt_out(&self, out: &Tensor) -> Tensor {
        self.f_rsqrt_out(out).unwrap()
    }

    pub fn rsub(&self, other: &Tensor) -> Tensor {
        self.f_rsub(other).unwrap()
    }

    pub fn rsub1<S: Into<Scalar>>(&self, other: S) -> Tensor {
        self.f_rsub1(other).unwrap()
    }

    pub fn scalar_tensor<S: Into<Scalar>>(s: S, options: (Kind, Device)) -> Tensor {
        Tensor::f_scalar_tensor(s, options).unwrap()
    }

    pub fn scatter(&self, dim: i64, index: &Tensor, src: &Tensor) -> Tensor {
        self.f_scatter(dim, index, src).unwrap()
    }

    pub fn scatter1<S: Into<Scalar>>(&self, dim: i64, index: &Tensor, value: S) -> Tensor {
        self.f_scatter1(dim, index, value).unwrap()
    }

    pub fn scatter_(&mut self, dim: i64, index: &Tensor, src: &Tensor) -> Tensor {
        self.f_scatter_(dim, index, src).unwrap()
    }

    pub fn scatter_1<S: Into<Scalar>>(&mut self, dim: i64, index: &Tensor, value: S) -> Tensor {
        self.f_scatter_1(dim, index, value).unwrap()
    }

    pub fn scatter_add(&self, dim: i64, index: &Tensor, src: &Tensor) -> Tensor {
        self.f_scatter_add(dim, index, src).unwrap()
    }

    pub fn scatter_add_(&mut self, dim: i64, index: &Tensor, src: &Tensor) -> Tensor {
        self.f_scatter_add_(dim, index, src).unwrap()
    }

    pub fn select(&self, dim: i64, index: i64) -> Tensor {
        self.f_select(dim, index).unwrap()
    }

    pub fn selu(&self) -> Tensor {
        self.f_selu().unwrap()
    }

    pub fn selu_(&mut self) -> Tensor {
        self.f_selu_().unwrap()
    }

    pub fn set_(&mut self) -> Tensor {
        self.f_set_().unwrap()
    }

    pub fn set_1(&mut self, source: &Tensor) -> Tensor {
        self.f_set_1(source).unwrap()
    }

    pub fn set_requires_grad(&self, r: bool) -> Tensor {
        self.f_set_requires_grad(r).unwrap()
    }

    pub fn sigmoid(&self) -> Tensor {
        self.f_sigmoid().unwrap()
    }

    pub fn sigmoid_(&mut self) -> Tensor {
        self.f_sigmoid_().unwrap()
    }

    pub fn sigmoid_backward(grad_output: &Tensor, output: &Tensor) -> Tensor {
        Tensor::f_sigmoid_backward(grad_output, output).unwrap()
    }

    pub fn sigmoid_backward_out(
        grad_input: &Tensor,
        grad_output: &Tensor,
        output: &Tensor,
    ) -> Tensor {
        Tensor::f_sigmoid_backward_out(grad_input, grad_output, output).unwrap()
    }

    pub fn sigmoid_out(&self, out: &Tensor) -> Tensor {
        self.f_sigmoid_out(out).unwrap()
    }

    pub fn sign(&self) -> Tensor {
        self.f_sign().unwrap()
    }

    pub fn sign_(&mut self) -> Tensor {
        self.f_sign_().unwrap()
    }

    pub fn sign_out(&self, out: &Tensor) -> Tensor {
        self.f_sign_out(out).unwrap()
    }

    pub fn sin(&self) -> Tensor {
        self.f_sin().unwrap()
    }

    pub fn sin_(&mut self) -> Tensor {
        self.f_sin_().unwrap()
    }

    pub fn sin_out(&self, out: &Tensor) -> Tensor {
        self.f_sin_out(out).unwrap()
    }

    pub fn sinh(&self) -> Tensor {
        self.f_sinh().unwrap()
    }

    pub fn sinh_(&mut self) -> Tensor {
        self.f_sinh_().unwrap()
    }

    pub fn sinh_out(&self, out: &Tensor) -> Tensor {
        self.f_sinh_out(out).unwrap()
    }

    pub fn slice(&self, dim: i64, start: i64, end: i64, step: i64) -> Tensor {
        self.f_slice(dim, start, end, step).unwrap()
    }

    pub fn slogdet(&self) -> (Tensor, Tensor) {
        self.f_slogdet().unwrap()
    }

    pub fn slow_conv3d<T: Borrow<Tensor>>(
        &self,
        weight: &Tensor,
        kernel_size: &[i64],
        bias: Option<T>,
        stride: &[i64],
        padding: &[i64],
    ) -> Tensor {
        self.f_slow_conv3d(weight, kernel_size, bias, stride, padding)
            .unwrap()
    }

    pub fn slow_conv3d_out<T: Borrow<Tensor>>(
        &self,
        out: &Tensor,
        weight: &Tensor,
        kernel_size: &[i64],
        bias: Option<T>,
        stride: &[i64],
        padding: &[i64],
    ) -> Tensor {
        self.f_slow_conv3d_out(out, weight, kernel_size, bias, stride, padding)
            .unwrap()
    }

    pub fn slow_conv_dilated2d<T: Borrow<Tensor>>(
        &self,
        weight: &Tensor,
        kernel_size: &[i64],
        bias: Option<T>,
        stride: &[i64],
        padding: &[i64],
        dilation: &[i64],
    ) -> Tensor {
        self.f_slow_conv_dilated2d(weight, kernel_size, bias, stride, padding, dilation)
            .unwrap()
    }

    pub fn slow_conv_dilated3d<T: Borrow<Tensor>>(
        &self,
        weight: &Tensor,
        kernel_size: &[i64],
        bias: Option<T>,
        stride: &[i64],
        padding: &[i64],
        dilation: &[i64],
    ) -> Tensor {
        self.f_slow_conv_dilated3d(weight, kernel_size, bias, stride, padding, dilation)
            .unwrap()
    }

    pub fn slow_conv_transpose2d<T: Borrow<Tensor>>(
        &self,
        weight: &Tensor,
        kernel_size: &[i64],
        bias: Option<T>,
        stride: &[i64],
        padding: &[i64],
        output_padding: &[i64],
        dilation: &[i64],
    ) -> Tensor {
        self.f_slow_conv_transpose2d(
            weight,
            kernel_size,
            bias,
            stride,
            padding,
            output_padding,
            dilation,
        )
        .unwrap()
    }

    pub fn slow_conv_transpose2d_out<T: Borrow<Tensor>>(
        &self,
        out: &Tensor,
        weight: &Tensor,
        kernel_size: &[i64],
        bias: Option<T>,
        stride: &[i64],
        padding: &[i64],
        output_padding: &[i64],
        dilation: &[i64],
    ) -> Tensor {
        self.f_slow_conv_transpose2d_out(
            out,
            weight,
            kernel_size,
            bias,
            stride,
            padding,
            output_padding,
            dilation,
        )
        .unwrap()
    }

    pub fn slow_conv_transpose3d<T: Borrow<Tensor>>(
        &self,
        weight: &Tensor,
        kernel_size: &[i64],
        bias: Option<T>,
        stride: &[i64],
        padding: &[i64],
        output_padding: &[i64],
        dilation: &[i64],
    ) -> Tensor {
        self.f_slow_conv_transpose3d(
            weight,
            kernel_size,
            bias,
            stride,
            padding,
            output_padding,
            dilation,
        )
        .unwrap()
    }

    pub fn slow_conv_transpose3d_out<T: Borrow<Tensor>>(
        &self,
        out: &Tensor,
        weight: &Tensor,
        kernel_size: &[i64],
        bias: Option<T>,
        stride: &[i64],
        padding: &[i64],
        output_padding: &[i64],
        dilation: &[i64],
    ) -> Tensor {
        self.f_slow_conv_transpose3d_out(
            out,
            weight,
            kernel_size,
            bias,
            stride,
            padding,
            output_padding,
            dilation,
        )
        .unwrap()
    }

    pub fn smm(&self, mat2: &Tensor) -> Tensor {
        self.f_smm(mat2).unwrap()
    }

    pub fn smooth_l1_loss(&self, target: &Tensor, reduction: crate::Reduction) -> Tensor {
        self.f_smooth_l1_loss(target, reduction).unwrap()
    }

    pub fn smooth_l1_loss_backward(
        &self,
        grad_output: &Tensor,
        target: &Tensor,
        reduction: crate::Reduction,
    ) -> Tensor {
        self.f_smooth_l1_loss_backward(grad_output, target, reduction)
            .unwrap()
    }

    pub fn smooth_l1_loss_backward_out(
        &self,
        grad_input: &Tensor,
        grad_output: &Tensor,
        target: &Tensor,
        reduction: crate::Reduction,
    ) -> Tensor {
        self.f_smooth_l1_loss_backward_out(grad_input, grad_output, target, reduction)
            .unwrap()
    }

    pub fn smooth_l1_loss_out(
        &self,
        out: &Tensor,
        target: &Tensor,
        reduction: crate::Reduction,
    ) -> Tensor {
        self.f_smooth_l1_loss_out(out, target, reduction).unwrap()
    }

    pub fn soft_margin_loss(&self, target: &Tensor, reduction: crate::Reduction) -> Tensor {
        self.f_soft_margin_loss(target, reduction).unwrap()
    }

    pub fn soft_margin_loss_backward(
        &self,
        grad_output: &Tensor,
        target: &Tensor,
        reduction: crate::Reduction,
    ) -> Tensor {
        self.f_soft_margin_loss_backward(grad_output, target, reduction)
            .unwrap()
    }

    pub fn soft_margin_loss_backward_out(
        &self,
        grad_input: &Tensor,
        grad_output: &Tensor,
        target: &Tensor,
        reduction: crate::Reduction,
    ) -> Tensor {
        self.f_soft_margin_loss_backward_out(grad_input, grad_output, target, reduction)
            .unwrap()
    }

    pub fn soft_margin_loss_out(
        &self,
        out: &Tensor,
        target: &Tensor,
        reduction: crate::Reduction,
    ) -> Tensor {
        self.f_soft_margin_loss_out(out, target, reduction).unwrap()
    }

    pub fn softmax(&self, dim: i64, dtype: Kind) -> Tensor {
        self.f_softmax(dim, dtype).unwrap()
    }

    pub fn softplus(&self) -> Tensor {
        self.f_softplus().unwrap()
    }

    pub fn softplus_backward<S: Into<Scalar>>(
        &self,
        grad_output: &Tensor,
        beta: S,
        threshold: S,
        output: &Tensor,
    ) -> Tensor {
        self.f_softplus_backward(grad_output, beta, threshold, output)
            .unwrap()
    }

    pub fn softplus_backward_out<S: Into<Scalar>>(
        &self,
        grad_input: &Tensor,
        grad_output: &Tensor,
        beta: S,
        threshold: S,
        output: &Tensor,
    ) -> Tensor {
        self.f_softplus_backward_out(grad_input, grad_output, beta, threshold, output)
            .unwrap()
    }

    pub fn softplus_out(&self, out: &Tensor) -> Tensor {
        self.f_softplus_out(out).unwrap()
    }

    pub fn softshrink(&self) -> Tensor {
        self.f_softshrink().unwrap()
    }

    pub fn softshrink_backward<S: Into<Scalar>>(&self, grad_output: &Tensor, lambd: S) -> Tensor {
        self.f_softshrink_backward(grad_output, lambd).unwrap()
    }

    pub fn softshrink_backward_out<S: Into<Scalar>>(
        &self,
        grad_input: &Tensor,
        grad_output: &Tensor,
        lambd: S,
    ) -> Tensor {
        self.f_softshrink_backward_out(grad_input, grad_output, lambd)
            .unwrap()
    }

    pub fn softshrink_out(&self, out: &Tensor) -> Tensor {
        self.f_softshrink_out(out).unwrap()
    }

    pub fn solve(&self, a: &Tensor) -> (Tensor, Tensor) {
        self.f_solve(a).unwrap()
    }

    pub fn solve_out(&self, solution: &Tensor, lu: &Tensor, a: &Tensor) -> (Tensor, Tensor) {
        self.f_solve_out(solution, lu, a).unwrap()
    }

    pub fn sort(&self, dim: i64, descending: bool) -> (Tensor, Tensor) {
        self.f_sort(dim, descending).unwrap()
    }

    pub fn sort_out(
        &self,
        values: &Tensor,
        indices: &Tensor,
        dim: i64,
        descending: bool,
    ) -> (Tensor, Tensor) {
        self.f_sort_out(values, indices, dim, descending).unwrap()
    }

    pub fn sparse_coo_tensor(size: &[i64], options: (Kind, Device)) -> Tensor {
        Tensor::f_sparse_coo_tensor(size, options).unwrap()
    }

    pub fn sparse_coo_tensor1(
        indices: &Tensor,
        values: &Tensor,
        options: (Kind, Device),
    ) -> Tensor {
        Tensor::f_sparse_coo_tensor1(indices, values, options).unwrap()
    }

    pub fn sparse_coo_tensor2(
        indices: &Tensor,
        values: &Tensor,
        size: &[i64],
        options: (Kind, Device),
    ) -> Tensor {
        Tensor::f_sparse_coo_tensor2(indices, values, size, options).unwrap()
    }

    pub fn sparse_mask(&self, mask: &Tensor) -> Tensor {
        self.f_sparse_mask(mask).unwrap()
    }

    pub fn sparse_resize_(&mut self, size: &[i64], sparse_dim: i64, dense_dim: i64) -> Tensor {
        self.f_sparse_resize_(size, sparse_dim, dense_dim).unwrap()
    }

    pub fn sparse_resize_and_clear_(
        &mut self,
        size: &[i64],
        sparse_dim: i64,
        dense_dim: i64,
    ) -> Tensor {
        self.f_sparse_resize_and_clear_(size, sparse_dim, dense_dim)
            .unwrap()
    }

    pub fn split(&self, split_size: i64, dim: i64) -> Vec<Tensor> {
        self.f_split(split_size, dim).unwrap()
    }

    pub fn split_with_sizes(&self, split_sizes: &[i64], dim: i64) -> Vec<Tensor> {
        self.f_split_with_sizes(split_sizes, dim).unwrap()
    }

    pub fn sqrt(&self) -> Tensor {
        self.f_sqrt().unwrap()
    }

    pub fn sqrt_(&mut self) -> Tensor {
        self.f_sqrt_().unwrap()
    }

    pub fn sqrt_out(&self, out: &Tensor) -> Tensor {
        self.f_sqrt_out(out).unwrap()
    }

    pub fn squeeze(&self) -> Tensor {
        self.f_squeeze().unwrap()
    }

    pub fn squeeze1(&self, dim: i64) -> Tensor {
        self.f_squeeze1(dim).unwrap()
    }

    pub fn squeeze_(&mut self) -> Tensor {
        self.f_squeeze_().unwrap()
    }

    pub fn squeeze_1(&mut self, dim: i64) -> Tensor {
        self.f_squeeze_1(dim).unwrap()
    }

    pub fn sspaddmm(&self, mat1: &Tensor, mat2: &Tensor) -> Tensor {
        self.f_sspaddmm(mat1, mat2).unwrap()
    }

    pub fn sspaddmm_out(&self, out: &Tensor, mat1: &Tensor, mat2: &Tensor) -> Tensor {
        self.f_sspaddmm_out(out, mat1, mat2).unwrap()
    }

    pub fn stack<T: Borrow<Tensor>>(tensors: &[T], dim: i64) -> Tensor {
        Tensor::f_stack(tensors, dim).unwrap()
    }

    pub fn stack_out<T: Borrow<Tensor>>(out: &Tensor, tensors: &[T], dim: i64) -> Tensor {
        Tensor::f_stack_out(out, tensors, dim).unwrap()
    }

    pub fn std(&self, unbiased: bool) -> Tensor {
        self.f_std(unbiased).unwrap()
    }

    pub fn std1(&self, dim: &[i64], unbiased: bool, keepdim: bool) -> Tensor {
        self.f_std1(dim, unbiased, keepdim).unwrap()
    }

    pub fn std_mean(&self, unbiased: bool) -> (Tensor, Tensor) {
        self.f_std_mean(unbiased).unwrap()
    }

    pub fn std_mean1(&self, dim: &[i64], unbiased: bool, keepdim: bool) -> (Tensor, Tensor) {
        self.f_std_mean1(dim, unbiased, keepdim).unwrap()
    }

    pub fn std_out(&self, out: &Tensor, dim: &[i64], unbiased: bool, keepdim: bool) -> Tensor {
        self.f_std_out(out, dim, unbiased, keepdim).unwrap()
    }

    pub fn stft<T: Borrow<Tensor>>(
        &self,
        n_fft: i64,
        hop_length: i64,
        win_length: i64,
        window: Option<T>,
        normalized: bool,
        onesided: bool,
    ) -> Tensor {
        self.f_stft(n_fft, hop_length, win_length, window, normalized, onesided)
            .unwrap()
    }

    pub fn g_sub(&self, other: &Tensor) -> Tensor {
        self.f_sub(other).unwrap()
    }

    pub fn g_sub1<S: Into<Scalar>>(&self, other: S) -> Tensor {
        self.f_sub1(other).unwrap()
    }

    pub fn g_sub_(&mut self, other: &Tensor) -> Tensor {
        self.f_sub_(other).unwrap()
    }

    pub fn g_sub_1<S: Into<Scalar>>(&mut self, other: S) -> Tensor {
        self.f_sub_1(other).unwrap()
    }

    pub fn sub_out(&self, out: &Tensor, other: &Tensor) -> Tensor {
        self.f_sub_out(out, other).unwrap()
    }

    pub fn sum(&self, dtype: Kind) -> Tensor {
        self.f_sum(dtype).unwrap()
    }

    pub fn sum1(&self, dim: &[i64], keepdim: bool, dtype: Kind) -> Tensor {
        self.f_sum1(dim, keepdim, dtype).unwrap()
    }

    pub fn sum_out(&self, out: &Tensor, dim: &[i64], keepdim: bool, dtype: Kind) -> Tensor {
        self.f_sum_out(out, dim, keepdim, dtype).unwrap()
    }

    pub fn sum_to_size(&self, size: &[i64]) -> Tensor {
        self.f_sum_to_size(size).unwrap()
    }

    pub fn svd(&self, some: bool, compute_uv: bool) -> (Tensor, Tensor, Tensor) {
        self.f_svd(some, compute_uv).unwrap()
    }

    pub fn svd_out(
        &self,
        u: &Tensor,
        s: &Tensor,
        v: &Tensor,
        some: bool,
        compute_uv: bool,
    ) -> (Tensor, Tensor, Tensor) {
        self.f_svd_out(u, s, v, some, compute_uv).unwrap()
    }

    pub fn symeig(&self, eigenvectors: bool, upper: bool) -> (Tensor, Tensor) {
        self.f_symeig(eigenvectors, upper).unwrap()
    }

    pub fn symeig_out(
        &self,
        e: &Tensor,
        v: &Tensor,
        eigenvectors: bool,
        upper: bool,
    ) -> (Tensor, Tensor) {
        self.f_symeig_out(e, v, eigenvectors, upper).unwrap()
    }

    pub fn tr(&self) -> Tensor {
        self.f_tr().unwrap()
    }

    pub fn t_(&mut self) -> Tensor {
        self.f_t_().unwrap()
    }

    pub fn take(&self, index: &Tensor) -> Tensor {
        self.f_take(index).unwrap()
    }

    pub fn take_out(&self, out: &Tensor, index: &Tensor) -> Tensor {
        self.f_take_out(out, index).unwrap()
    }

    pub fn tan(&self) -> Tensor {
        self.f_tan().unwrap()
    }

    pub fn tan_(&mut self) -> Tensor {
        self.f_tan_().unwrap()
    }

    pub fn tan_out(&self, out: &Tensor) -> Tensor {
        self.f_tan_out(out).unwrap()
    }

    pub fn tanh(&self) -> Tensor {
        self.f_tanh().unwrap()
    }

    pub fn tanh_(&mut self) -> Tensor {
        self.f_tanh_().unwrap()
    }

    pub fn tanh_backward(grad_output: &Tensor, output: &Tensor) -> Tensor {
        Tensor::f_tanh_backward(grad_output, output).unwrap()
    }

    pub fn tanh_backward_out(grad_input: &Tensor, grad_output: &Tensor, output: &Tensor) -> Tensor {
        Tensor::f_tanh_backward_out(grad_input, grad_output, output).unwrap()
    }

    pub fn tanh_out(&self, out: &Tensor) -> Tensor {
        self.f_tanh_out(out).unwrap()
    }

    pub fn tensordot(&self, other: &Tensor, dims_self: &[i64], dims_other: &[i64]) -> Tensor {
        self.f_tensordot(other, dims_self, dims_other).unwrap()
    }

    pub fn threshold<S: Into<Scalar>>(&self, threshold: S, value: S) -> Tensor {
        self.f_threshold(threshold, value).unwrap()
    }

    pub fn threshold_<S: Into<Scalar>>(&mut self, threshold: S, value: S) -> Tensor {
        self.f_threshold_(threshold, value).unwrap()
    }

    pub fn threshold_backward<S: Into<Scalar>>(
        &self,
        grad_output: &Tensor,
        threshold: S,
    ) -> Tensor {
        self.f_threshold_backward(grad_output, threshold).unwrap()
    }

    pub fn threshold_out<S: Into<Scalar>>(&self, out: &Tensor, threshold: S, value: S) -> Tensor {
        self.f_threshold_out(out, threshold, value).unwrap()
    }

    pub fn to(&self, device: Device) -> Tensor {
        self.f_to(device).unwrap()
    }

    pub fn to1(&self, options: (Kind, Device), non_blocking: bool, copy: bool) -> Tensor {
        self.f_to1(options, non_blocking, copy).unwrap()
    }

    pub fn to2(&self, dtype: Kind, non_blocking: bool, copy: bool) -> Tensor {
        self.f_to2(dtype, non_blocking, copy).unwrap()
    }

    pub fn to3(&self, other: &Tensor, non_blocking: bool, copy: bool) -> Tensor {
        self.f_to3(other, non_blocking, copy).unwrap()
    }

    pub fn to4(&self, device: Device, dtype: Kind, non_blocking: bool, copy: bool) -> Tensor {
        self.f_to4(device, dtype, non_blocking, copy).unwrap()
    }

    pub fn to_dense(&self) -> Tensor {
        self.f_to_dense().unwrap()
    }

    pub fn to_dense_backward(&self, grad: &Tensor) -> Tensor {
        self.f_to_dense_backward(grad).unwrap()
    }

    pub fn to_mkldnn(&self) -> Tensor {
        self.f_to_mkldnn().unwrap()
    }

    pub fn to_mkldnn_backward(&self, grad: &Tensor) -> Tensor {
        self.f_to_mkldnn_backward(grad).unwrap()
    }

    pub fn to_sparse(&self) -> Tensor {
        self.f_to_sparse().unwrap()
    }

    pub fn to_sparse1(&self, sparse_dim: i64) -> Tensor {
        self.f_to_sparse1(sparse_dim).unwrap()
    }

    pub fn topk(&self, k: i64, dim: i64, largest: bool, sorted: bool) -> (Tensor, Tensor) {
        self.f_topk(k, dim, largest, sorted).unwrap()
    }

    pub fn topk_out(
        &self,
        values: &Tensor,
        indices: &Tensor,
        k: i64,
        dim: i64,
        largest: bool,
        sorted: bool,
    ) -> (Tensor, Tensor) {
        self.f_topk_out(values, indices, k, dim, largest, sorted)
            .unwrap()
    }

    pub fn totype(&self, scalar_type: Kind) -> Tensor {
        self.f_totype(scalar_type).unwrap()
    }

    pub fn trace(&self) -> Tensor {
        self.f_trace().unwrap()
    }

    pub fn transpose(&self, dim0: i64, dim1: i64) -> Tensor {
        self.f_transpose(dim0, dim1).unwrap()
    }

    pub fn transpose_(&mut self, dim0: i64, dim1: i64) -> Tensor {
        self.f_transpose_(dim0, dim1).unwrap()
    }

    pub fn trapz(y: &Tensor, x: &Tensor, dim: i64) -> Tensor {
        Tensor::f_trapz(y, x, dim).unwrap()
    }

    pub fn trapz1(y: &Tensor, dx: f64, dim: i64) -> Tensor {
        Tensor::f_trapz1(y, dx, dim).unwrap()
    }

    pub fn triangular_solve(
        &self,
        a: &Tensor,
        upper: bool,
        transpose: bool,
        unitriangular: bool,
    ) -> (Tensor, Tensor) {
        self.f_triangular_solve(a, upper, transpose, unitriangular)
            .unwrap()
    }

    pub fn triangular_solve_out(
        &self,
        x: &Tensor,
        m: &Tensor,
        a: &Tensor,
        upper: bool,
        transpose: bool,
        unitriangular: bool,
    ) -> (Tensor, Tensor) {
        self.f_triangular_solve_out(x, m, a, upper, transpose, unitriangular)
            .unwrap()
    }

    pub fn tril(&self, diagonal: i64) -> Tensor {
        self.f_tril(diagonal).unwrap()
    }

    pub fn tril_(&mut self, diagonal: i64) -> Tensor {
        self.f_tril_(diagonal).unwrap()
    }

    pub fn tril_indices(row: i64, col: i64, offset: i64, options: (Kind, Device)) -> Tensor {
        Tensor::f_tril_indices(row, col, offset, options).unwrap()
    }

    pub fn tril_out(&self, out: &Tensor, diagonal: i64) -> Tensor {
        self.f_tril_out(out, diagonal).unwrap()
    }

    pub fn triplet_margin_loss(
        anchor: &Tensor,
        positive: &Tensor,
        negative: &Tensor,
        margin: f64,
        p: f64,
        eps: f64,
        swap: bool,
        reduction: crate::Reduction,
    ) -> Tensor {
        Tensor::f_triplet_margin_loss(anchor, positive, negative, margin, p, eps, swap, reduction)
            .unwrap()
    }

    pub fn triu(&self, diagonal: i64) -> Tensor {
        self.f_triu(diagonal).unwrap()
    }

    pub fn triu_(&mut self, diagonal: i64) -> Tensor {
        self.f_triu_(diagonal).unwrap()
    }

    pub fn triu_indices(row: i64, col: i64, offset: i64, options: (Kind, Device)) -> Tensor {
        Tensor::f_triu_indices(row, col, offset, options).unwrap()
    }

    pub fn triu_out(&self, out: &Tensor, diagonal: i64) -> Tensor {
        self.f_triu_out(out, diagonal).unwrap()
    }

    pub fn trunc(&self) -> Tensor {
        self.f_trunc().unwrap()
    }

    pub fn trunc_(&mut self) -> Tensor {
        self.f_trunc_().unwrap()
    }

    pub fn trunc_out(&self, out: &Tensor) -> Tensor {
        self.f_trunc_out(out).unwrap()
    }

    pub fn type_as(&self, other: &Tensor) -> Tensor {
        self.f_type_as(other).unwrap()
    }

    pub fn unbind(&self, dim: i64) -> Vec<Tensor> {
        self.f_unbind(dim).unwrap()
    }

    pub fn unfold(&self, dimension: i64, size: i64, step: i64) -> Tensor {
        self.f_unfold(dimension, size, step).unwrap()
    }

    pub fn uniform_(&mut self, from: f64, to: f64) -> Tensor {
        self.f_uniform_(from, to).unwrap()
    }

    pub fn unique_consecutive(
        &self,
        return_inverse: bool,
        return_counts: bool,
        dim: i64,
    ) -> (Tensor, Tensor, Tensor) {
        self.f_unique_consecutive(return_inverse, return_counts, dim)
            .unwrap()
    }

    pub fn unique_dim(
        &self,
        dim: i64,
        sorted: bool,
        return_inverse: bool,
        return_counts: bool,
    ) -> (Tensor, Tensor, Tensor) {
        self.f_unique_dim(dim, sorted, return_inverse, return_counts)
            .unwrap()
    }

    pub fn unique_dim_consecutive(
        &self,
        dim: i64,
        return_inverse: bool,
        return_counts: bool,
    ) -> (Tensor, Tensor, Tensor) {
        self.f_unique_dim_consecutive(dim, return_inverse, return_counts)
            .unwrap()
    }

    pub fn unsqueeze(&self, dim: i64) -> Tensor {
        self.f_unsqueeze(dim).unwrap()
    }

    pub fn unsqueeze_(&mut self, dim: i64) -> Tensor {
        self.f_unsqueeze_(dim).unwrap()
    }

    pub fn upsample_bicubic2d(&self, output_size: &[i64], align_corners: bool) -> Tensor {
        self.f_upsample_bicubic2d(output_size, align_corners)
            .unwrap()
    }

    pub fn upsample_bicubic2d_backward(
        grad_output: &Tensor,
        output_size: &[i64],
        input_size: &[i64],
        align_corners: bool,
    ) -> Tensor {
        Tensor::f_upsample_bicubic2d_backward(grad_output, output_size, input_size, align_corners)
            .unwrap()
    }

    pub fn upsample_bicubic2d_backward_out(
        grad_input: &Tensor,
        grad_output: &Tensor,
        output_size: &[i64],
        input_size: &[i64],
        align_corners: bool,
    ) -> Tensor {
        Tensor::f_upsample_bicubic2d_backward_out(
            grad_input,
            grad_output,
            output_size,
            input_size,
            align_corners,
        )
        .unwrap()
    }

    pub fn upsample_bicubic2d_out(
        &self,
        out: &Tensor,
        output_size: &[i64],
        align_corners: bool,
    ) -> Tensor {
        self.f_upsample_bicubic2d_out(out, output_size, align_corners)
            .unwrap()
    }

    pub fn upsample_bilinear2d(&self, output_size: &[i64], align_corners: bool) -> Tensor {
        self.f_upsample_bilinear2d(output_size, align_corners)
            .unwrap()
    }

    pub fn upsample_bilinear2d_backward(
        grad_output: &Tensor,
        output_size: &[i64],
        input_size: &[i64],
        align_corners: bool,
    ) -> Tensor {
        Tensor::f_upsample_bilinear2d_backward(grad_output, output_size, input_size, align_corners)
            .unwrap()
    }

    pub fn upsample_bilinear2d_backward_out(
        grad_input: &Tensor,
        grad_output: &Tensor,
        output_size: &[i64],
        input_size: &[i64],
        align_corners: bool,
    ) -> Tensor {
        Tensor::f_upsample_bilinear2d_backward_out(
            grad_input,
            grad_output,
            output_size,
            input_size,
            align_corners,
        )
        .unwrap()
    }

    pub fn upsample_bilinear2d_out(
        &self,
        out: &Tensor,
        output_size: &[i64],
        align_corners: bool,
    ) -> Tensor {
        self.f_upsample_bilinear2d_out(out, output_size, align_corners)
            .unwrap()
    }

    pub fn upsample_linear1d(&self, output_size: &[i64], align_corners: bool) -> Tensor {
        self.f_upsample_linear1d(output_size, align_corners)
            .unwrap()
    }

    pub fn upsample_linear1d_backward(
        grad_output: &Tensor,
        output_size: &[i64],
        input_size: &[i64],
        align_corners: bool,
    ) -> Tensor {
        Tensor::f_upsample_linear1d_backward(grad_output, output_size, input_size, align_corners)
            .unwrap()
    }

    pub fn upsample_linear1d_backward_out(
        grad_input: &Tensor,
        grad_output: &Tensor,
        output_size: &[i64],
        input_size: &[i64],
        align_corners: bool,
    ) -> Tensor {
        Tensor::f_upsample_linear1d_backward_out(
            grad_input,
            grad_output,
            output_size,
            input_size,
            align_corners,
        )
        .unwrap()
    }

    pub fn upsample_linear1d_out(
        &self,
        out: &Tensor,
        output_size: &[i64],
        align_corners: bool,
    ) -> Tensor {
        self.f_upsample_linear1d_out(out, output_size, align_corners)
            .unwrap()
    }

    pub fn upsample_nearest1d(&self, output_size: &[i64]) -> Tensor {
        self.f_upsample_nearest1d(output_size).unwrap()
    }

    pub fn upsample_nearest1d_backward(
        grad_output: &Tensor,
        output_size: &[i64],
        input_size: &[i64],
    ) -> Tensor {
        Tensor::f_upsample_nearest1d_backward(grad_output, output_size, input_size).unwrap()
    }

    pub fn upsample_nearest1d_backward_out(
        grad_input: &Tensor,
        grad_output: &Tensor,
        output_size: &[i64],
        input_size: &[i64],
    ) -> Tensor {
        Tensor::f_upsample_nearest1d_backward_out(grad_input, grad_output, output_size, input_size)
            .unwrap()
    }

    pub fn upsample_nearest1d_out(&self, out: &Tensor, output_size: &[i64]) -> Tensor {
        self.f_upsample_nearest1d_out(out, output_size).unwrap()
    }

    pub fn upsample_nearest2d(&self, output_size: &[i64]) -> Tensor {
        self.f_upsample_nearest2d(output_size).unwrap()
    }

    pub fn upsample_nearest2d_backward(
        grad_output: &Tensor,
        output_size: &[i64],
        input_size: &[i64],
    ) -> Tensor {
        Tensor::f_upsample_nearest2d_backward(grad_output, output_size, input_size).unwrap()
    }

    pub fn upsample_nearest2d_backward_out(
        grad_input: &Tensor,
        grad_output: &Tensor,
        output_size: &[i64],
        input_size: &[i64],
    ) -> Tensor {
        Tensor::f_upsample_nearest2d_backward_out(grad_input, grad_output, output_size, input_size)
            .unwrap()
    }

    pub fn upsample_nearest2d_out(&self, out: &Tensor, output_size: &[i64]) -> Tensor {
        self.f_upsample_nearest2d_out(out, output_size).unwrap()
    }

    pub fn upsample_nearest3d(&self, output_size: &[i64]) -> Tensor {
        self.f_upsample_nearest3d(output_size).unwrap()
    }

    pub fn upsample_nearest3d_backward(
        grad_output: &Tensor,
        output_size: &[i64],
        input_size: &[i64],
    ) -> Tensor {
        Tensor::f_upsample_nearest3d_backward(grad_output, output_size, input_size).unwrap()
    }

    pub fn upsample_nearest3d_backward_out(
        grad_input: &Tensor,
        grad_output: &Tensor,
        output_size: &[i64],
        input_size: &[i64],
    ) -> Tensor {
        Tensor::f_upsample_nearest3d_backward_out(grad_input, grad_output, output_size, input_size)
            .unwrap()
    }

    pub fn upsample_nearest3d_out(&self, out: &Tensor, output_size: &[i64]) -> Tensor {
        self.f_upsample_nearest3d_out(out, output_size).unwrap()
    }

    pub fn upsample_trilinear3d(&self, output_size: &[i64], align_corners: bool) -> Tensor {
        self.f_upsample_trilinear3d(output_size, align_corners)
            .unwrap()
    }

    pub fn upsample_trilinear3d_backward(
        grad_output: &Tensor,
        output_size: &[i64],
        input_size: &[i64],
        align_corners: bool,
    ) -> Tensor {
        Tensor::f_upsample_trilinear3d_backward(grad_output, output_size, input_size, align_corners)
            .unwrap()
    }

    pub fn upsample_trilinear3d_backward_out(
        grad_input: &Tensor,
        grad_output: &Tensor,
        output_size: &[i64],
        input_size: &[i64],
        align_corners: bool,
    ) -> Tensor {
        Tensor::f_upsample_trilinear3d_backward_out(
            grad_input,
            grad_output,
            output_size,
            input_size,
            align_corners,
        )
        .unwrap()
    }

    pub fn upsample_trilinear3d_out(
        &self,
        out: &Tensor,
        output_size: &[i64],
        align_corners: bool,
    ) -> Tensor {
        self.f_upsample_trilinear3d_out(out, output_size, align_corners)
            .unwrap()
    }

    pub fn values(&self) -> Tensor {
        self.f_values().unwrap()
    }

    pub fn var(&self, unbiased: bool) -> Tensor {
        self.f_var(unbiased).unwrap()
    }

    pub fn var1(&self, dim: &[i64], unbiased: bool, keepdim: bool) -> Tensor {
        self.f_var1(dim, unbiased, keepdim).unwrap()
    }

    pub fn var_mean(&self, unbiased: bool) -> (Tensor, Tensor) {
        self.f_var_mean(unbiased).unwrap()
    }

    pub fn var_mean1(&self, dim: &[i64], unbiased: bool, keepdim: bool) -> (Tensor, Tensor) {
        self.f_var_mean1(dim, unbiased, keepdim).unwrap()
    }

    pub fn var_out(&self, out: &Tensor, dim: &[i64], unbiased: bool, keepdim: bool) -> Tensor {
        self.f_var_out(out, dim, unbiased, keepdim).unwrap()
    }

    pub fn view_(&self, size: &[i64]) -> Tensor {
        self.f_view_(size).unwrap()
    }

    pub fn view_as(&self, other: &Tensor) -> Tensor {
        self.f_view_as(other).unwrap()
    }

    pub fn where_(condition: &Tensor) -> Vec<Tensor> {
        Tensor::f_where_(condition).unwrap()
    }

    pub fn where1(&self, condition: &Tensor, other: &Tensor) -> Tensor {
        self.f_where1(condition, other).unwrap()
    }

    pub fn zero_(&mut self) -> Tensor {
        self.f_zero_().unwrap()
    }

    pub fn zeros(size: &[i64], options: (Kind, Device)) -> Tensor {
        Tensor::f_zeros(size, options).unwrap()
    }

    pub fn zeros_like(&self) -> Tensor {
        self.f_zeros_like().unwrap()
    }

    pub fn zeros_like1(&self, options: (Kind, Device)) -> Tensor {
        self.f_zeros_like1(options).unwrap()
    }

    pub fn zeros_out(out: &Tensor, size: &[i64]) -> Tensor {
        Tensor::f_zeros_out(out, size).unwrap()
    }
}
