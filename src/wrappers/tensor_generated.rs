/* THIS FILE IS AUTOMATICALLY GENERATED, DO NOT EDIT BY HAND! */
#[allow(clippy::all)]
use crate::{Device, Kind, Scalar, Tensor};
use std::convert::Into;
use std::borrow::Borrow;

impl Tensor {

    pub fn abs(
        &self, 
    ) -> Tensor {
        self.f_abs().unwrap()
    }

    pub fn abs_(
        &self, 
    ) -> Tensor {
        self.f_abs_().unwrap()
    }

    pub fn abs_out(
        &self, result: &Tensor
    ) -> Tensor {
        self.f_abs_out(result).unwrap()
    }

    pub fn acos(
        &self, 
    ) -> Tensor {
        self.f_acos().unwrap()
    }

    pub fn acos_(
        &self, 
    ) -> Tensor {
        self.f_acos_().unwrap()
    }

    pub fn acos_out(
        &self, result: &Tensor
    ) -> Tensor {
        self.f_acos_out(result).unwrap()
    }

    pub fn adaptive_avg_pool1d(
        &self, output_size: &[i64]
    ) -> Tensor {
        self.f_adaptive_avg_pool1d(output_size).unwrap()
    }

    pub fn adaptive_avg_pool2d(
        &self, output_size: &[i64]
    ) -> Tensor {
        self.f_adaptive_avg_pool2d(output_size).unwrap()
    }

    pub fn adaptive_avg_pool2d_backward(
        &self, grad_output: &Tensor
    ) -> Tensor {
        self.f_adaptive_avg_pool2d_backward(grad_output).unwrap()
    }

    pub fn adaptive_avg_pool2d_backward_out(
        &self, grad_input: &Tensor, grad_output: &Tensor
    ) -> Tensor {
        self.f_adaptive_avg_pool2d_backward_out(grad_input, grad_output).unwrap()
    }

    pub fn adaptive_avg_pool2d_out(
        &self, output: &Tensor, output_size: &[i64]
    ) -> Tensor {
        self.f_adaptive_avg_pool2d_out(output, output_size).unwrap()
    }

    pub fn adaptive_avg_pool3d(
        &self, output_size: &[i64]
    ) -> Tensor {
        self.f_adaptive_avg_pool3d(output_size).unwrap()
    }

    pub fn adaptive_avg_pool3d_backward(
        &self, grad_output: &Tensor
    ) -> Tensor {
        self.f_adaptive_avg_pool3d_backward(grad_output).unwrap()
    }

    pub fn adaptive_avg_pool3d_backward_out(
        &self, grad_input: &Tensor, grad_output: &Tensor
    ) -> Tensor {
        self.f_adaptive_avg_pool3d_backward_out(grad_input, grad_output).unwrap()
    }

    pub fn adaptive_avg_pool3d_out(
        &self, output: &Tensor, output_size: &[i64]
    ) -> Tensor {
        self.f_adaptive_avg_pool3d_out(output, output_size).unwrap()
    }

    pub fn adaptive_max_pool1d(
        &self, output_size: &[i64]
    ) -> (Tensor, Tensor) {
        self.f_adaptive_max_pool1d(output_size).unwrap()
    }

    pub fn adaptive_max_pool2d(
        &self, output_size: &[i64]
    ) -> (Tensor, Tensor) {
        self.f_adaptive_max_pool2d(output_size).unwrap()
    }

    pub fn adaptive_max_pool2d_backward(
        &self, grad_output: &Tensor, indices: &Tensor
    ) -> Tensor {
        self.f_adaptive_max_pool2d_backward(grad_output, indices).unwrap()
    }

    pub fn adaptive_max_pool2d_backward_out(
        &self, grad_input: &Tensor, grad_output: &Tensor, indices: &Tensor
    ) -> Tensor {
        self.f_adaptive_max_pool2d_backward_out(grad_input, grad_output, indices).unwrap()
    }

    pub fn adaptive_max_pool2d_out(
        &self, output: &Tensor, indices: &Tensor, output_size: &[i64]
    ) -> (Tensor, Tensor) {
        self.f_adaptive_max_pool2d_out(output, indices, output_size).unwrap()
    }

    pub fn adaptive_max_pool3d(
        &self, output_size: &[i64]
    ) -> (Tensor, Tensor) {
        self.f_adaptive_max_pool3d(output_size).unwrap()
    }

    pub fn adaptive_max_pool3d_backward(
        &self, grad_output: &Tensor, indices: &Tensor
    ) -> Tensor {
        self.f_adaptive_max_pool3d_backward(grad_output, indices).unwrap()
    }

    pub fn adaptive_max_pool3d_backward_out(
        &self, grad_input: &Tensor, grad_output: &Tensor, indices: &Tensor
    ) -> Tensor {
        self.f_adaptive_max_pool3d_backward_out(grad_input, grad_output, indices).unwrap()
    }

    pub fn adaptive_max_pool3d_out(
        &self, output: &Tensor, indices: &Tensor, output_size: &[i64]
    ) -> (Tensor, Tensor) {
        self.f_adaptive_max_pool3d_out(output, indices, output_size).unwrap()
    }

    pub fn g_add(
        &self, other: &Tensor
    ) -> Tensor {
        self.f_add(other).unwrap()
    }

    pub fn g_add1<S: Into<Scalar>>(
        &self, other: S
    ) -> Tensor {
        self.f_add1(other).unwrap()
    }

    pub fn g_add_(
        &self, other: &Tensor
    ) -> Tensor {
        self.f_add_(other).unwrap()
    }

    pub fn g_add_1<S: Into<Scalar>>(
        &self, other: S
    ) -> Tensor {
        self.f_add_1(other).unwrap()
    }

    pub fn add_out(
        &self, result: &Tensor, other: &Tensor
    ) -> Tensor {
        self.f_add_out(result, other).unwrap()
    }

    pub fn addbmm(
        &self, batch1: &Tensor, batch2: &Tensor
    ) -> Tensor {
        self.f_addbmm(batch1, batch2).unwrap()
    }

    pub fn addbmm_(
        &self, batch1: &Tensor, batch2: &Tensor
    ) -> Tensor {
        self.f_addbmm_(batch1, batch2).unwrap()
    }

    pub fn addbmm_out(
        &self, result: &Tensor, batch1: &Tensor, batch2: &Tensor
    ) -> Tensor {
        self.f_addbmm_out(result, batch1, batch2).unwrap()
    }

    pub fn addcdiv(
        &self, tensor1: &Tensor, tensor2: &Tensor
    ) -> Tensor {
        self.f_addcdiv(tensor1, tensor2).unwrap()
    }

    pub fn addcdiv_(
        &self, tensor1: &Tensor, tensor2: &Tensor
    ) -> Tensor {
        self.f_addcdiv_(tensor1, tensor2).unwrap()
    }

    pub fn addcdiv_out(
        &self, result: &Tensor, tensor1: &Tensor, tensor2: &Tensor
    ) -> Tensor {
        self.f_addcdiv_out(result, tensor1, tensor2).unwrap()
    }

    pub fn addcmul(
        &self, tensor1: &Tensor, tensor2: &Tensor
    ) -> Tensor {
        self.f_addcmul(tensor1, tensor2).unwrap()
    }

    pub fn addcmul_(
        &self, tensor1: &Tensor, tensor2: &Tensor
    ) -> Tensor {
        self.f_addcmul_(tensor1, tensor2).unwrap()
    }

    pub fn addcmul_out(
        &self, result: &Tensor, tensor1: &Tensor, tensor2: &Tensor
    ) -> Tensor {
        self.f_addcmul_out(result, tensor1, tensor2).unwrap()
    }

    pub fn addmm(
        &self, mat1: &Tensor, mat2: &Tensor
    ) -> Tensor {
        self.f_addmm(mat1, mat2).unwrap()
    }

    pub fn addmm_(
        &self, mat1: &Tensor, mat2: &Tensor
    ) -> Tensor {
        self.f_addmm_(mat1, mat2).unwrap()
    }

    pub fn addmm_out(
        &self, result: &Tensor, mat1: &Tensor, mat2: &Tensor
    ) -> Tensor {
        self.f_addmm_out(result, mat1, mat2).unwrap()
    }

    pub fn addmv(
        &self, mat: &Tensor, vec: &Tensor
    ) -> Tensor {
        self.f_addmv(mat, vec).unwrap()
    }

    pub fn addmv_(
        &self, mat: &Tensor, vec: &Tensor
    ) -> Tensor {
        self.f_addmv_(mat, vec).unwrap()
    }

    pub fn addmv_out(
        &self, result: &Tensor, mat: &Tensor, vec: &Tensor
    ) -> Tensor {
        self.f_addmv_out(result, mat, vec).unwrap()
    }

    pub fn addr(
        &self, vec1: &Tensor, vec2: &Tensor
    ) -> Tensor {
        self.f_addr(vec1, vec2).unwrap()
    }

    pub fn addr_(
        &self, vec1: &Tensor, vec2: &Tensor
    ) -> Tensor {
        self.f_addr_(vec1, vec2).unwrap()
    }

    pub fn addr_out(
        &self, result: &Tensor, vec1: &Tensor, vec2: &Tensor
    ) -> Tensor {
        self.f_addr_out(result, vec1, vec2).unwrap()
    }

    pub fn alias(
        &self, 
    ) -> Tensor {
        self.f_alias().unwrap()
    }

    pub fn all(
        &self, 
    ) -> Tensor {
        self.f_all().unwrap()
    }

    pub fn all1(
        &self, dim: i64, keepdim: bool
    ) -> Tensor {
        self.f_all1(dim, keepdim).unwrap()
    }

    pub fn all_out(
        &self, result: &Tensor, dim: i64, keepdim: bool
    ) -> Tensor {
        self.f_all_out(result, dim, keepdim).unwrap()
    }

    pub fn alpha_dropout(
        &self, p: f64, train: bool
    ) -> Tensor {
        self.f_alpha_dropout(p, train).unwrap()
    }

    pub fn alpha_dropout_(
        &self, p: f64, train: bool
    ) -> Tensor {
        self.f_alpha_dropout_(p, train).unwrap()
    }

    pub fn any(
        &self, 
    ) -> Tensor {
        self.f_any().unwrap()
    }

    pub fn any1(
        &self, dim: i64, keepdim: bool
    ) -> Tensor {
        self.f_any1(dim, keepdim).unwrap()
    }

    pub fn any_out(
        &self, result: &Tensor, dim: i64, keepdim: bool
    ) -> Tensor {
        self.f_any_out(result, dim, keepdim).unwrap()
    }

    pub fn arange<S: Into<Scalar>>(
        end: S, options: (Kind, Device)
    ) -> Tensor {
        Tensor::f_arange(end, options).unwrap()
    }

    pub fn arange1<S: Into<Scalar>>(
        start: S, end: S, options: (Kind, Device)
    ) -> Tensor {
        Tensor::f_arange1(start, end, options).unwrap()
    }

    pub fn arange2<S: Into<Scalar>>(
        start: S, end: S, step: S, options: (Kind, Device)
    ) -> Tensor {
        Tensor::f_arange2(start, end, step, options).unwrap()
    }

    pub fn arange_out<S: Into<Scalar>>(
        result: &Tensor, end: S
    ) -> Tensor {
        Tensor::f_arange_out(result, end).unwrap()
    }

    pub fn arange_out1<S: Into<Scalar>>(
        result: &Tensor, start: S, end: S
    ) -> Tensor {
        Tensor::f_arange_out1(result, start, end).unwrap()
    }

    pub fn arange_out2<S: Into<Scalar>>(
        result: &Tensor, start: S, end: S, step: S
    ) -> Tensor {
        Tensor::f_arange_out2(result, start, end, step).unwrap()
    }

    pub fn argmax(
        &self, 
    ) -> Tensor {
        self.f_argmax().unwrap()
    }

    pub fn argmax1(
        &self, dim: i64, keepdim: bool
    ) -> Tensor {
        self.f_argmax1(dim, keepdim).unwrap()
    }

    pub fn argmin(
        &self, 
    ) -> Tensor {
        self.f_argmin().unwrap()
    }

    pub fn argmin1(
        &self, dim: i64, keepdim: bool
    ) -> Tensor {
        self.f_argmin1(dim, keepdim).unwrap()
    }

    pub fn as_strided(
        &self, size: &[i64], stride: &[i64]
    ) -> Tensor {
        self.f_as_strided(size, stride).unwrap()
    }

    pub fn as_strided1(
        &self, size: &[i64], stride: &[i64], storage_offset: i64
    ) -> Tensor {
        self.f_as_strided1(size, stride, storage_offset).unwrap()
    }

    pub fn as_strided_(
        &self, size: &[i64], stride: &[i64]
    ) -> Tensor {
        self.f_as_strided_(size, stride).unwrap()
    }

    pub fn as_strided_1(
        &self, size: &[i64], stride: &[i64], storage_offset: i64
    ) -> Tensor {
        self.f_as_strided_1(size, stride, storage_offset).unwrap()
    }

    pub fn asin(
        &self, 
    ) -> Tensor {
        self.f_asin().unwrap()
    }

    pub fn asin_(
        &self, 
    ) -> Tensor {
        self.f_asin_().unwrap()
    }

    pub fn asin_out(
        &self, result: &Tensor
    ) -> Tensor {
        self.f_asin_out(result).unwrap()
    }

    pub fn atan(
        &self, 
    ) -> Tensor {
        self.f_atan().unwrap()
    }

    pub fn atan2(
        &self, other: &Tensor
    ) -> Tensor {
        self.f_atan2(other).unwrap()
    }

    pub fn atan2_(
        &self, other: &Tensor
    ) -> Tensor {
        self.f_atan2_(other).unwrap()
    }

    pub fn atan2_out(
        &self, result: &Tensor, other: &Tensor
    ) -> Tensor {
        self.f_atan2_out(result, other).unwrap()
    }

    pub fn atan_(
        &self, 
    ) -> Tensor {
        self.f_atan_().unwrap()
    }

    pub fn atan_out(
        &self, result: &Tensor
    ) -> Tensor {
        self.f_atan_out(result).unwrap()
    }

    pub fn avg_pool1d(
        &self, kernel_size: &[i64], stride: &[i64], padding: &[i64], ceil_mode: bool, count_include_pad: bool
    ) -> Tensor {
        self.f_avg_pool1d(kernel_size, stride, padding, ceil_mode, count_include_pad).unwrap()
    }

    pub fn avg_pool2d(
        &self, kernel_size: &[i64], stride: &[i64], padding: &[i64], ceil_mode: bool, count_include_pad: bool
    ) -> Tensor {
        self.f_avg_pool2d(kernel_size, stride, padding, ceil_mode, count_include_pad).unwrap()
    }

    pub fn avg_pool2d_backward(
        &self, grad_output: &Tensor, kernel_size: &[i64], stride: &[i64], padding: &[i64], ceil_mode: bool, count_include_pad: bool
    ) -> Tensor {
        self.f_avg_pool2d_backward(grad_output, kernel_size, stride, padding, ceil_mode, count_include_pad).unwrap()
    }

    pub fn avg_pool2d_backward_out(
        &self, grad_input: &Tensor, grad_output: &Tensor, kernel_size: &[i64], stride: &[i64], padding: &[i64], ceil_mode: bool, count_include_pad: bool
    ) -> Tensor {
        self.f_avg_pool2d_backward_out(grad_input, grad_output, kernel_size, stride, padding, ceil_mode, count_include_pad).unwrap()
    }

    pub fn avg_pool2d_out(
        &self, output: &Tensor, kernel_size: &[i64], stride: &[i64], padding: &[i64], ceil_mode: bool, count_include_pad: bool
    ) -> Tensor {
        self.f_avg_pool2d_out(output, kernel_size, stride, padding, ceil_mode, count_include_pad).unwrap()
    }

    pub fn avg_pool3d(
        &self, kernel_size: &[i64], stride: &[i64], padding: &[i64], ceil_mode: bool, count_include_pad: bool
    ) -> Tensor {
        self.f_avg_pool3d(kernel_size, stride, padding, ceil_mode, count_include_pad).unwrap()
    }

    pub fn avg_pool3d_backward(
        &self, grad_output: &Tensor, kernel_size: &[i64], stride: &[i64], padding: &[i64], ceil_mode: bool, count_include_pad: bool
    ) -> Tensor {
        self.f_avg_pool3d_backward(grad_output, kernel_size, stride, padding, ceil_mode, count_include_pad).unwrap()
    }

    pub fn avg_pool3d_backward_out(
        &self, grad_input: &Tensor, grad_output: &Tensor, kernel_size: &[i64], stride: &[i64], padding: &[i64], ceil_mode: bool, count_include_pad: bool
    ) -> Tensor {
        self.f_avg_pool3d_backward_out(grad_input, grad_output, kernel_size, stride, padding, ceil_mode, count_include_pad).unwrap()
    }

    pub fn avg_pool3d_out(
        &self, output: &Tensor, kernel_size: &[i64], stride: &[i64], padding: &[i64], ceil_mode: bool, count_include_pad: bool
    ) -> Tensor {
        self.f_avg_pool3d_out(output, kernel_size, stride, padding, ceil_mode, count_include_pad).unwrap()
    }

    pub fn baddbmm(
        &self, batch1: &Tensor, batch2: &Tensor
    ) -> Tensor {
        self.f_baddbmm(batch1, batch2).unwrap()
    }

    pub fn baddbmm_(
        &self, batch1: &Tensor, batch2: &Tensor
    ) -> Tensor {
        self.f_baddbmm_(batch1, batch2).unwrap()
    }

    pub fn baddbmm_out(
        &self, result: &Tensor, batch1: &Tensor, batch2: &Tensor
    ) -> Tensor {
        self.f_baddbmm_out(result, batch1, batch2).unwrap()
    }

    pub fn bartlett_window(
        window_length: i64, options: (Kind, Device)
    ) -> Tensor {
        Tensor::f_bartlett_window(window_length, options).unwrap()
    }

    pub fn bartlett_window1(
        window_length: i64, periodic: bool, options: (Kind, Device)
    ) -> Tensor {
        Tensor::f_bartlett_window1(window_length, periodic, options).unwrap()
    }

    pub fn batch_norm<T: Borrow<Tensor>>(
        &self, weight: Option<T>, bias: Option<T>, running_mean: Option<T>, running_var: Option<T>, training: bool, momentum: f64, eps: f64, cudnn_enabled: bool
    ) -> Tensor {
        self.f_batch_norm(weight, bias, running_mean, running_var, training, momentum, eps, cudnn_enabled).unwrap()
    }

    pub fn bernoulli(
        &self, 
    ) -> Tensor {
        self.f_bernoulli().unwrap()
    }

    pub fn bernoulli1(
        &self, p: f64
    ) -> Tensor {
        self.f_bernoulli1(p).unwrap()
    }

    pub fn bernoulli_(
        &self, p: &Tensor
    ) -> Tensor {
        self.f_bernoulli_(p).unwrap()
    }

    pub fn bernoulli_1(
        &self, p: f64
    ) -> Tensor {
        self.f_bernoulli_1(p).unwrap()
    }

    pub fn bernoulli_out(
        &self, result: &Tensor
    ) -> Tensor {
        self.f_bernoulli_out(result).unwrap()
    }

    pub fn bilinear<T: Borrow<Tensor>>(
        input1: &Tensor, input2: &Tensor, weight: &Tensor, bias: Option<T>
    ) -> Tensor {
        Tensor::f_bilinear(input1, input2, weight, bias).unwrap()
    }

    pub fn binary_cross_entropy(
        &self, target: &Tensor, weight: &Tensor, reduction: i64
    ) -> Tensor {
        self.f_binary_cross_entropy(target, weight, reduction).unwrap()
    }

    pub fn binary_cross_entropy_backward<T: Borrow<Tensor>>(
        &self, grad_output: &Tensor, target: &Tensor, weight: Option<T>, reduction: i64
    ) -> Tensor {
        self.f_binary_cross_entropy_backward(grad_output, target, weight, reduction).unwrap()
    }

    pub fn binary_cross_entropy_backward_out<T: Borrow<Tensor>>(
        &self, grad_input: &Tensor, grad_output: &Tensor, target: &Tensor, weight: Option<T>, reduction: i64
    ) -> Tensor {
        self.f_binary_cross_entropy_backward_out(grad_input, grad_output, target, weight, reduction).unwrap()
    }

    pub fn binary_cross_entropy_out(
        &self, output: &Tensor, target: &Tensor, weight: &Tensor, reduction: i64
    ) -> Tensor {
        self.f_binary_cross_entropy_out(output, target, weight, reduction).unwrap()
    }

    pub fn binary_cross_entropy_with_logits<T: Borrow<Tensor>>(
        &self, target: &Tensor, weight: Option<T>, pos_weight: Option<T>, reduction: i64
    ) -> Tensor {
        self.f_binary_cross_entropy_with_logits(target, weight, pos_weight, reduction).unwrap()
    }

    pub fn binary_cross_entropy_with_logits_backward<T: Borrow<Tensor>>(
        &self, grad_output: &Tensor, target: &Tensor, weight: Option<T>, pos_weight: Option<T>, reduction: i64
    ) -> Tensor {
        self.f_binary_cross_entropy_with_logits_backward(grad_output, target, weight, pos_weight, reduction).unwrap()
    }

    pub fn bincount<T: Borrow<Tensor>>(
        &self, weights: Option<T>, minlength: i64
    ) -> Tensor {
        self.f_bincount(weights, minlength).unwrap()
    }

    pub fn blackman_window(
        window_length: i64, options: (Kind, Device)
    ) -> Tensor {
        Tensor::f_blackman_window(window_length, options).unwrap()
    }

    pub fn blackman_window1(
        window_length: i64, periodic: bool, options: (Kind, Device)
    ) -> Tensor {
        Tensor::f_blackman_window1(window_length, periodic, options).unwrap()
    }

    pub fn bmm(
        &self, mat2: &Tensor
    ) -> Tensor {
        self.f_bmm(mat2).unwrap()
    }

    pub fn bmm_out(
        &self, result: &Tensor, mat2: &Tensor
    ) -> Tensor {
        self.f_bmm_out(result, mat2).unwrap()
    }

    pub fn broadcast_tensors<T: Borrow<Tensor>>(
        tensors: &[T]
    ) -> Vec<Tensor> {
        Tensor::f_broadcast_tensors(tensors).unwrap()
    }

    pub fn btrifact(
        &self, pivot: bool
    ) -> (Tensor, Tensor) {
        self.f_btrifact(pivot).unwrap()
    }

    pub fn btrifact_out(
        &self, a_lu: &Tensor, pivots: &Tensor, pivot: bool
    ) -> (Tensor, Tensor) {
        self.f_btrifact_out(a_lu, pivots, pivot).unwrap()
    }

    pub fn btrifact_with_info(
        &self, pivot: bool
    ) -> (Tensor, Tensor, Tensor) {
        self.f_btrifact_with_info(pivot).unwrap()
    }

    pub fn btrifact_with_info_out(
        &self, a_lu: &Tensor, pivots: &Tensor, info: &Tensor, pivot: bool
    ) -> (Tensor, Tensor, Tensor) {
        self.f_btrifact_with_info_out(a_lu, pivots, info, pivot).unwrap()
    }

    pub fn btrisolve(
        &self, lu_data: &Tensor, lu_pivots: &Tensor
    ) -> Tensor {
        self.f_btrisolve(lu_data, lu_pivots).unwrap()
    }

    pub fn btrisolve_out(
        &self, result: &Tensor, lu_data: &Tensor, lu_pivots: &Tensor
    ) -> Tensor {
        self.f_btrisolve_out(result, lu_data, lu_pivots).unwrap()
    }

    pub fn cat<T: Borrow<Tensor>>(
        tensors: &[T], dim: i64
    ) -> Tensor {
        Tensor::f_cat(tensors, dim).unwrap()
    }

    pub fn cat_out<T: Borrow<Tensor>>(
        result: &Tensor, tensors: &[T], dim: i64
    ) -> Tensor {
        Tensor::f_cat_out(result, tensors, dim).unwrap()
    }

    pub fn cauchy_(
        &self, median: f64, sigma: f64
    ) -> Tensor {
        self.f_cauchy_(median, sigma).unwrap()
    }

    pub fn ceil(
        &self, 
    ) -> Tensor {
        self.f_ceil().unwrap()
    }

    pub fn ceil_(
        &self, 
    ) -> Tensor {
        self.f_ceil_().unwrap()
    }

    pub fn ceil_out(
        &self, result: &Tensor
    ) -> Tensor {
        self.f_ceil_out(result).unwrap()
    }

    pub fn celu(
        &self, 
    ) -> Tensor {
        self.f_celu().unwrap()
    }

    pub fn celu_(
        &self, 
    ) -> Tensor {
        self.f_celu_().unwrap()
    }

    pub fn chain_matmul<T: Borrow<Tensor>>(
        matrices: &[T]
    ) -> Tensor {
        Tensor::f_chain_matmul(matrices).unwrap()
    }

    pub fn cholesky(
        &self, upper: bool
    ) -> Tensor {
        self.f_cholesky(upper).unwrap()
    }

    pub fn cholesky_out(
        &self, result: &Tensor, upper: bool
    ) -> Tensor {
        self.f_cholesky_out(result, upper).unwrap()
    }

    pub fn chunk(
        &self, chunks: i64, dim: i64
    ) -> Vec<Tensor> {
        self.f_chunk(chunks, dim).unwrap()
    }

    pub fn clamp<S: Into<Scalar>>(
        &self, min: S, max: S
    ) -> Tensor {
        self.f_clamp(min, max).unwrap()
    }

    pub fn clamp_<S: Into<Scalar>>(
        &self, min: S, max: S
    ) -> Tensor {
        self.f_clamp_(min, max).unwrap()
    }

    pub fn clamp_max<S: Into<Scalar>>(
        &self, max: S
    ) -> Tensor {
        self.f_clamp_max(max).unwrap()
    }

    pub fn clamp_max_<S: Into<Scalar>>(
        &self, max: S
    ) -> Tensor {
        self.f_clamp_max_(max).unwrap()
    }

    pub fn clamp_max_out<S: Into<Scalar>>(
        &self, result: &Tensor, max: S
    ) -> Tensor {
        self.f_clamp_max_out(result, max).unwrap()
    }

    pub fn clamp_min<S: Into<Scalar>>(
        &self, min: S
    ) -> Tensor {
        self.f_clamp_min(min).unwrap()
    }

    pub fn clamp_min_<S: Into<Scalar>>(
        &self, min: S
    ) -> Tensor {
        self.f_clamp_min_(min).unwrap()
    }

    pub fn clamp_min_out<S: Into<Scalar>>(
        &self, result: &Tensor, min: S
    ) -> Tensor {
        self.f_clamp_min_out(result, min).unwrap()
    }

    pub fn clamp_out<S: Into<Scalar>>(
        &self, result: &Tensor, min: S, max: S
    ) -> Tensor {
        self.f_clamp_out(result, min, max).unwrap()
    }

    pub fn coalesce(
        &self, 
    ) -> Tensor {
        self.f_coalesce().unwrap()
    }

    pub fn constant_pad_nd(
        &self, pad: &[i64]
    ) -> Tensor {
        self.f_constant_pad_nd(pad).unwrap()
    }

    pub fn contiguous(
        &self, 
    ) -> Tensor {
        self.f_contiguous().unwrap()
    }

    pub fn conv1d(
        &self, weight: &Tensor, bias: &Tensor, stride: &[i64], padding: &[i64], dilation: &[i64], groups: i64
    ) -> Tensor {
        self.f_conv1d(weight, bias, stride, padding, dilation, groups).unwrap()
    }

    pub fn conv2d(
        &self, weight: &Tensor, bias: &Tensor, stride: &[i64], padding: &[i64], dilation: &[i64], groups: i64
    ) -> Tensor {
        self.f_conv2d(weight, bias, stride, padding, dilation, groups).unwrap()
    }

    pub fn conv3d(
        &self, weight: &Tensor, bias: &Tensor, stride: &[i64], padding: &[i64], dilation: &[i64], groups: i64
    ) -> Tensor {
        self.f_conv3d(weight, bias, stride, padding, dilation, groups).unwrap()
    }

    pub fn conv_tbc(
        &self, weight: &Tensor, bias: &Tensor, pad: i64
    ) -> Tensor {
        self.f_conv_tbc(weight, bias, pad).unwrap()
    }

    pub fn conv_tbc_backward(
        &self, input: &Tensor, weight: &Tensor, bias: &Tensor, pad: i64
    ) -> (Tensor, Tensor, Tensor) {
        self.f_conv_tbc_backward(input, weight, bias, pad).unwrap()
    }

    pub fn conv_transpose1d(
        &self, weight: &Tensor, bias: &Tensor, stride: &[i64], padding: &[i64], output_padding: &[i64], groups: i64, dilation: &[i64]
    ) -> Tensor {
        self.f_conv_transpose1d(weight, bias, stride, padding, output_padding, groups, dilation).unwrap()
    }

    pub fn conv_transpose2d(
        &self, weight: &Tensor, bias: &Tensor, stride: &[i64], padding: &[i64], output_padding: &[i64], groups: i64, dilation: &[i64]
    ) -> Tensor {
        self.f_conv_transpose2d(weight, bias, stride, padding, output_padding, groups, dilation).unwrap()
    }

    pub fn conv_transpose3d(
        &self, weight: &Tensor, bias: &Tensor, stride: &[i64], padding: &[i64], output_padding: &[i64], groups: i64, dilation: &[i64]
    ) -> Tensor {
        self.f_conv_transpose3d(weight, bias, stride, padding, output_padding, groups, dilation).unwrap()
    }

    pub fn convolution<T: Borrow<Tensor>>(
        &self, weight: &Tensor, bias: Option<T>, stride: &[i64], padding: &[i64], dilation: &[i64], transposed: bool, output_padding: &[i64], groups: i64
    ) -> Tensor {
        self.f_convolution(weight, bias, stride, padding, dilation, transposed, output_padding, groups).unwrap()
    }

    pub fn copy_sparse_to_sparse_(
        &self, src: &Tensor, non_blocking: bool
    ) -> Tensor {
        self.f_copy_sparse_to_sparse_(src, non_blocking).unwrap()
    }

    pub fn cos(
        &self, 
    ) -> Tensor {
        self.f_cos().unwrap()
    }

    pub fn cos_(
        &self, 
    ) -> Tensor {
        self.f_cos_().unwrap()
    }

    pub fn cos_out(
        &self, result: &Tensor
    ) -> Tensor {
        self.f_cos_out(result).unwrap()
    }

    pub fn cosh(
        &self, 
    ) -> Tensor {
        self.f_cosh().unwrap()
    }

    pub fn cosh_(
        &self, 
    ) -> Tensor {
        self.f_cosh_().unwrap()
    }

    pub fn cosh_out(
        &self, result: &Tensor
    ) -> Tensor {
        self.f_cosh_out(result).unwrap()
    }

    pub fn cosine_embedding_loss(
        input1: &Tensor, input2: &Tensor, target: &Tensor, margin: f64, reduction: i64
    ) -> Tensor {
        Tensor::f_cosine_embedding_loss(input1, input2, target, margin, reduction).unwrap()
    }

    pub fn cross(
        &self, other: &Tensor, dim: i64
    ) -> Tensor {
        self.f_cross(other, dim).unwrap()
    }

    pub fn cross_out(
        &self, result: &Tensor, other: &Tensor, dim: i64
    ) -> Tensor {
        self.f_cross_out(result, other, dim).unwrap()
    }

    pub fn ctc_loss(
        log_probs: &Tensor, targets: &Tensor, input_lengths: &[i64], target_lengths: &[i64], blank: i64, reduction: i64
    ) -> Tensor {
        Tensor::f_ctc_loss(log_probs, targets, input_lengths, target_lengths, blank, reduction).unwrap()
    }

    pub fn ctc_loss1(
        log_probs: &Tensor, targets: &Tensor, input_lengths: &Tensor, target_lengths: &Tensor, blank: i64, reduction: i64
    ) -> Tensor {
        Tensor::f_ctc_loss1(log_probs, targets, input_lengths, target_lengths, blank, reduction).unwrap()
    }

    pub fn cudnn_affine_grid_generator(
        theta: &Tensor, n: i64, c: i64, h: i64, w: i64
    ) -> Tensor {
        Tensor::f_cudnn_affine_grid_generator(theta, n, c, h, w).unwrap()
    }

    pub fn cudnn_affine_grid_generator_backward(
        grad: &Tensor, n: i64, c: i64, h: i64, w: i64
    ) -> Tensor {
        Tensor::f_cudnn_affine_grid_generator_backward(grad, n, c, h, w).unwrap()
    }

    pub fn cudnn_batch_norm<T: Borrow<Tensor>>(
        &self, weight: &Tensor, bias: Option<T>, running_mean: Option<T>, running_var: Option<T>, training: bool, exponential_average_factor: f64, epsilon: f64
    ) -> (Tensor, Tensor, Tensor) {
        self.f_cudnn_batch_norm(weight, bias, running_mean, running_var, training, exponential_average_factor, epsilon).unwrap()
    }

    pub fn cudnn_batch_norm_backward<T: Borrow<Tensor>>(
        &self, grad_output: &Tensor, weight: &Tensor, running_mean: Option<T>, running_var: Option<T>, save_mean: Option<T>, save_var: Option<T>, epsilon: f64
    ) -> (Tensor, Tensor, Tensor) {
        self.f_cudnn_batch_norm_backward(grad_output, weight, running_mean, running_var, save_mean, save_var, epsilon).unwrap()
    }

    pub fn cudnn_convolution<T: Borrow<Tensor>>(
        &self, weight: &Tensor, bias: Option<T>, padding: &[i64], stride: &[i64], dilation: &[i64], groups: i64, benchmark: bool, deterministic: bool
    ) -> Tensor {
        self.f_cudnn_convolution(weight, bias, padding, stride, dilation, groups, benchmark, deterministic).unwrap()
    }

    pub fn cudnn_convolution_backward_bias(
        grad_output: &Tensor
    ) -> Tensor {
        Tensor::f_cudnn_convolution_backward_bias(grad_output).unwrap()
    }

    pub fn cudnn_convolution_backward_input(
        self_size: &[i64], grad_output: &Tensor, weight: &Tensor, padding: &[i64], stride: &[i64], dilation: &[i64], groups: i64, benchmark: bool, deterministic: bool
    ) -> Tensor {
        Tensor::f_cudnn_convolution_backward_input(self_size, grad_output, weight, padding, stride, dilation, groups, benchmark, deterministic).unwrap()
    }

    pub fn cudnn_convolution_backward_weight(
        &self, weight_size: &[i64], grad_output: &Tensor, padding: &[i64], stride: &[i64], dilation: &[i64], groups: i64, benchmark: bool, deterministic: bool
    ) -> Tensor {
        self.f_cudnn_convolution_backward_weight(weight_size, grad_output, padding, stride, dilation, groups, benchmark, deterministic).unwrap()
    }

    pub fn cudnn_convolution_transpose<T: Borrow<Tensor>>(
        &self, weight: &Tensor, bias: Option<T>, padding: &[i64], output_padding: &[i64], stride: &[i64], dilation: &[i64], groups: i64, benchmark: bool, deterministic: bool
    ) -> Tensor {
        self.f_cudnn_convolution_transpose(weight, bias, padding, output_padding, stride, dilation, groups, benchmark, deterministic).unwrap()
    }

    pub fn cudnn_convolution_transpose_backward_bias(
        grad_output: &Tensor
    ) -> Tensor {
        Tensor::f_cudnn_convolution_transpose_backward_bias(grad_output).unwrap()
    }

    pub fn cudnn_convolution_transpose_backward_input(
        grad_output: &Tensor, weight: &Tensor, padding: &[i64], stride: &[i64], dilation: &[i64], groups: i64, benchmark: bool, deterministic: bool
    ) -> Tensor {
        Tensor::f_cudnn_convolution_transpose_backward_input(grad_output, weight, padding, stride, dilation, groups, benchmark, deterministic).unwrap()
    }

    pub fn cudnn_convolution_transpose_backward_weight(
        &self, weight_size: &[i64], grad_output: &Tensor, padding: &[i64], stride: &[i64], dilation: &[i64], groups: i64, benchmark: bool, deterministic: bool
    ) -> Tensor {
        self.f_cudnn_convolution_transpose_backward_weight(weight_size, grad_output, padding, stride, dilation, groups, benchmark, deterministic).unwrap()
    }

    pub fn cudnn_grid_sampler(
        &self, grid: &Tensor
    ) -> Tensor {
        self.f_cudnn_grid_sampler(grid).unwrap()
    }

    pub fn cudnn_grid_sampler_backward(
        &self, grid: &Tensor, grad_output: &Tensor
    ) -> (Tensor, Tensor) {
        self.f_cudnn_grid_sampler_backward(grid, grad_output).unwrap()
    }

    pub fn cumprod(
        &self, dim: i64
    ) -> Tensor {
        self.f_cumprod(dim).unwrap()
    }

    pub fn cumprod1(
        &self, dim: i64, dtype: Kind
    ) -> Tensor {
        self.f_cumprod1(dim, dtype).unwrap()
    }

    pub fn cumprod_out(
        &self, result: &Tensor, dim: i64
    ) -> Tensor {
        self.f_cumprod_out(result, dim).unwrap()
    }

    pub fn cumprod_out1(
        &self, result: &Tensor, dim: i64, dtype: Kind
    ) -> Tensor {
        self.f_cumprod_out1(result, dim, dtype).unwrap()
    }

    pub fn cumsum(
        &self, dim: i64
    ) -> Tensor {
        self.f_cumsum(dim).unwrap()
    }

    pub fn cumsum1(
        &self, dim: i64, dtype: Kind
    ) -> Tensor {
        self.f_cumsum1(dim, dtype).unwrap()
    }

    pub fn cumsum_out(
        &self, result: &Tensor, dim: i64
    ) -> Tensor {
        self.f_cumsum_out(result, dim).unwrap()
    }

    pub fn cumsum_out1(
        &self, result: &Tensor, dim: i64, dtype: Kind
    ) -> Tensor {
        self.f_cumsum_out1(result, dim, dtype).unwrap()
    }

    pub fn det(
        &self, 
    ) -> Tensor {
        self.f_det().unwrap()
    }

    pub fn detach(
        &self, 
    ) -> Tensor {
        self.f_detach().unwrap()
    }

    pub fn detach_(
        &self, 
    ) -> Tensor {
        self.f_detach_().unwrap()
    }

    pub fn diag(
        &self, diagonal: i64
    ) -> Tensor {
        self.f_diag(diagonal).unwrap()
    }

    pub fn diag_embed(
        &self, offset: i64, dim1: i64, dim2: i64
    ) -> Tensor {
        self.f_diag_embed(offset, dim1, dim2).unwrap()
    }

    pub fn diag_out(
        &self, result: &Tensor, diagonal: i64
    ) -> Tensor {
        self.f_diag_out(result, diagonal).unwrap()
    }

    pub fn diagflat(
        &self, offset: i64
    ) -> Tensor {
        self.f_diagflat(offset).unwrap()
    }

    pub fn diagonal(
        &self, offset: i64, dim1: i64, dim2: i64
    ) -> Tensor {
        self.f_diagonal(offset, dim1, dim2).unwrap()
    }

    pub fn digamma(
        &self, 
    ) -> Tensor {
        self.f_digamma().unwrap()
    }

    pub fn digamma_(
        &self, 
    ) -> Tensor {
        self.f_digamma_().unwrap()
    }

    pub fn digamma_out(
        &self, result: &Tensor
    ) -> Tensor {
        self.f_digamma_out(result).unwrap()
    }

    pub fn dist(
        &self, other: &Tensor
    ) -> Tensor {
        self.f_dist(other).unwrap()
    }

    pub fn g_div(
        &self, other: &Tensor
    ) -> Tensor {
        self.f_div(other).unwrap()
    }

    pub fn g_div1<S: Into<Scalar>>(
        &self, other: S
    ) -> Tensor {
        self.f_div1(other).unwrap()
    }

    pub fn g_div_(
        &self, other: &Tensor
    ) -> Tensor {
        self.f_div_(other).unwrap()
    }

    pub fn g_div_1<S: Into<Scalar>>(
        &self, other: S
    ) -> Tensor {
        self.f_div_1(other).unwrap()
    }

    pub fn div_out(
        &self, result: &Tensor, other: &Tensor
    ) -> Tensor {
        self.f_div_out(result, other).unwrap()
    }

    pub fn dot(
        &self, tensor: &Tensor
    ) -> Tensor {
        self.f_dot(tensor).unwrap()
    }

    pub fn dot_out(
        &self, result: &Tensor, tensor: &Tensor
    ) -> Tensor {
        self.f_dot_out(result, tensor).unwrap()
    }

    pub fn dropout(
        &self, p: f64, train: bool
    ) -> Tensor {
        self.f_dropout(p, train).unwrap()
    }

    pub fn dropout_(
        &self, p: f64, train: bool
    ) -> Tensor {
        self.f_dropout_(p, train).unwrap()
    }

    pub fn eig(
        &self, eigenvectors: bool
    ) -> (Tensor, Tensor) {
        self.f_eig(eigenvectors).unwrap()
    }

    pub fn eig_out(
        &self, e: &Tensor, v: &Tensor, eigenvectors: bool
    ) -> (Tensor, Tensor) {
        self.f_eig_out(e, v, eigenvectors).unwrap()
    }

    pub fn elu(
        &self, 
    ) -> Tensor {
        self.f_elu().unwrap()
    }

    pub fn elu_(
        &self, 
    ) -> Tensor {
        self.f_elu_().unwrap()
    }

    pub fn elu_out(
        &self, output: &Tensor
    ) -> Tensor {
        self.f_elu_out(output).unwrap()
    }

    pub fn embedding(
        weight: &Tensor, indices: &Tensor, padding_idx: i64, scale_grad_by_freq: bool, sparse: bool
    ) -> Tensor {
        Tensor::f_embedding(weight, indices, padding_idx, scale_grad_by_freq, sparse).unwrap()
    }

    pub fn embedding_backward(
        grad: &Tensor, indices: &Tensor, num_weights: i64, padding_idx: i64, scale_grad_by_freq: bool, sparse: bool
    ) -> Tensor {
        Tensor::f_embedding_backward(grad, indices, num_weights, padding_idx, scale_grad_by_freq, sparse).unwrap()
    }

    pub fn embedding_bag(
        weight: &Tensor, indices: &Tensor, offsets: &Tensor, scale_grad_by_freq: bool, mode: i64, sparse: bool
    ) -> (Tensor, Tensor, Tensor, Tensor) {
        Tensor::f_embedding_bag(weight, indices, offsets, scale_grad_by_freq, mode, sparse).unwrap()
    }

    pub fn embedding_dense_backward(
        grad: &Tensor, indices: &Tensor, num_weights: i64, padding_idx: i64, scale_grad_by_freq: bool
    ) -> Tensor {
        Tensor::f_embedding_dense_backward(grad, indices, num_weights, padding_idx, scale_grad_by_freq).unwrap()
    }

    pub fn embedding_renorm_(
        &self, indices: &Tensor, max_norm: f64, norm_type: f64
    ) -> Tensor {
        self.f_embedding_renorm_(indices, max_norm, norm_type).unwrap()
    }

    pub fn embedding_sparse_backward(
        grad: &Tensor, indices: &Tensor, num_weights: i64, padding_idx: i64, scale_grad_by_freq: bool
    ) -> Tensor {
        Tensor::f_embedding_sparse_backward(grad, indices, num_weights, padding_idx, scale_grad_by_freq).unwrap()
    }

    pub fn empty(
        size: &[i64], options: (Kind, Device)
    ) -> Tensor {
        Tensor::f_empty(size, options).unwrap()
    }

    pub fn empty_like(
        &self, 
    ) -> Tensor {
        self.f_empty_like().unwrap()
    }

    pub fn empty_like1(
        &self, options: (Kind, Device)
    ) -> Tensor {
        self.f_empty_like1(options).unwrap()
    }

    pub fn empty_out(
        result: &Tensor, size: &[i64]
    ) -> Tensor {
        Tensor::f_empty_out(result, size).unwrap()
    }

    pub fn empty_strided(
        size: &[i64], stride: &[i64], options: (Kind, Device)
    ) -> Tensor {
        Tensor::f_empty_strided(size, stride, options).unwrap()
    }

    pub fn eq<S: Into<Scalar>>(
        &self, other: S
    ) -> Tensor {
        self.f_eq(other).unwrap()
    }

    pub fn eq1(
        &self, other: &Tensor
    ) -> Tensor {
        self.f_eq1(other).unwrap()
    }

    pub fn eq_<S: Into<Scalar>>(
        &self, other: S
    ) -> Tensor {
        self.f_eq_(other).unwrap()
    }

    pub fn eq_1(
        &self, other: &Tensor
    ) -> Tensor {
        self.f_eq_1(other).unwrap()
    }

    pub fn eq_out<S: Into<Scalar>>(
        &self, result: &Tensor, other: S
    ) -> Tensor {
        self.f_eq_out(result, other).unwrap()
    }

    pub fn eq_out1(
        &self, result: &Tensor, other: &Tensor
    ) -> Tensor {
        self.f_eq_out1(result, other).unwrap()
    }

    pub fn erf(
        &self, 
    ) -> Tensor {
        self.f_erf().unwrap()
    }

    pub fn erf_(
        &self, 
    ) -> Tensor {
        self.f_erf_().unwrap()
    }

    pub fn erf_out(
        &self, result: &Tensor
    ) -> Tensor {
        self.f_erf_out(result).unwrap()
    }

    pub fn erfc(
        &self, 
    ) -> Tensor {
        self.f_erfc().unwrap()
    }

    pub fn erfc_(
        &self, 
    ) -> Tensor {
        self.f_erfc_().unwrap()
    }

    pub fn erfc_out(
        &self, result: &Tensor
    ) -> Tensor {
        self.f_erfc_out(result).unwrap()
    }

    pub fn erfinv(
        &self, 
    ) -> Tensor {
        self.f_erfinv().unwrap()
    }

    pub fn erfinv_(
        &self, 
    ) -> Tensor {
        self.f_erfinv_().unwrap()
    }

    pub fn erfinv_out(
        &self, result: &Tensor
    ) -> Tensor {
        self.f_erfinv_out(result).unwrap()
    }

    pub fn exp(
        &self, 
    ) -> Tensor {
        self.f_exp().unwrap()
    }

    pub fn exp_(
        &self, 
    ) -> Tensor {
        self.f_exp_().unwrap()
    }

    pub fn exp_out(
        &self, result: &Tensor
    ) -> Tensor {
        self.f_exp_out(result).unwrap()
    }

    pub fn expand(
        &self, size: &[i64], implicit: bool
    ) -> Tensor {
        self.f_expand(size, implicit).unwrap()
    }

    pub fn expand_as(
        &self, other: &Tensor
    ) -> Tensor {
        self.f_expand_as(other).unwrap()
    }

    pub fn expm1(
        &self, 
    ) -> Tensor {
        self.f_expm1().unwrap()
    }

    pub fn expm1_(
        &self, 
    ) -> Tensor {
        self.f_expm1_().unwrap()
    }

    pub fn expm1_out(
        &self, result: &Tensor
    ) -> Tensor {
        self.f_expm1_out(result).unwrap()
    }

    pub fn exponential_(
        &self, lambd: f64
    ) -> Tensor {
        self.f_exponential_(lambd).unwrap()
    }

    pub fn eye(
        n: i64, options: (Kind, Device)
    ) -> Tensor {
        Tensor::f_eye(n, options).unwrap()
    }

    pub fn eye1(
        n: i64, m: i64, options: (Kind, Device)
    ) -> Tensor {
        Tensor::f_eye1(n, m, options).unwrap()
    }

    pub fn eye_out(
        result: &Tensor, n: i64
    ) -> Tensor {
        Tensor::f_eye_out(result, n).unwrap()
    }

    pub fn eye_out1(
        result: &Tensor, n: i64, m: i64
    ) -> Tensor {
        Tensor::f_eye_out1(result, n, m).unwrap()
    }

    pub fn feature_alpha_dropout(
        &self, p: f64, train: bool
    ) -> Tensor {
        self.f_feature_alpha_dropout(p, train).unwrap()
    }

    pub fn feature_alpha_dropout_(
        &self, p: f64, train: bool
    ) -> Tensor {
        self.f_feature_alpha_dropout_(p, train).unwrap()
    }

    pub fn feature_dropout(
        &self, p: f64, train: bool
    ) -> Tensor {
        self.f_feature_dropout(p, train).unwrap()
    }

    pub fn feature_dropout_(
        &self, p: f64, train: bool
    ) -> Tensor {
        self.f_feature_dropout_(p, train).unwrap()
    }

    pub fn fft(
        &self, signal_ndim: i64, normalized: bool
    ) -> Tensor {
        self.f_fft(signal_ndim, normalized).unwrap()
    }

    pub fn fill_<S: Into<Scalar>>(
        &self, value: S
    ) -> Tensor {
        self.f_fill_(value).unwrap()
    }

    pub fn fill_1(
        &self, value: &Tensor
    ) -> Tensor {
        self.f_fill_1(value).unwrap()
    }

    pub fn flatten(
        &self, start_dim: i64, end_dim: i64
    ) -> Tensor {
        self.f_flatten(start_dim, end_dim).unwrap()
    }

    pub fn flip(
        &self, dims: &[i64]
    ) -> Tensor {
        self.f_flip(dims).unwrap()
    }

    pub fn floor(
        &self, 
    ) -> Tensor {
        self.f_floor().unwrap()
    }

    pub fn floor_(
        &self, 
    ) -> Tensor {
        self.f_floor_().unwrap()
    }

    pub fn floor_out(
        &self, result: &Tensor
    ) -> Tensor {
        self.f_floor_out(result).unwrap()
    }

    pub fn fmod<S: Into<Scalar>>(
        &self, other: S
    ) -> Tensor {
        self.f_fmod(other).unwrap()
    }

    pub fn fmod1(
        &self, other: &Tensor
    ) -> Tensor {
        self.f_fmod1(other).unwrap()
    }

    pub fn fmod_<S: Into<Scalar>>(
        &self, other: S
    ) -> Tensor {
        self.f_fmod_(other).unwrap()
    }

    pub fn fmod_1(
        &self, other: &Tensor
    ) -> Tensor {
        self.f_fmod_1(other).unwrap()
    }

    pub fn fmod_out<S: Into<Scalar>>(
        &self, result: &Tensor, other: S
    ) -> Tensor {
        self.f_fmod_out(result, other).unwrap()
    }

    pub fn fmod_out1(
        &self, result: &Tensor, other: &Tensor
    ) -> Tensor {
        self.f_fmod_out1(result, other).unwrap()
    }

    pub fn frac(
        &self, 
    ) -> Tensor {
        self.f_frac().unwrap()
    }

    pub fn frac_(
        &self, 
    ) -> Tensor {
        self.f_frac_().unwrap()
    }

    pub fn frac_out(
        &self, result: &Tensor
    ) -> Tensor {
        self.f_frac_out(result).unwrap()
    }

    pub fn fractional_max_pool2d(
        &self, kernel_size: &[i64], output_size: &[i64], random_samples: &Tensor
    ) -> (Tensor, Tensor) {
        self.f_fractional_max_pool2d(kernel_size, output_size, random_samples).unwrap()
    }

    pub fn fractional_max_pool2d_backward(
        &self, grad_output: &Tensor, kernel_size: &[i64], output_size: &[i64], indices: &Tensor
    ) -> Tensor {
        self.f_fractional_max_pool2d_backward(grad_output, kernel_size, output_size, indices).unwrap()
    }

    pub fn fractional_max_pool2d_backward_out(
        &self, grad_input: &Tensor, grad_output: &Tensor, kernel_size: &[i64], output_size: &[i64], indices: &Tensor
    ) -> Tensor {
        self.f_fractional_max_pool2d_backward_out(grad_input, grad_output, kernel_size, output_size, indices).unwrap()
    }

    pub fn fractional_max_pool2d_out(
        &self, output: &Tensor, indices: &Tensor, kernel_size: &[i64], output_size: &[i64], random_samples: &Tensor
    ) -> (Tensor, Tensor) {
        self.f_fractional_max_pool2d_out(output, indices, kernel_size, output_size, random_samples).unwrap()
    }

    pub fn frobenius_norm(
        &self, 
    ) -> Tensor {
        self.f_frobenius_norm().unwrap()
    }

    pub fn frobenius_norm1(
        &self, dim: &[i64], keepdim: bool
    ) -> Tensor {
        self.f_frobenius_norm1(dim, keepdim).unwrap()
    }

    pub fn frobenius_norm_out(
        &self, result: &Tensor, dim: &[i64], keepdim: bool
    ) -> Tensor {
        self.f_frobenius_norm_out(result, dim, keepdim).unwrap()
    }

    pub fn full<S: Into<Scalar>>(
        size: &[i64], fill_value: S, options: (Kind, Device)
    ) -> Tensor {
        Tensor::f_full(size, fill_value, options).unwrap()
    }

    pub fn full_like<S: Into<Scalar>>(
        &self, fill_value: S
    ) -> Tensor {
        self.f_full_like(fill_value).unwrap()
    }

    pub fn full_like1<S: Into<Scalar>>(
        &self, fill_value: S, options: (Kind, Device)
    ) -> Tensor {
        self.f_full_like1(fill_value, options).unwrap()
    }

    pub fn full_out<S: Into<Scalar>>(
        result: &Tensor, size: &[i64], fill_value: S
    ) -> Tensor {
        Tensor::f_full_out(result, size, fill_value).unwrap()
    }

    pub fn gather(
        &self, dim: i64, index: &Tensor
    ) -> Tensor {
        self.f_gather(dim, index).unwrap()
    }

    pub fn gather_out(
        &self, result: &Tensor, dim: i64, index: &Tensor
    ) -> Tensor {
        self.f_gather_out(result, dim, index).unwrap()
    }

    pub fn ge<S: Into<Scalar>>(
        &self, other: S
    ) -> Tensor {
        self.f_ge(other).unwrap()
    }

    pub fn ge1(
        &self, other: &Tensor
    ) -> Tensor {
        self.f_ge1(other).unwrap()
    }

    pub fn ge_<S: Into<Scalar>>(
        &self, other: S
    ) -> Tensor {
        self.f_ge_(other).unwrap()
    }

    pub fn ge_1(
        &self, other: &Tensor
    ) -> Tensor {
        self.f_ge_1(other).unwrap()
    }

    pub fn ge_out<S: Into<Scalar>>(
        &self, result: &Tensor, other: S
    ) -> Tensor {
        self.f_ge_out(result, other).unwrap()
    }

    pub fn ge_out1(
        &self, result: &Tensor, other: &Tensor
    ) -> Tensor {
        self.f_ge_out1(result, other).unwrap()
    }

    pub fn gels(
        &self, a: &Tensor
    ) -> (Tensor, Tensor) {
        self.f_gels(a).unwrap()
    }

    pub fn gels_out(
        &self, x: &Tensor, qr: &Tensor, a: &Tensor
    ) -> (Tensor, Tensor) {
        self.f_gels_out(x, qr, a).unwrap()
    }

    pub fn geometric_(
        &self, p: f64
    ) -> Tensor {
        self.f_geometric_(p).unwrap()
    }

    pub fn geqrf(
        &self, 
    ) -> (Tensor, Tensor) {
        self.f_geqrf().unwrap()
    }

    pub fn geqrf_out(
        &self, result0: &Tensor, result1: &Tensor
    ) -> (Tensor, Tensor) {
        self.f_geqrf_out(result0, result1).unwrap()
    }

    pub fn ger(
        &self, vec2: &Tensor
    ) -> Tensor {
        self.f_ger(vec2).unwrap()
    }

    pub fn ger_out(
        &self, result: &Tensor, vec2: &Tensor
    ) -> Tensor {
        self.f_ger_out(result, vec2).unwrap()
    }

    pub fn gesv(
        &self, a: &Tensor
    ) -> (Tensor, Tensor) {
        self.f_gesv(a).unwrap()
    }

    pub fn gesv_out(
        &self, solution: &Tensor, lu: &Tensor, a: &Tensor
    ) -> (Tensor, Tensor) {
        self.f_gesv_out(solution, lu, a).unwrap()
    }

    pub fn glu(
        &self, dim: i64
    ) -> Tensor {
        self.f_glu(dim).unwrap()
    }

    pub fn glu_backward(
        &self, grad_output: &Tensor, dim: i64
    ) -> Tensor {
        self.f_glu_backward(grad_output, dim).unwrap()
    }

    pub fn glu_backward_out(
        &self, grad_input: &Tensor, grad_output: &Tensor, dim: i64
    ) -> Tensor {
        self.f_glu_backward_out(grad_input, grad_output, dim).unwrap()
    }

    pub fn glu_out(
        &self, output: &Tensor, dim: i64
    ) -> Tensor {
        self.f_glu_out(output, dim).unwrap()
    }

    pub fn grad(
        &self, 
    ) -> Tensor {
        self.f_grad().unwrap()
    }

    pub fn grid_sampler(
        &self, grid: &Tensor, interpolation_mode: i64, padding_mode: i64
    ) -> Tensor {
        self.f_grid_sampler(grid, interpolation_mode, padding_mode).unwrap()
    }

    pub fn grid_sampler_2d(
        &self, grid: &Tensor, interpolation_mode: i64, padding_mode: i64
    ) -> Tensor {
        self.f_grid_sampler_2d(grid, interpolation_mode, padding_mode).unwrap()
    }

    pub fn grid_sampler_2d_backward(
        &self, grad_output: &Tensor, grid: &Tensor, interpolation_mode: i64, padding_mode: i64
    ) -> (Tensor, Tensor) {
        self.f_grid_sampler_2d_backward(grad_output, grid, interpolation_mode, padding_mode).unwrap()
    }

    pub fn grid_sampler_3d(
        &self, grid: &Tensor, interpolation_mode: i64, padding_mode: i64
    ) -> Tensor {
        self.f_grid_sampler_3d(grid, interpolation_mode, padding_mode).unwrap()
    }

    pub fn grid_sampler_3d_backward(
        &self, grad_output: &Tensor, grid: &Tensor, interpolation_mode: i64, padding_mode: i64
    ) -> (Tensor, Tensor) {
        self.f_grid_sampler_3d_backward(grad_output, grid, interpolation_mode, padding_mode).unwrap()
    }

    pub fn group_norm<T: Borrow<Tensor>>(
        &self, num_groups: i64, weight: Option<T>, bias: Option<T>, eps: f64, cudnn_enabled: bool
    ) -> Tensor {
        self.f_group_norm(num_groups, weight, bias, eps, cudnn_enabled).unwrap()
    }

    pub fn gru<T: Borrow<Tensor>>(
        &self, hx: &Tensor, params: &[T], has_biases: bool, num_layers: i64, dropout: f64, train: bool, bidirectional: bool, batch_first: bool
    ) -> (Tensor, Tensor) {
        self.f_gru(hx, params, has_biases, num_layers, dropout, train, bidirectional, batch_first).unwrap()
    }

    pub fn gru1<T: Borrow<Tensor>>(
        data: &Tensor, batch_sizes: &Tensor, hx: &Tensor, params: &[T], has_biases: bool, num_layers: i64, dropout: f64, train: bool, bidirectional: bool
    ) -> (Tensor, Tensor) {
        Tensor::f_gru1(data, batch_sizes, hx, params, has_biases, num_layers, dropout, train, bidirectional).unwrap()
    }

    pub fn gru_cell<T: Borrow<Tensor>>(
        &self, hx: &Tensor, w_ih: &Tensor, w_hh: &Tensor, b_ih: Option<T>, b_hh: Option<T>
    ) -> Tensor {
        self.f_gru_cell(hx, w_ih, w_hh, b_ih, b_hh).unwrap()
    }

    pub fn gt<S: Into<Scalar>>(
        &self, other: S
    ) -> Tensor {
        self.f_gt(other).unwrap()
    }

    pub fn gt1(
        &self, other: &Tensor
    ) -> Tensor {
        self.f_gt1(other).unwrap()
    }

    pub fn gt_<S: Into<Scalar>>(
        &self, other: S
    ) -> Tensor {
        self.f_gt_(other).unwrap()
    }

    pub fn gt_1(
        &self, other: &Tensor
    ) -> Tensor {
        self.f_gt_1(other).unwrap()
    }

    pub fn gt_out<S: Into<Scalar>>(
        &self, result: &Tensor, other: S
    ) -> Tensor {
        self.f_gt_out(result, other).unwrap()
    }

    pub fn gt_out1(
        &self, result: &Tensor, other: &Tensor
    ) -> Tensor {
        self.f_gt_out1(result, other).unwrap()
    }

    pub fn hamming_window(
        window_length: i64, options: (Kind, Device)
    ) -> Tensor {
        Tensor::f_hamming_window(window_length, options).unwrap()
    }

    pub fn hamming_window1(
        window_length: i64, periodic: bool, options: (Kind, Device)
    ) -> Tensor {
        Tensor::f_hamming_window1(window_length, periodic, options).unwrap()
    }

    pub fn hamming_window2(
        window_length: i64, periodic: bool, alpha: f64, options: (Kind, Device)
    ) -> Tensor {
        Tensor::f_hamming_window2(window_length, periodic, alpha, options).unwrap()
    }

    pub fn hamming_window3(
        window_length: i64, periodic: bool, alpha: f64, beta: f64, options: (Kind, Device)
    ) -> Tensor {
        Tensor::f_hamming_window3(window_length, periodic, alpha, beta, options).unwrap()
    }

    pub fn hann_window(
        window_length: i64, options: (Kind, Device)
    ) -> Tensor {
        Tensor::f_hann_window(window_length, options).unwrap()
    }

    pub fn hann_window1(
        window_length: i64, periodic: bool, options: (Kind, Device)
    ) -> Tensor {
        Tensor::f_hann_window1(window_length, periodic, options).unwrap()
    }

    pub fn hardshrink(
        &self, 
    ) -> Tensor {
        self.f_hardshrink().unwrap()
    }

    pub fn hardshrink_backward<S: Into<Scalar>>(
        &self, grad_out: &Tensor, lambd: S
    ) -> Tensor {
        self.f_hardshrink_backward(grad_out, lambd).unwrap()
    }

    pub fn hardtanh(
        &self, 
    ) -> Tensor {
        self.f_hardtanh().unwrap()
    }

    pub fn hardtanh_(
        &self, 
    ) -> Tensor {
        self.f_hardtanh_().unwrap()
    }

    pub fn hardtanh_out(
        &self, output: &Tensor
    ) -> Tensor {
        self.f_hardtanh_out(output).unwrap()
    }

    pub fn hinge_embedding_loss(
        &self, target: &Tensor, margin: f64, reduction: i64
    ) -> Tensor {
        self.f_hinge_embedding_loss(target, margin, reduction).unwrap()
    }

    pub fn histc(
        &self, bins: i64
    ) -> Tensor {
        self.f_histc(bins).unwrap()
    }

    pub fn histc_out(
        &self, result: &Tensor, bins: i64
    ) -> Tensor {
        self.f_histc_out(result, bins).unwrap()
    }

    pub fn hspmm(
        mat1: &Tensor, mat2: &Tensor
    ) -> Tensor {
        Tensor::f_hspmm(mat1, mat2).unwrap()
    }

    pub fn hspmm_out(
        result: &Tensor, mat1: &Tensor, mat2: &Tensor
    ) -> Tensor {
        Tensor::f_hspmm_out(result, mat1, mat2).unwrap()
    }

    pub fn ifft(
        &self, signal_ndim: i64, normalized: bool
    ) -> Tensor {
        self.f_ifft(signal_ndim, normalized).unwrap()
    }

    pub fn index<T: Borrow<Tensor>>(
        &self, indices: &[T]
    ) -> Tensor {
        self.f_index(indices).unwrap()
    }

    pub fn index_add_(
        &self, dim: i64, index: &Tensor, source: &Tensor
    ) -> Tensor {
        self.f_index_add_(dim, index, source).unwrap()
    }

    pub fn index_copy_(
        &self, dim: i64, index: &Tensor, source: &Tensor
    ) -> Tensor {
        self.f_index_copy_(dim, index, source).unwrap()
    }

    pub fn index_fill_<S: Into<Scalar>>(
        &self, dim: i64, index: &Tensor, value: S
    ) -> Tensor {
        self.f_index_fill_(dim, index, value).unwrap()
    }

    pub fn index_fill_1(
        &self, dim: i64, index: &Tensor, value: &Tensor
    ) -> Tensor {
        self.f_index_fill_1(dim, index, value).unwrap()
    }

    pub fn index_put<T: Borrow<Tensor>>(
        &self, indices: &[T], values: &Tensor
    ) -> Tensor {
        self.f_index_put(indices, values).unwrap()
    }

    pub fn index_put_<T: Borrow<Tensor>>(
        &self, indices: &[T], values: &Tensor
    ) -> Tensor {
        self.f_index_put_(indices, values).unwrap()
    }

    pub fn index_select(
        &self, dim: i64, index: &Tensor
    ) -> Tensor {
        self.f_index_select(dim, index).unwrap()
    }

    pub fn index_select_out(
        &self, result: &Tensor, dim: i64, index: &Tensor
    ) -> Tensor {
        self.f_index_select_out(result, dim, index).unwrap()
    }

    pub fn indices(
        &self, 
    ) -> Tensor {
        self.f_indices().unwrap()
    }

    pub fn instance_norm<T: Borrow<Tensor>>(
        &self, weight: Option<T>, bias: Option<T>, running_mean: Option<T>, running_var: Option<T>, use_input_stats: bool, momentum: f64, eps: f64, cudnn_enabled: bool
    ) -> Tensor {
        self.f_instance_norm(weight, bias, running_mean, running_var, use_input_stats, momentum, eps, cudnn_enabled).unwrap()
    }

    pub fn inverse(
        &self, 
    ) -> Tensor {
        self.f_inverse().unwrap()
    }

    pub fn inverse_out(
        &self, result: &Tensor
    ) -> Tensor {
        self.f_inverse_out(result).unwrap()
    }

    pub fn irfft(
        &self, signal_ndim: i64, normalized: bool, onesided: bool, signal_sizes: &[i64]
    ) -> Tensor {
        self.f_irfft(signal_ndim, normalized, onesided, signal_sizes).unwrap()
    }

    pub fn isclose(
        &self, other: &Tensor, rtol: f64, atol: f64, equal_nan: bool
    ) -> Tensor {
        self.f_isclose(other, rtol, atol, equal_nan).unwrap()
    }

    pub fn kl_div(
        &self, target: &Tensor, reduction: i64
    ) -> Tensor {
        self.f_kl_div(target, reduction).unwrap()
    }

    pub fn kl_div_backward(
        &self, grad_output: &Tensor, target: &Tensor, reduction: i64
    ) -> Tensor {
        self.f_kl_div_backward(grad_output, target, reduction).unwrap()
    }

    pub fn kthvalue(
        &self, k: i64, dim: i64, keepdim: bool
    ) -> (Tensor, Tensor) {
        self.f_kthvalue(k, dim, keepdim).unwrap()
    }

    pub fn kthvalue_out(
        &self, values: &Tensor, indices: &Tensor, k: i64, dim: i64, keepdim: bool
    ) -> (Tensor, Tensor) {
        self.f_kthvalue_out(values, indices, k, dim, keepdim).unwrap()
    }

    pub fn l1_loss(
        &self, target: &Tensor, reduction: i64
    ) -> Tensor {
        self.f_l1_loss(target, reduction).unwrap()
    }

    pub fn l1_loss_backward(
        &self, grad_output: &Tensor, target: &Tensor, reduction: i64
    ) -> Tensor {
        self.f_l1_loss_backward(grad_output, target, reduction).unwrap()
    }

    pub fn l1_loss_backward_out(
        &self, grad_input: &Tensor, grad_output: &Tensor, target: &Tensor, reduction: i64
    ) -> Tensor {
        self.f_l1_loss_backward_out(grad_input, grad_output, target, reduction).unwrap()
    }

    pub fn l1_loss_out(
        &self, output: &Tensor, target: &Tensor, reduction: i64
    ) -> Tensor {
        self.f_l1_loss_out(output, target, reduction).unwrap()
    }

    pub fn layer_norm<T: Borrow<Tensor>>(
        &self, normalized_shape: &[i64], weight: Option<T>, bias: Option<T>, eps: f64, cudnn_enable: bool
    ) -> Tensor {
        self.f_layer_norm(normalized_shape, weight, bias, eps, cudnn_enable).unwrap()
    }

    pub fn le<S: Into<Scalar>>(
        &self, other: S
    ) -> Tensor {
        self.f_le(other).unwrap()
    }

    pub fn le1(
        &self, other: &Tensor
    ) -> Tensor {
        self.f_le1(other).unwrap()
    }

    pub fn le_<S: Into<Scalar>>(
        &self, other: S
    ) -> Tensor {
        self.f_le_(other).unwrap()
    }

    pub fn le_1(
        &self, other: &Tensor
    ) -> Tensor {
        self.f_le_1(other).unwrap()
    }

    pub fn le_out<S: Into<Scalar>>(
        &self, result: &Tensor, other: S
    ) -> Tensor {
        self.f_le_out(result, other).unwrap()
    }

    pub fn le_out1(
        &self, result: &Tensor, other: &Tensor
    ) -> Tensor {
        self.f_le_out1(result, other).unwrap()
    }

    pub fn leaky_relu(
        &self, 
    ) -> Tensor {
        self.f_leaky_relu().unwrap()
    }

    pub fn leaky_relu_(
        &self, 
    ) -> Tensor {
        self.f_leaky_relu_().unwrap()
    }

    pub fn leaky_relu_out(
        &self, output: &Tensor
    ) -> Tensor {
        self.f_leaky_relu_out(output).unwrap()
    }

    pub fn lerp<S: Into<Scalar>>(
        &self, end: &Tensor, weight: S
    ) -> Tensor {
        self.f_lerp(end, weight).unwrap()
    }

    pub fn lerp_<S: Into<Scalar>>(
        &self, end: &Tensor, weight: S
    ) -> Tensor {
        self.f_lerp_(end, weight).unwrap()
    }

    pub fn lerp_out<S: Into<Scalar>>(
        &self, result: &Tensor, end: &Tensor, weight: S
    ) -> Tensor {
        self.f_lerp_out(result, end, weight).unwrap()
    }

    pub fn lgamma(
        &self, 
    ) -> Tensor {
        self.f_lgamma().unwrap()
    }

    pub fn lgamma_(
        &self, 
    ) -> Tensor {
        self.f_lgamma_().unwrap()
    }

    pub fn lgamma_out(
        &self, result: &Tensor
    ) -> Tensor {
        self.f_lgamma_out(result).unwrap()
    }

    pub fn linear(
        &self, weight: &Tensor, bias: &Tensor
    ) -> Tensor {
        self.f_linear(weight, bias).unwrap()
    }

    pub fn linspace<S: Into<Scalar>>(
        start: S, end: S, options: (Kind, Device)
    ) -> Tensor {
        Tensor::f_linspace(start, end, options).unwrap()
    }

    pub fn linspace1<S: Into<Scalar>>(
        start: S, end: S, steps: i64, options: (Kind, Device)
    ) -> Tensor {
        Tensor::f_linspace1(start, end, steps, options).unwrap()
    }

    pub fn linspace_out<S: Into<Scalar>>(
        result: &Tensor, start: S, end: S
    ) -> Tensor {
        Tensor::f_linspace_out(result, start, end).unwrap()
    }

    pub fn linspace_out1<S: Into<Scalar>>(
        result: &Tensor, start: S, end: S, steps: i64
    ) -> Tensor {
        Tensor::f_linspace_out1(result, start, end, steps).unwrap()
    }

    pub fn log(
        &self, 
    ) -> Tensor {
        self.f_log().unwrap()
    }

    pub fn log10(
        &self, 
    ) -> Tensor {
        self.f_log10().unwrap()
    }

    pub fn log10_(
        &self, 
    ) -> Tensor {
        self.f_log10_().unwrap()
    }

    pub fn log10_out(
        &self, result: &Tensor
    ) -> Tensor {
        self.f_log10_out(result).unwrap()
    }

    pub fn log1p(
        &self, 
    ) -> Tensor {
        self.f_log1p().unwrap()
    }

    pub fn log1p_(
        &self, 
    ) -> Tensor {
        self.f_log1p_().unwrap()
    }

    pub fn log1p_out(
        &self, result: &Tensor
    ) -> Tensor {
        self.f_log1p_out(result).unwrap()
    }

    pub fn log2(
        &self, 
    ) -> Tensor {
        self.f_log2().unwrap()
    }

    pub fn log2_(
        &self, 
    ) -> Tensor {
        self.f_log2_().unwrap()
    }

    pub fn log2_out(
        &self, result: &Tensor
    ) -> Tensor {
        self.f_log2_out(result).unwrap()
    }

    pub fn log_(
        &self, 
    ) -> Tensor {
        self.f_log_().unwrap()
    }

    pub fn log_normal_(
        &self, mean: f64, std: f64
    ) -> Tensor {
        self.f_log_normal_(mean, std).unwrap()
    }

    pub fn log_out(
        &self, result: &Tensor
    ) -> Tensor {
        self.f_log_out(result).unwrap()
    }

    pub fn log_sigmoid(
        &self, 
    ) -> Tensor {
        self.f_log_sigmoid().unwrap()
    }

    pub fn log_sigmoid_backward(
        &self, grad_output: &Tensor, buffer: &Tensor
    ) -> Tensor {
        self.f_log_sigmoid_backward(grad_output, buffer).unwrap()
    }

    pub fn log_sigmoid_backward_out(
        &self, grad_input: &Tensor, grad_output: &Tensor, buffer: &Tensor
    ) -> Tensor {
        self.f_log_sigmoid_backward_out(grad_input, grad_output, buffer).unwrap()
    }

    pub fn log_sigmoid_out(
        &self, output: &Tensor
    ) -> Tensor {
        self.f_log_sigmoid_out(output).unwrap()
    }

    pub fn log_softmax(
        &self, dim: i64
    ) -> Tensor {
        self.f_log_softmax(dim).unwrap()
    }

    pub fn log_softmax1(
        &self, dim: i64, dtype: Kind
    ) -> Tensor {
        self.f_log_softmax1(dim, dtype).unwrap()
    }

    pub fn logdet(
        &self, 
    ) -> Tensor {
        self.f_logdet().unwrap()
    }

    pub fn logspace<S: Into<Scalar>>(
        start: S, end: S, options: (Kind, Device)
    ) -> Tensor {
        Tensor::f_logspace(start, end, options).unwrap()
    }

    pub fn logspace1<S: Into<Scalar>>(
        start: S, end: S, steps: i64, options: (Kind, Device)
    ) -> Tensor {
        Tensor::f_logspace1(start, end, steps, options).unwrap()
    }

    pub fn logspace_out<S: Into<Scalar>>(
        result: &Tensor, start: S, end: S
    ) -> Tensor {
        Tensor::f_logspace_out(result, start, end).unwrap()
    }

    pub fn logspace_out1<S: Into<Scalar>>(
        result: &Tensor, start: S, end: S, steps: i64
    ) -> Tensor {
        Tensor::f_logspace_out1(result, start, end, steps).unwrap()
    }

    pub fn logsumexp(
        &self, dim: i64, keepdim: bool
    ) -> Tensor {
        self.f_logsumexp(dim, keepdim).unwrap()
    }

    pub fn logsumexp_out(
        &self, result: &Tensor, dim: i64, keepdim: bool
    ) -> Tensor {
        self.f_logsumexp_out(result, dim, keepdim).unwrap()
    }

    pub fn lstm<T: Borrow<Tensor>>(
        &self, hx: &[T], params: &[T], has_biases: bool, num_layers: i64, dropout: f64, train: bool, bidirectional: bool, batch_first: bool
    ) -> (Tensor, Tensor, Tensor) {
        self.f_lstm(hx, params, has_biases, num_layers, dropout, train, bidirectional, batch_first).unwrap()
    }

    pub fn lstm1<T: Borrow<Tensor>>(
        data: &Tensor, batch_sizes: &Tensor, hx: &[T], params: &[T], has_biases: bool, num_layers: i64, dropout: f64, train: bool, bidirectional: bool
    ) -> (Tensor, Tensor, Tensor) {
        Tensor::f_lstm1(data, batch_sizes, hx, params, has_biases, num_layers, dropout, train, bidirectional).unwrap()
    }

    pub fn lstm_cell<T: Borrow<Tensor>>(
        &self, hx: &[T], w_ih: &Tensor, w_hh: &Tensor, b_ih: Option<T>, b_hh: Option<T>
    ) -> (Tensor, Tensor) {
        self.f_lstm_cell(hx, w_ih, w_hh, b_ih, b_hh).unwrap()
    }

    pub fn lt<S: Into<Scalar>>(
        &self, other: S
    ) -> Tensor {
        self.f_lt(other).unwrap()
    }

    pub fn lt1(
        &self, other: &Tensor
    ) -> Tensor {
        self.f_lt1(other).unwrap()
    }

    pub fn lt_<S: Into<Scalar>>(
        &self, other: S
    ) -> Tensor {
        self.f_lt_(other).unwrap()
    }

    pub fn lt_1(
        &self, other: &Tensor
    ) -> Tensor {
        self.f_lt_1(other).unwrap()
    }

    pub fn lt_out<S: Into<Scalar>>(
        &self, result: &Tensor, other: S
    ) -> Tensor {
        self.f_lt_out(result, other).unwrap()
    }

    pub fn lt_out1(
        &self, result: &Tensor, other: &Tensor
    ) -> Tensor {
        self.f_lt_out1(result, other).unwrap()
    }

    pub fn margin_ranking_loss(
        input1: &Tensor, input2: &Tensor, target: &Tensor, margin: f64, reduction: i64
    ) -> Tensor {
        Tensor::f_margin_ranking_loss(input1, input2, target, margin, reduction).unwrap()
    }

    pub fn masked_fill_<S: Into<Scalar>>(
        &self, mask: &Tensor, value: S
    ) -> Tensor {
        self.f_masked_fill_(mask, value).unwrap()
    }

    pub fn masked_fill_1(
        &self, mask: &Tensor, value: &Tensor
    ) -> Tensor {
        self.f_masked_fill_1(mask, value).unwrap()
    }

    pub fn masked_scatter_(
        &self, mask: &Tensor, source: &Tensor
    ) -> Tensor {
        self.f_masked_scatter_(mask, source).unwrap()
    }

    pub fn masked_select(
        &self, mask: &Tensor
    ) -> Tensor {
        self.f_masked_select(mask).unwrap()
    }

    pub fn masked_select_out(
        &self, result: &Tensor, mask: &Tensor
    ) -> Tensor {
        self.f_masked_select_out(result, mask).unwrap()
    }

    pub fn matmul(
        &self, other: &Tensor
    ) -> Tensor {
        self.f_matmul(other).unwrap()
    }

    pub fn matmul_out(
        &self, result: &Tensor, other: &Tensor
    ) -> Tensor {
        self.f_matmul_out(result, other).unwrap()
    }

    pub fn matrix_power(
        &self, n: i64
    ) -> Tensor {
        self.f_matrix_power(n).unwrap()
    }

    pub fn matrix_rank(
        &self, symmetric: bool
    ) -> Tensor {
        self.f_matrix_rank(symmetric).unwrap()
    }

    pub fn matrix_rank1(
        &self, tol: f64, symmetric: bool
    ) -> Tensor {
        self.f_matrix_rank1(tol, symmetric).unwrap()
    }

    pub fn max(
        &self, 
    ) -> Tensor {
        self.f_max().unwrap()
    }

    pub fn max1(
        &self, other: &Tensor
    ) -> Tensor {
        self.f_max1(other).unwrap()
    }

    pub fn max2(
        &self, dim: i64, keepdim: bool
    ) -> (Tensor, Tensor) {
        self.f_max2(dim, keepdim).unwrap()
    }

    pub fn max_out(
        &self, result: &Tensor, other: &Tensor
    ) -> Tensor {
        self.f_max_out(result, other).unwrap()
    }

    pub fn max_out1(
        &self, max: &Tensor, max_values: &Tensor, dim: i64, keepdim: bool
    ) -> (Tensor, Tensor) {
        self.f_max_out1(max, max_values, dim, keepdim).unwrap()
    }

    pub fn max_pool1d(
        &self, kernel_size: &[i64], stride: &[i64], padding: &[i64], dilation: &[i64], ceil_mode: bool
    ) -> Tensor {
        self.f_max_pool1d(kernel_size, stride, padding, dilation, ceil_mode).unwrap()
    }

    pub fn max_pool1d_with_indices(
        &self, kernel_size: &[i64], stride: &[i64], padding: &[i64], dilation: &[i64], ceil_mode: bool
    ) -> (Tensor, Tensor) {
        self.f_max_pool1d_with_indices(kernel_size, stride, padding, dilation, ceil_mode).unwrap()
    }

    pub fn max_pool2d(
        &self, kernel_size: &[i64], stride: &[i64], padding: &[i64], dilation: &[i64], ceil_mode: bool
    ) -> Tensor {
        self.f_max_pool2d(kernel_size, stride, padding, dilation, ceil_mode).unwrap()
    }

    pub fn max_pool2d_with_indices(
        &self, kernel_size: &[i64], stride: &[i64], padding: &[i64], dilation: &[i64], ceil_mode: bool
    ) -> (Tensor, Tensor) {
        self.f_max_pool2d_with_indices(kernel_size, stride, padding, dilation, ceil_mode).unwrap()
    }

    pub fn max_pool2d_with_indices_backward(
        &self, grad_output: &Tensor, kernel_size: &[i64], stride: &[i64], padding: &[i64], dilation: &[i64], ceil_mode: bool, indices: &Tensor
    ) -> Tensor {
        self.f_max_pool2d_with_indices_backward(grad_output, kernel_size, stride, padding, dilation, ceil_mode, indices).unwrap()
    }

    pub fn max_pool2d_with_indices_backward_out(
        &self, grad_input: &Tensor, grad_output: &Tensor, kernel_size: &[i64], stride: &[i64], padding: &[i64], dilation: &[i64], ceil_mode: bool, indices: &Tensor
    ) -> Tensor {
        self.f_max_pool2d_with_indices_backward_out(grad_input, grad_output, kernel_size, stride, padding, dilation, ceil_mode, indices).unwrap()
    }

    pub fn max_pool2d_with_indices_out(
        &self, output: &Tensor, indices: &Tensor, kernel_size: &[i64], stride: &[i64], padding: &[i64], dilation: &[i64], ceil_mode: bool
    ) -> (Tensor, Tensor) {
        self.f_max_pool2d_with_indices_out(output, indices, kernel_size, stride, padding, dilation, ceil_mode).unwrap()
    }

    pub fn max_pool3d(
        &self, kernel_size: &[i64], stride: &[i64], padding: &[i64], dilation: &[i64], ceil_mode: bool
    ) -> Tensor {
        self.f_max_pool3d(kernel_size, stride, padding, dilation, ceil_mode).unwrap()
    }

    pub fn max_pool3d_with_indices(
        &self, kernel_size: &[i64], stride: &[i64], padding: &[i64], dilation: &[i64], ceil_mode: bool
    ) -> (Tensor, Tensor) {
        self.f_max_pool3d_with_indices(kernel_size, stride, padding, dilation, ceil_mode).unwrap()
    }

    pub fn max_pool3d_with_indices_backward(
        &self, grad_output: &Tensor, kernel_size: &[i64], stride: &[i64], padding: &[i64], dilation: &[i64], ceil_mode: bool, indices: &Tensor
    ) -> Tensor {
        self.f_max_pool3d_with_indices_backward(grad_output, kernel_size, stride, padding, dilation, ceil_mode, indices).unwrap()
    }

    pub fn max_pool3d_with_indices_backward_out(
        &self, grad_input: &Tensor, grad_output: &Tensor, kernel_size: &[i64], stride: &[i64], padding: &[i64], dilation: &[i64], ceil_mode: bool, indices: &Tensor
    ) -> Tensor {
        self.f_max_pool3d_with_indices_backward_out(grad_input, grad_output, kernel_size, stride, padding, dilation, ceil_mode, indices).unwrap()
    }

    pub fn max_pool3d_with_indices_out(
        &self, output: &Tensor, indices: &Tensor, kernel_size: &[i64], stride: &[i64], padding: &[i64], dilation: &[i64], ceil_mode: bool
    ) -> (Tensor, Tensor) {
        self.f_max_pool3d_with_indices_out(output, indices, kernel_size, stride, padding, dilation, ceil_mode).unwrap()
    }

    pub fn max_unpool2d(
        &self, indices: &Tensor, output_size: &[i64]
    ) -> Tensor {
        self.f_max_unpool2d(indices, output_size).unwrap()
    }

    pub fn max_unpool2d_backward(
        &self, grad_output: &Tensor, indices: &Tensor, output_size: &[i64]
    ) -> Tensor {
        self.f_max_unpool2d_backward(grad_output, indices, output_size).unwrap()
    }

    pub fn max_unpool2d_backward_out(
        &self, grad_input: &Tensor, grad_output: &Tensor, indices: &Tensor, output_size: &[i64]
    ) -> Tensor {
        self.f_max_unpool2d_backward_out(grad_input, grad_output, indices, output_size).unwrap()
    }

    pub fn max_unpool2d_out(
        &self, output: &Tensor, indices: &Tensor, output_size: &[i64]
    ) -> Tensor {
        self.f_max_unpool2d_out(output, indices, output_size).unwrap()
    }

    pub fn max_unpool3d(
        &self, indices: &Tensor, output_size: &[i64], stride: &[i64], padding: &[i64]
    ) -> Tensor {
        self.f_max_unpool3d(indices, output_size, stride, padding).unwrap()
    }

    pub fn max_unpool3d_backward(
        &self, grad_output: &Tensor, indices: &Tensor, output_size: &[i64], stride: &[i64], padding: &[i64]
    ) -> Tensor {
        self.f_max_unpool3d_backward(grad_output, indices, output_size, stride, padding).unwrap()
    }

    pub fn max_unpool3d_backward_out(
        &self, grad_input: &Tensor, grad_output: &Tensor, indices: &Tensor, output_size: &[i64], stride: &[i64], padding: &[i64]
    ) -> Tensor {
        self.f_max_unpool3d_backward_out(grad_input, grad_output, indices, output_size, stride, padding).unwrap()
    }

    pub fn max_unpool3d_out(
        &self, output: &Tensor, indices: &Tensor, output_size: &[i64], stride: &[i64], padding: &[i64]
    ) -> Tensor {
        self.f_max_unpool3d_out(output, indices, output_size, stride, padding).unwrap()
    }

    pub fn max_values(
        &self, dim: i64, keepdim: bool
    ) -> Tensor {
        self.f_max_values(dim, keepdim).unwrap()
    }

    pub fn mean(
        &self, 
    ) -> Tensor {
        self.f_mean().unwrap()
    }

    pub fn mean1(
        &self, dtype: Kind
    ) -> Tensor {
        self.f_mean1(dtype).unwrap()
    }

    pub fn mean2(
        &self, dim: i64, keepdim: bool
    ) -> Tensor {
        self.f_mean2(dim, keepdim).unwrap()
    }

    pub fn mean3(
        &self, dim: i64, dtype: Kind
    ) -> Tensor {
        self.f_mean3(dim, dtype).unwrap()
    }

    pub fn mean4(
        &self, dim: i64, keepdim: bool, dtype: Kind
    ) -> Tensor {
        self.f_mean4(dim, keepdim, dtype).unwrap()
    }

    pub fn mean_out(
        &self, result: &Tensor, dim: i64, keepdim: bool
    ) -> Tensor {
        self.f_mean_out(result, dim, keepdim).unwrap()
    }

    pub fn mean_out1(
        &self, result: &Tensor, dim: i64, dtype: Kind
    ) -> Tensor {
        self.f_mean_out1(result, dim, dtype).unwrap()
    }

    pub fn mean_out2(
        &self, result: &Tensor, dim: i64, keepdim: bool, dtype: Kind
    ) -> Tensor {
        self.f_mean_out2(result, dim, keepdim, dtype).unwrap()
    }

    pub fn median(
        &self, 
    ) -> Tensor {
        self.f_median().unwrap()
    }

    pub fn median1(
        &self, dim: i64, keepdim: bool
    ) -> (Tensor, Tensor) {
        self.f_median1(dim, keepdim).unwrap()
    }

    pub fn median_out(
        &self, values: &Tensor, indices: &Tensor, dim: i64, keepdim: bool
    ) -> (Tensor, Tensor) {
        self.f_median_out(values, indices, dim, keepdim).unwrap()
    }

    pub fn meshgrid<T: Borrow<Tensor>>(
        tensors: &[T]
    ) -> Vec<Tensor> {
        Tensor::f_meshgrid(tensors).unwrap()
    }

    pub fn min(
        &self, 
    ) -> Tensor {
        self.f_min().unwrap()
    }

    pub fn min1(
        &self, other: &Tensor
    ) -> Tensor {
        self.f_min1(other).unwrap()
    }

    pub fn min2(
        &self, dim: i64, keepdim: bool
    ) -> (Tensor, Tensor) {
        self.f_min2(dim, keepdim).unwrap()
    }

    pub fn min_out(
        &self, result: &Tensor, other: &Tensor
    ) -> Tensor {
        self.f_min_out(result, other).unwrap()
    }

    pub fn min_out1(
        &self, min: &Tensor, min_indices: &Tensor, dim: i64, keepdim: bool
    ) -> (Tensor, Tensor) {
        self.f_min_out1(min, min_indices, dim, keepdim).unwrap()
    }

    pub fn min_values(
        &self, dim: i64, keepdim: bool
    ) -> Tensor {
        self.f_min_values(dim, keepdim).unwrap()
    }

    pub fn miopen_batch_norm<T: Borrow<Tensor>>(
        &self, weight: &Tensor, bias: Option<T>, running_mean: Option<T>, running_var: Option<T>, training: bool, exponential_average_factor: f64, epsilon: f64
    ) -> (Tensor, Tensor, Tensor) {
        self.f_miopen_batch_norm(weight, bias, running_mean, running_var, training, exponential_average_factor, epsilon).unwrap()
    }

    pub fn miopen_batch_norm_backward<T: Borrow<Tensor>>(
        &self, grad_output: &Tensor, weight: &Tensor, running_mean: Option<T>, running_var: Option<T>, save_mean: Option<T>, save_var: Option<T>, epsilon: f64
    ) -> (Tensor, Tensor, Tensor) {
        self.f_miopen_batch_norm_backward(grad_output, weight, running_mean, running_var, save_mean, save_var, epsilon).unwrap()
    }

    pub fn miopen_convolution<T: Borrow<Tensor>>(
        &self, weight: &Tensor, bias: Option<T>, padding: &[i64], stride: &[i64], dilation: &[i64], groups: i64, benchmark: bool, deterministic: bool
    ) -> Tensor {
        self.f_miopen_convolution(weight, bias, padding, stride, dilation, groups, benchmark, deterministic).unwrap()
    }

    pub fn miopen_convolution_backward_bias(
        grad_output: &Tensor
    ) -> Tensor {
        Tensor::f_miopen_convolution_backward_bias(grad_output).unwrap()
    }

    pub fn miopen_convolution_backward_input(
        self_size: &[i64], grad_output: &Tensor, weight: &Tensor, padding: &[i64], stride: &[i64], dilation: &[i64], groups: i64, benchmark: bool, deterministic: bool
    ) -> Tensor {
        Tensor::f_miopen_convolution_backward_input(self_size, grad_output, weight, padding, stride, dilation, groups, benchmark, deterministic).unwrap()
    }

    pub fn miopen_convolution_backward_weight(
        &self, weight_size: &[i64], grad_output: &Tensor, padding: &[i64], stride: &[i64], dilation: &[i64], groups: i64, benchmark: bool, deterministic: bool
    ) -> Tensor {
        self.f_miopen_convolution_backward_weight(weight_size, grad_output, padding, stride, dilation, groups, benchmark, deterministic).unwrap()
    }

    pub fn miopen_convolution_transpose<T: Borrow<Tensor>>(
        &self, weight: &Tensor, bias: Option<T>, padding: &[i64], output_padding: &[i64], stride: &[i64], dilation: &[i64], groups: i64, benchmark: bool, deterministic: bool
    ) -> Tensor {
        self.f_miopen_convolution_transpose(weight, bias, padding, output_padding, stride, dilation, groups, benchmark, deterministic).unwrap()
    }

    pub fn miopen_convolution_transpose_backward_input(
        grad_output: &Tensor, weight: &Tensor, padding: &[i64], stride: &[i64], dilation: &[i64], groups: i64, benchmark: bool, deterministic: bool
    ) -> Tensor {
        Tensor::f_miopen_convolution_transpose_backward_input(grad_output, weight, padding, stride, dilation, groups, benchmark, deterministic).unwrap()
    }

    pub fn miopen_convolution_transpose_backward_weight(
        &self, weight_size: &[i64], grad_output: &Tensor, padding: &[i64], stride: &[i64], dilation: &[i64], groups: i64, benchmark: bool, deterministic: bool
    ) -> Tensor {
        self.f_miopen_convolution_transpose_backward_weight(weight_size, grad_output, padding, stride, dilation, groups, benchmark, deterministic).unwrap()
    }

    pub fn mkldnn_convolution<T: Borrow<Tensor>>(
        &self, weight: &Tensor, bias: Option<T>, padding: &[i64], stride: &[i64], dilation: &[i64], groups: i64
    ) -> Tensor {
        self.f_mkldnn_convolution(weight, bias, padding, stride, dilation, groups).unwrap()
    }

    pub fn mkldnn_convolution_backward_input(
        self_size: &[i64], grad_output: &Tensor, weight: &Tensor, padding: &[i64], stride: &[i64], dilation: &[i64], groups: i64, bias_defined: bool
    ) -> Tensor {
        Tensor::f_mkldnn_convolution_backward_input(self_size, grad_output, weight, padding, stride, dilation, groups, bias_defined).unwrap()
    }

    pub fn mkldnn_convolution_backward_weights(
        &self, weight_size: &[i64], grad_output: &Tensor, padding: &[i64], stride: &[i64], dilation: &[i64], groups: i64, bias_defined: bool
    ) -> (Tensor, Tensor) {
        self.f_mkldnn_convolution_backward_weights(weight_size, grad_output, padding, stride, dilation, groups, bias_defined).unwrap()
    }

    pub fn mm(
        &self, mat2: &Tensor
    ) -> Tensor {
        self.f_mm(mat2).unwrap()
    }

    pub fn mm_out(
        &self, result: &Tensor, mat2: &Tensor
    ) -> Tensor {
        self.f_mm_out(result, mat2).unwrap()
    }

    pub fn mode(
        &self, dim: i64, keepdim: bool
    ) -> (Tensor, Tensor) {
        self.f_mode(dim, keepdim).unwrap()
    }

    pub fn mode_out(
        &self, values: &Tensor, indices: &Tensor, dim: i64, keepdim: bool
    ) -> (Tensor, Tensor) {
        self.f_mode_out(values, indices, dim, keepdim).unwrap()
    }

    pub fn mse_loss(
        &self, target: &Tensor, reduction: i64
    ) -> Tensor {
        self.f_mse_loss(target, reduction).unwrap()
    }

    pub fn mse_loss_backward(
        &self, grad_output: &Tensor, target: &Tensor, reduction: i64
    ) -> Tensor {
        self.f_mse_loss_backward(grad_output, target, reduction).unwrap()
    }

    pub fn mse_loss_backward_out(
        &self, grad_input: &Tensor, grad_output: &Tensor, target: &Tensor, reduction: i64
    ) -> Tensor {
        self.f_mse_loss_backward_out(grad_input, grad_output, target, reduction).unwrap()
    }

    pub fn mse_loss_out(
        &self, output: &Tensor, target: &Tensor, reduction: i64
    ) -> Tensor {
        self.f_mse_loss_out(output, target, reduction).unwrap()
    }

    pub fn g_mul(
        &self, other: &Tensor
    ) -> Tensor {
        self.f_mul(other).unwrap()
    }

    pub fn g_mul1<S: Into<Scalar>>(
        &self, other: S
    ) -> Tensor {
        self.f_mul1(other).unwrap()
    }

    pub fn g_mul_(
        &self, other: &Tensor
    ) -> Tensor {
        self.f_mul_(other).unwrap()
    }

    pub fn g_mul_1<S: Into<Scalar>>(
        &self, other: S
    ) -> Tensor {
        self.f_mul_1(other).unwrap()
    }

    pub fn mul_out(
        &self, result: &Tensor, other: &Tensor
    ) -> Tensor {
        self.f_mul_out(result, other).unwrap()
    }

    pub fn multilabel_margin_loss(
        &self, target: &Tensor, reduction: i64
    ) -> Tensor {
        self.f_multilabel_margin_loss(target, reduction).unwrap()
    }

    pub fn multilabel_margin_loss_backward(
        &self, grad_output: &Tensor, target: &Tensor, reduction: i64, is_target: &Tensor
    ) -> Tensor {
        self.f_multilabel_margin_loss_backward(grad_output, target, reduction, is_target).unwrap()
    }

    pub fn multilabel_margin_loss_backward_out(
        &self, grad_input: &Tensor, grad_output: &Tensor, target: &Tensor, reduction: i64, is_target: &Tensor
    ) -> Tensor {
        self.f_multilabel_margin_loss_backward_out(grad_input, grad_output, target, reduction, is_target).unwrap()
    }

    pub fn multilabel_margin_loss_out(
        &self, output: &Tensor, target: &Tensor, reduction: i64
    ) -> Tensor {
        self.f_multilabel_margin_loss_out(output, target, reduction).unwrap()
    }

    pub fn multinomial(
        &self, num_samples: i64, replacement: bool
    ) -> Tensor {
        self.f_multinomial(num_samples, replacement).unwrap()
    }

    pub fn multinomial_out(
        &self, result: &Tensor, num_samples: i64, replacement: bool
    ) -> Tensor {
        self.f_multinomial_out(result, num_samples, replacement).unwrap()
    }

    pub fn mv(
        &self, vec: &Tensor
    ) -> Tensor {
        self.f_mv(vec).unwrap()
    }

    pub fn mv_out(
        &self, result: &Tensor, vec: &Tensor
    ) -> Tensor {
        self.f_mv_out(result, vec).unwrap()
    }

    pub fn mvlgamma(
        &self, p: i64
    ) -> Tensor {
        self.f_mvlgamma(p).unwrap()
    }

    pub fn mvlgamma_(
        &self, p: i64
    ) -> Tensor {
        self.f_mvlgamma_(p).unwrap()
    }

    pub fn narrow(
        &self, dim: i64, start: i64, length: i64
    ) -> Tensor {
        self.f_narrow(dim, start, length).unwrap()
    }

    pub fn narrow_copy(
        &self, dim: i64, start: i64, length: i64
    ) -> Tensor {
        self.f_narrow_copy(dim, start, length).unwrap()
    }

    pub fn native_batch_norm<T: Borrow<Tensor>>(
        &self, weight: Option<T>, bias: Option<T>, running_mean: Option<T>, running_var: Option<T>, training: bool, momentum: f64, eps: f64
    ) -> (Tensor, Tensor, Tensor) {
        self.f_native_batch_norm(weight, bias, running_mean, running_var, training, momentum, eps).unwrap()
    }

    pub fn native_clone(
        &self, 
    ) -> Tensor {
        self.f_native_clone().unwrap()
    }

    pub fn native_norm(
        &self, 
    ) -> Tensor {
        self.f_native_norm().unwrap()
    }

    pub fn native_pow<S: Into<Scalar>>(
        &self, exponent: S
    ) -> Tensor {
        self.f_native_pow(exponent).unwrap()
    }

    pub fn native_pow_out<S: Into<Scalar>>(
        &self, result: &Tensor, exponent: S
    ) -> Tensor {
        self.f_native_pow_out(result, exponent).unwrap()
    }

    pub fn native_resize_as_(
        &self, the_template: &Tensor
    ) -> Tensor {
        self.f_native_resize_as_(the_template).unwrap()
    }

    pub fn native_zero_(
        &self, 
    ) -> Tensor {
        self.f_native_zero_().unwrap()
    }

    pub fn ne<S: Into<Scalar>>(
        &self, other: S
    ) -> Tensor {
        self.f_ne(other).unwrap()
    }

    pub fn ne1(
        &self, other: &Tensor
    ) -> Tensor {
        self.f_ne1(other).unwrap()
    }

    pub fn ne_<S: Into<Scalar>>(
        &self, other: S
    ) -> Tensor {
        self.f_ne_(other).unwrap()
    }

    pub fn ne_1(
        &self, other: &Tensor
    ) -> Tensor {
        self.f_ne_1(other).unwrap()
    }

    pub fn ne_out<S: Into<Scalar>>(
        &self, result: &Tensor, other: S
    ) -> Tensor {
        self.f_ne_out(result, other).unwrap()
    }

    pub fn ne_out1(
        &self, result: &Tensor, other: &Tensor
    ) -> Tensor {
        self.f_ne_out1(result, other).unwrap()
    }

    pub fn neg(
        &self, 
    ) -> Tensor {
        self.f_neg().unwrap()
    }

    pub fn neg_(
        &self, 
    ) -> Tensor {
        self.f_neg_().unwrap()
    }

    pub fn neg_out(
        &self, result: &Tensor
    ) -> Tensor {
        self.f_neg_out(result).unwrap()
    }

    pub fn g_nll_loss(
        &self, target: &Tensor, weight: &Tensor, reduction: i64, ignore_index: i64
    ) -> Tensor {
        self.f_nll_loss(target, weight, reduction, ignore_index).unwrap()
    }

    pub fn nll_loss2d(
        &self, target: &Tensor, weight: &Tensor, reduction: i64, ignore_index: i64
    ) -> Tensor {
        self.f_nll_loss2d(target, weight, reduction, ignore_index).unwrap()
    }

    pub fn nll_loss2d_backward<T: Borrow<Tensor>>(
        &self, grad_output: &Tensor, target: &Tensor, weight: Option<T>, reduction: i64, ignore_index: i64, total_weight: &Tensor
    ) -> Tensor {
        self.f_nll_loss2d_backward(grad_output, target, weight, reduction, ignore_index, total_weight).unwrap()
    }

    pub fn nll_loss2d_backward_out<T: Borrow<Tensor>>(
        &self, grad_input: &Tensor, grad_output: &Tensor, target: &Tensor, weight: Option<T>, reduction: i64, ignore_index: i64, total_weight: &Tensor
    ) -> Tensor {
        self.f_nll_loss2d_backward_out(grad_input, grad_output, target, weight, reduction, ignore_index, total_weight).unwrap()
    }

    pub fn nll_loss2d_out(
        &self, output: &Tensor, target: &Tensor, weight: &Tensor, reduction: i64, ignore_index: i64
    ) -> Tensor {
        self.f_nll_loss2d_out(output, target, weight, reduction, ignore_index).unwrap()
    }

    pub fn nll_loss_backward<T: Borrow<Tensor>>(
        &self, grad_output: &Tensor, target: &Tensor, weight: Option<T>, reduction: i64, ignore_index: i64, total_weight: &Tensor
    ) -> Tensor {
        self.f_nll_loss_backward(grad_output, target, weight, reduction, ignore_index, total_weight).unwrap()
    }

    pub fn nll_loss_backward_out<T: Borrow<Tensor>>(
        &self, grad_input: &Tensor, grad_output: &Tensor, target: &Tensor, weight: Option<T>, reduction: i64, ignore_index: i64, total_weight: &Tensor
    ) -> Tensor {
        self.f_nll_loss_backward_out(grad_input, grad_output, target, weight, reduction, ignore_index, total_weight).unwrap()
    }

    pub fn nll_loss_out(
        &self, output: &Tensor, target: &Tensor, weight: &Tensor, reduction: i64, ignore_index: i64
    ) -> Tensor {
        self.f_nll_loss_out(output, target, weight, reduction, ignore_index).unwrap()
    }

    pub fn nonzero(
        &self, 
    ) -> Tensor {
        self.f_nonzero().unwrap()
    }

    pub fn nonzero_out(
        &self, result: &Tensor
    ) -> Tensor {
        self.f_nonzero_out(result).unwrap()
    }

    pub fn norm(
        &self, 
    ) -> Tensor {
        self.f_norm().unwrap()
    }

    pub fn norm1<S: Into<Scalar>>(
        &self, p: S, dim: i64, keepdim: bool
    ) -> Tensor {
        self.f_norm1(p, dim, keepdim).unwrap()
    }

    pub fn norm_except_dim(
        v: &Tensor, pow: i64, dim: i64
    ) -> Tensor {
        Tensor::f_norm_except_dim(v, pow, dim).unwrap()
    }

    pub fn norm_out<S: Into<Scalar>>(
        &self, result: &Tensor, p: S, dim: i64, keepdim: bool
    ) -> Tensor {
        self.f_norm_out(result, p, dim, keepdim).unwrap()
    }

    pub fn normal(
        mean: &Tensor, std: f64
    ) -> Tensor {
        Tensor::f_normal(mean, std).unwrap()
    }

    pub fn normal1(
        mean: f64, std: &Tensor
    ) -> Tensor {
        Tensor::f_normal1(mean, std).unwrap()
    }

    pub fn normal2(
        mean: &Tensor, std: &Tensor
    ) -> Tensor {
        Tensor::f_normal2(mean, std).unwrap()
    }

    pub fn normal_(
        &self, mean: f64, std: f64
    ) -> Tensor {
        self.f_normal_(mean, std).unwrap()
    }

    pub fn normal_out(
        output: &Tensor, mean: &Tensor, std: f64
    ) -> Tensor {
        Tensor::f_normal_out(output, mean, std).unwrap()
    }

    pub fn normal_out1(
        output: &Tensor, mean: f64, std: &Tensor
    ) -> Tensor {
        Tensor::f_normal_out1(output, mean, std).unwrap()
    }

    pub fn normal_out2(
        output: &Tensor, mean: &Tensor, std: &Tensor
    ) -> Tensor {
        Tensor::f_normal_out2(output, mean, std).unwrap()
    }

    pub fn nuclear_norm(
        &self, keepdim: bool
    ) -> Tensor {
        self.f_nuclear_norm(keepdim).unwrap()
    }

    pub fn nuclear_norm_out(
        &self, result: &Tensor, keepdim: bool
    ) -> Tensor {
        self.f_nuclear_norm_out(result, keepdim).unwrap()
    }

    pub fn ones(
        size: &[i64], options: (Kind, Device)
    ) -> Tensor {
        Tensor::f_ones(size, options).unwrap()
    }

    pub fn ones_like(
        &self, 
    ) -> Tensor {
        self.f_ones_like().unwrap()
    }

    pub fn ones_like1(
        &self, options: (Kind, Device)
    ) -> Tensor {
        self.f_ones_like1(options).unwrap()
    }

    pub fn ones_out(
        result: &Tensor, size: &[i64]
    ) -> Tensor {
        Tensor::f_ones_out(result, size).unwrap()
    }

    pub fn orgqr(
        &self, input2: &Tensor
    ) -> Tensor {
        self.f_orgqr(input2).unwrap()
    }

    pub fn orgqr_out(
        &self, result: &Tensor, input2: &Tensor
    ) -> Tensor {
        self.f_orgqr_out(result, input2).unwrap()
    }

    pub fn ormqr(
        &self, input2: &Tensor, input3: &Tensor, left: bool, transpose: bool
    ) -> Tensor {
        self.f_ormqr(input2, input3, left, transpose).unwrap()
    }

    pub fn ormqr_out(
        &self, result: &Tensor, input2: &Tensor, input3: &Tensor, left: bool, transpose: bool
    ) -> Tensor {
        self.f_ormqr_out(result, input2, input3, left, transpose).unwrap()
    }

    pub fn pairwise_distance(
        x1: &Tensor, x2: &Tensor, p: f64, eps: f64, keepdim: bool
    ) -> Tensor {
        Tensor::f_pairwise_distance(x1, x2, p, eps, keepdim).unwrap()
    }

    pub fn pdist(
        &self, p: f64
    ) -> Tensor {
        self.f_pdist(p).unwrap()
    }

    pub fn permute(
        &self, dims: &[i64]
    ) -> Tensor {
        self.f_permute(dims).unwrap()
    }

    pub fn pin_memory(
        &self, 
    ) -> Tensor {
        self.f_pin_memory().unwrap()
    }

    pub fn pinverse(
        &self, rcond: f64
    ) -> Tensor {
        self.f_pinverse(rcond).unwrap()
    }

    pub fn pixel_shuffle(
        &self, upscale_factor: i64
    ) -> Tensor {
        self.f_pixel_shuffle(upscale_factor).unwrap()
    }

    pub fn poisson(
        &self, 
    ) -> Tensor {
        self.f_poisson().unwrap()
    }

    pub fn polygamma(
        &self, n: i64
    ) -> Tensor {
        self.f_polygamma(n).unwrap()
    }

    pub fn polygamma_(
        &self, n: i64
    ) -> Tensor {
        self.f_polygamma_(n).unwrap()
    }

    pub fn polygamma_out(
        &self, result: &Tensor, n: i64
    ) -> Tensor {
        self.f_polygamma_out(result, n).unwrap()
    }

    pub fn potri(
        &self, upper: bool
    ) -> Tensor {
        self.f_potri(upper).unwrap()
    }

    pub fn potri_out(
        &self, result: &Tensor, upper: bool
    ) -> Tensor {
        self.f_potri_out(result, upper).unwrap()
    }

    pub fn potrs(
        &self, input2: &Tensor, upper: bool
    ) -> Tensor {
        self.f_potrs(input2, upper).unwrap()
    }

    pub fn potrs_out(
        &self, result: &Tensor, input2: &Tensor, upper: bool
    ) -> Tensor {
        self.f_potrs_out(result, input2, upper).unwrap()
    }

    pub fn pow<S: Into<Scalar>>(
        &self, exponent: S
    ) -> Tensor {
        self.f_pow(exponent).unwrap()
    }

    pub fn pow1(
        &self, exponent: &Tensor
    ) -> Tensor {
        self.f_pow1(exponent).unwrap()
    }

    pub fn pow2<S: Into<Scalar>>(
        self_scalar: S, exponent: &Tensor
    ) -> Tensor {
        Tensor::f_pow2(self_scalar, exponent).unwrap()
    }

    pub fn pow_<S: Into<Scalar>>(
        &self, exponent: S
    ) -> Tensor {
        self.f_pow_(exponent).unwrap()
    }

    pub fn pow_1(
        &self, exponent: &Tensor
    ) -> Tensor {
        self.f_pow_1(exponent).unwrap()
    }

    pub fn pow_out<S: Into<Scalar>>(
        &self, result: &Tensor, exponent: S
    ) -> Tensor {
        self.f_pow_out(result, exponent).unwrap()
    }

    pub fn pow_out1(
        &self, result: &Tensor, exponent: &Tensor
    ) -> Tensor {
        self.f_pow_out1(result, exponent).unwrap()
    }

    pub fn pow_out2<S: Into<Scalar>>(
        result: &Tensor, self_scalar: S, exponent: &Tensor
    ) -> Tensor {
        Tensor::f_pow_out2(result, self_scalar, exponent).unwrap()
    }

    pub fn prelu(
        &self, weight: &Tensor
    ) -> Tensor {
        self.f_prelu(weight).unwrap()
    }

    pub fn prelu_backward(
        &self, grad_output: &Tensor, weight: &Tensor
    ) -> (Tensor, Tensor) {
        self.f_prelu_backward(grad_output, weight).unwrap()
    }

    pub fn prod(
        &self, 
    ) -> Tensor {
        self.f_prod().unwrap()
    }

    pub fn prod1(
        &self, dtype: Kind
    ) -> Tensor {
        self.f_prod1(dtype).unwrap()
    }

    pub fn prod2(
        &self, dim: i64, keepdim: bool
    ) -> Tensor {
        self.f_prod2(dim, keepdim).unwrap()
    }

    pub fn prod3(
        &self, dim: i64, dtype: Kind
    ) -> Tensor {
        self.f_prod3(dim, dtype).unwrap()
    }

    pub fn prod4(
        &self, dim: i64, keepdim: bool, dtype: Kind
    ) -> Tensor {
        self.f_prod4(dim, keepdim, dtype).unwrap()
    }

    pub fn prod_out(
        &self, result: &Tensor, dim: i64, keepdim: bool
    ) -> Tensor {
        self.f_prod_out(result, dim, keepdim).unwrap()
    }

    pub fn prod_out1(
        &self, result: &Tensor, dim: i64, dtype: Kind
    ) -> Tensor {
        self.f_prod_out1(result, dim, dtype).unwrap()
    }

    pub fn prod_out2(
        &self, result: &Tensor, dim: i64, keepdim: bool, dtype: Kind
    ) -> Tensor {
        self.f_prod_out2(result, dim, keepdim, dtype).unwrap()
    }

    pub fn pstrf(
        &self, upper: bool
    ) -> (Tensor, Tensor) {
        self.f_pstrf(upper).unwrap()
    }

    pub fn pstrf_out(
        &self, u: &Tensor, piv: &Tensor, upper: bool
    ) -> (Tensor, Tensor) {
        self.f_pstrf_out(u, piv, upper).unwrap()
    }

    pub fn put_(
        &self, index: &Tensor, source: &Tensor, accumulate: bool
    ) -> Tensor {
        self.f_put_(index, source, accumulate).unwrap()
    }

    pub fn qr(
        &self, 
    ) -> (Tensor, Tensor) {
        self.f_qr().unwrap()
    }

    pub fn qr_out(
        &self, q: &Tensor, r: &Tensor
    ) -> (Tensor, Tensor) {
        self.f_qr_out(q, r).unwrap()
    }

    pub fn rand(
        size: &[i64], options: (Kind, Device)
    ) -> Tensor {
        Tensor::f_rand(size, options).unwrap()
    }

    pub fn rand_like(
        &self, 
    ) -> Tensor {
        self.f_rand_like().unwrap()
    }

    pub fn rand_like1(
        &self, options: (Kind, Device)
    ) -> Tensor {
        self.f_rand_like1(options).unwrap()
    }

    pub fn rand_out(
        result: &Tensor, size: &[i64]
    ) -> Tensor {
        Tensor::f_rand_out(result, size).unwrap()
    }

    pub fn randint(
        high: i64, size: &[i64], options: (Kind, Device)
    ) -> Tensor {
        Tensor::f_randint(high, size, options).unwrap()
    }

    pub fn randint1(
        low: i64, high: i64, size: &[i64], options: (Kind, Device)
    ) -> Tensor {
        Tensor::f_randint1(low, high, size, options).unwrap()
    }

    pub fn randint_like(
        &self, high: i64
    ) -> Tensor {
        self.f_randint_like(high).unwrap()
    }

    pub fn randint_like1(
        &self, low: i64, high: i64
    ) -> Tensor {
        self.f_randint_like1(low, high).unwrap()
    }

    pub fn randint_like2(
        &self, high: i64, options: (Kind, Device)
    ) -> Tensor {
        self.f_randint_like2(high, options).unwrap()
    }

    pub fn randint_like3(
        &self, low: i64, high: i64, options: (Kind, Device)
    ) -> Tensor {
        self.f_randint_like3(low, high, options).unwrap()
    }

    pub fn randint_out(
        result: &Tensor, high: i64, size: &[i64]
    ) -> Tensor {
        Tensor::f_randint_out(result, high, size).unwrap()
    }

    pub fn randint_out1(
        result: &Tensor, low: i64, high: i64, size: &[i64]
    ) -> Tensor {
        Tensor::f_randint_out1(result, low, high, size).unwrap()
    }

    pub fn randn(
        size: &[i64], options: (Kind, Device)
    ) -> Tensor {
        Tensor::f_randn(size, options).unwrap()
    }

    pub fn randn_like(
        &self, 
    ) -> Tensor {
        self.f_randn_like().unwrap()
    }

    pub fn randn_like1(
        &self, options: (Kind, Device)
    ) -> Tensor {
        self.f_randn_like1(options).unwrap()
    }

    pub fn randn_out(
        result: &Tensor, size: &[i64]
    ) -> Tensor {
        Tensor::f_randn_out(result, size).unwrap()
    }

    pub fn random_(
        &self, 
    ) -> Tensor {
        self.f_random_().unwrap()
    }

    pub fn random_1(
        &self, to: i64
    ) -> Tensor {
        self.f_random_1(to).unwrap()
    }

    pub fn random_2(
        &self, from: i64, to: i64
    ) -> Tensor {
        self.f_random_2(from, to).unwrap()
    }

    pub fn randperm(
        n: i64, options: (Kind, Device)
    ) -> Tensor {
        Tensor::f_randperm(n, options).unwrap()
    }

    pub fn randperm_out(
        result: &Tensor, n: i64
    ) -> Tensor {
        Tensor::f_randperm_out(result, n).unwrap()
    }

    pub fn range<S: Into<Scalar>>(
        start: S, end: S, options: (Kind, Device)
    ) -> Tensor {
        Tensor::f_range(start, end, options).unwrap()
    }

    pub fn range1<S: Into<Scalar>>(
        start: S, end: S, step: S, options: (Kind, Device)
    ) -> Tensor {
        Tensor::f_range1(start, end, step, options).unwrap()
    }

    pub fn range_out<S: Into<Scalar>>(
        result: &Tensor, start: S, end: S
    ) -> Tensor {
        Tensor::f_range_out(result, start, end).unwrap()
    }

    pub fn range_out1<S: Into<Scalar>>(
        result: &Tensor, start: S, end: S, step: S
    ) -> Tensor {
        Tensor::f_range_out1(result, start, end, step).unwrap()
    }

    pub fn reciprocal(
        &self, 
    ) -> Tensor {
        self.f_reciprocal().unwrap()
    }

    pub fn reciprocal_(
        &self, 
    ) -> Tensor {
        self.f_reciprocal_().unwrap()
    }

    pub fn reciprocal_out(
        &self, result: &Tensor
    ) -> Tensor {
        self.f_reciprocal_out(result).unwrap()
    }

    pub fn reflection_pad1d(
        &self, padding: &[i64]
    ) -> Tensor {
        self.f_reflection_pad1d(padding).unwrap()
    }

    pub fn reflection_pad1d_backward(
        &self, grad_output: &Tensor, padding: &[i64]
    ) -> Tensor {
        self.f_reflection_pad1d_backward(grad_output, padding).unwrap()
    }

    pub fn reflection_pad1d_backward_out(
        &self, grad_input: &Tensor, grad_output: &Tensor, padding: &[i64]
    ) -> Tensor {
        self.f_reflection_pad1d_backward_out(grad_input, grad_output, padding).unwrap()
    }

    pub fn reflection_pad1d_out(
        &self, output: &Tensor, padding: &[i64]
    ) -> Tensor {
        self.f_reflection_pad1d_out(output, padding).unwrap()
    }

    pub fn reflection_pad2d(
        &self, padding: &[i64]
    ) -> Tensor {
        self.f_reflection_pad2d(padding).unwrap()
    }

    pub fn reflection_pad2d_backward(
        &self, grad_output: &Tensor, padding: &[i64]
    ) -> Tensor {
        self.f_reflection_pad2d_backward(grad_output, padding).unwrap()
    }

    pub fn reflection_pad2d_backward_out(
        &self, grad_input: &Tensor, grad_output: &Tensor, padding: &[i64]
    ) -> Tensor {
        self.f_reflection_pad2d_backward_out(grad_input, grad_output, padding).unwrap()
    }

    pub fn reflection_pad2d_out(
        &self, output: &Tensor, padding: &[i64]
    ) -> Tensor {
        self.f_reflection_pad2d_out(output, padding).unwrap()
    }

    pub fn relu(
        &self, 
    ) -> Tensor {
        self.f_relu().unwrap()
    }

    pub fn relu_(
        &self, 
    ) -> Tensor {
        self.f_relu_().unwrap()
    }

    pub fn remainder<S: Into<Scalar>>(
        &self, other: S
    ) -> Tensor {
        self.f_remainder(other).unwrap()
    }

    pub fn remainder1(
        &self, other: &Tensor
    ) -> Tensor {
        self.f_remainder1(other).unwrap()
    }

    pub fn remainder_<S: Into<Scalar>>(
        &self, other: S
    ) -> Tensor {
        self.f_remainder_(other).unwrap()
    }

    pub fn remainder_1(
        &self, other: &Tensor
    ) -> Tensor {
        self.f_remainder_1(other).unwrap()
    }

    pub fn remainder_out<S: Into<Scalar>>(
        &self, result: &Tensor, other: S
    ) -> Tensor {
        self.f_remainder_out(result, other).unwrap()
    }

    pub fn remainder_out1(
        &self, result: &Tensor, other: &Tensor
    ) -> Tensor {
        self.f_remainder_out1(result, other).unwrap()
    }

    pub fn renorm<S: Into<Scalar>>(
        &self, p: S, dim: i64, maxnorm: S
    ) -> Tensor {
        self.f_renorm(p, dim, maxnorm).unwrap()
    }

    pub fn renorm_<S: Into<Scalar>>(
        &self, p: S, dim: i64, maxnorm: S
    ) -> Tensor {
        self.f_renorm_(p, dim, maxnorm).unwrap()
    }

    pub fn renorm_out<S: Into<Scalar>>(
        &self, result: &Tensor, p: S, dim: i64, maxnorm: S
    ) -> Tensor {
        self.f_renorm_out(result, p, dim, maxnorm).unwrap()
    }

    pub fn repeat(
        &self, repeats: &[i64]
    ) -> Tensor {
        self.f_repeat(repeats).unwrap()
    }

    pub fn replication_pad1d(
        &self, padding: &[i64]
    ) -> Tensor {
        self.f_replication_pad1d(padding).unwrap()
    }

    pub fn replication_pad1d_backward(
        &self, grad_output: &Tensor, padding: &[i64]
    ) -> Tensor {
        self.f_replication_pad1d_backward(grad_output, padding).unwrap()
    }

    pub fn replication_pad1d_backward_out(
        &self, grad_input: &Tensor, grad_output: &Tensor, padding: &[i64]
    ) -> Tensor {
        self.f_replication_pad1d_backward_out(grad_input, grad_output, padding).unwrap()
    }

    pub fn replication_pad1d_out(
        &self, output: &Tensor, padding: &[i64]
    ) -> Tensor {
        self.f_replication_pad1d_out(output, padding).unwrap()
    }

    pub fn replication_pad2d(
        &self, padding: &[i64]
    ) -> Tensor {
        self.f_replication_pad2d(padding).unwrap()
    }

    pub fn replication_pad2d_backward(
        &self, grad_output: &Tensor, padding: &[i64]
    ) -> Tensor {
        self.f_replication_pad2d_backward(grad_output, padding).unwrap()
    }

    pub fn replication_pad2d_backward_out(
        &self, grad_input: &Tensor, grad_output: &Tensor, padding: &[i64]
    ) -> Tensor {
        self.f_replication_pad2d_backward_out(grad_input, grad_output, padding).unwrap()
    }

    pub fn replication_pad2d_out(
        &self, output: &Tensor, padding: &[i64]
    ) -> Tensor {
        self.f_replication_pad2d_out(output, padding).unwrap()
    }

    pub fn replication_pad3d(
        &self, padding: &[i64]
    ) -> Tensor {
        self.f_replication_pad3d(padding).unwrap()
    }

    pub fn replication_pad3d_backward(
        &self, grad_output: &Tensor, padding: &[i64]
    ) -> Tensor {
        self.f_replication_pad3d_backward(grad_output, padding).unwrap()
    }

    pub fn replication_pad3d_backward_out(
        &self, grad_input: &Tensor, grad_output: &Tensor, padding: &[i64]
    ) -> Tensor {
        self.f_replication_pad3d_backward_out(grad_input, grad_output, padding).unwrap()
    }

    pub fn replication_pad3d_out(
        &self, output: &Tensor, padding: &[i64]
    ) -> Tensor {
        self.f_replication_pad3d_out(output, padding).unwrap()
    }

    pub fn reshape(
        &self, shape: &[i64]
    ) -> Tensor {
        self.f_reshape(shape).unwrap()
    }

    pub fn reshape_as(
        &self, other: &Tensor
    ) -> Tensor {
        self.f_reshape_as(other).unwrap()
    }

    pub fn resize_(
        &self, size: &[i64]
    ) -> Tensor {
        self.f_resize_(size).unwrap()
    }

    pub fn resize_as_(
        &self, the_template: &Tensor
    ) -> Tensor {
        self.f_resize_as_(the_template).unwrap()
    }

    pub fn rfft(
        &self, signal_ndim: i64, normalized: bool, onesided: bool
    ) -> Tensor {
        self.f_rfft(signal_ndim, normalized, onesided).unwrap()
    }

    pub fn rnn_relu<T: Borrow<Tensor>>(
        &self, hx: &Tensor, params: &[T], has_biases: bool, num_layers: i64, dropout: f64, train: bool, bidirectional: bool, batch_first: bool
    ) -> (Tensor, Tensor) {
        self.f_rnn_relu(hx, params, has_biases, num_layers, dropout, train, bidirectional, batch_first).unwrap()
    }

    pub fn rnn_relu1<T: Borrow<Tensor>>(
        data: &Tensor, batch_sizes: &Tensor, hx: &Tensor, params: &[T], has_biases: bool, num_layers: i64, dropout: f64, train: bool, bidirectional: bool
    ) -> (Tensor, Tensor) {
        Tensor::f_rnn_relu1(data, batch_sizes, hx, params, has_biases, num_layers, dropout, train, bidirectional).unwrap()
    }

    pub fn rnn_relu_cell<T: Borrow<Tensor>>(
        &self, hx: &Tensor, w_ih: &Tensor, w_hh: &Tensor, b_ih: Option<T>, b_hh: Option<T>
    ) -> Tensor {
        self.f_rnn_relu_cell(hx, w_ih, w_hh, b_ih, b_hh).unwrap()
    }

    pub fn rnn_tanh<T: Borrow<Tensor>>(
        &self, hx: &Tensor, params: &[T], has_biases: bool, num_layers: i64, dropout: f64, train: bool, bidirectional: bool, batch_first: bool
    ) -> (Tensor, Tensor) {
        self.f_rnn_tanh(hx, params, has_biases, num_layers, dropout, train, bidirectional, batch_first).unwrap()
    }

    pub fn rnn_tanh1<T: Borrow<Tensor>>(
        data: &Tensor, batch_sizes: &Tensor, hx: &Tensor, params: &[T], has_biases: bool, num_layers: i64, dropout: f64, train: bool, bidirectional: bool
    ) -> (Tensor, Tensor) {
        Tensor::f_rnn_tanh1(data, batch_sizes, hx, params, has_biases, num_layers, dropout, train, bidirectional).unwrap()
    }

    pub fn rnn_tanh_cell<T: Borrow<Tensor>>(
        &self, hx: &Tensor, w_ih: &Tensor, w_hh: &Tensor, b_ih: Option<T>, b_hh: Option<T>
    ) -> Tensor {
        self.f_rnn_tanh_cell(hx, w_ih, w_hh, b_ih, b_hh).unwrap()
    }

    pub fn roipooling2d_backward(
        &self, rois: &Tensor, pooledheight: i64, pooledwidth: i64, spatialscale: f64, gradoutput: &Tensor, argmaxes: &Tensor
    ) -> Tensor {
        self.f_roipooling2d_backward(rois, pooledheight, pooledwidth, spatialscale, gradoutput, argmaxes).unwrap()
    }

    pub fn roll(
        &self, shifts: &[i64], dims: &[i64]
    ) -> Tensor {
        self.f_roll(shifts, dims).unwrap()
    }

    pub fn rot90(
        &self, k: i64, dims: &[i64]
    ) -> Tensor {
        self.f_rot90(k, dims).unwrap()
    }

    pub fn round(
        &self, 
    ) -> Tensor {
        self.f_round().unwrap()
    }

    pub fn round_(
        &self, 
    ) -> Tensor {
        self.f_round_().unwrap()
    }

    pub fn round_out(
        &self, result: &Tensor
    ) -> Tensor {
        self.f_round_out(result).unwrap()
    }

    pub fn rrelu(
        &self, training: bool
    ) -> Tensor {
        self.f_rrelu(training).unwrap()
    }

    pub fn rrelu_(
        &self, training: bool
    ) -> Tensor {
        self.f_rrelu_(training).unwrap()
    }

    pub fn rrelu_with_noise(
        &self, noise: &Tensor, training: bool
    ) -> Tensor {
        self.f_rrelu_with_noise(noise, training).unwrap()
    }

    pub fn rrelu_with_noise_(
        &self, noise: &Tensor, training: bool
    ) -> Tensor {
        self.f_rrelu_with_noise_(noise, training).unwrap()
    }

    pub fn rrelu_with_noise_out(
        &self, output: &Tensor, noise: &Tensor, training: bool
    ) -> Tensor {
        self.f_rrelu_with_noise_out(output, noise, training).unwrap()
    }

    pub fn rsqrt(
        &self, 
    ) -> Tensor {
        self.f_rsqrt().unwrap()
    }

    pub fn rsqrt_(
        &self, 
    ) -> Tensor {
        self.f_rsqrt_().unwrap()
    }

    pub fn rsqrt_out(
        &self, result: &Tensor
    ) -> Tensor {
        self.f_rsqrt_out(result).unwrap()
    }

    pub fn rsub(
        &self, other: &Tensor
    ) -> Tensor {
        self.f_rsub(other).unwrap()
    }

    pub fn rsub1<S: Into<Scalar>>(
        &self, other: S
    ) -> Tensor {
        self.f_rsub1(other).unwrap()
    }

    pub fn s_native_addmm(
        &self, mat1: &Tensor, mat2: &Tensor
    ) -> Tensor {
        self.f_s_native_addmm(mat1, mat2).unwrap()
    }

    pub fn s_native_addmm_(
        &self, mat1: &Tensor, mat2: &Tensor
    ) -> Tensor {
        self.f_s_native_addmm_(mat1, mat2).unwrap()
    }

    pub fn s_native_addmm_out(
        &self, result: &Tensor, mat1: &Tensor, mat2: &Tensor
    ) -> Tensor {
        self.f_s_native_addmm_out(result, mat1, mat2).unwrap()
    }

    pub fn scatter_(
        &self, dim: i64, index: &Tensor, src: &Tensor
    ) -> Tensor {
        self.f_scatter_(dim, index, src).unwrap()
    }

    pub fn scatter_1<S: Into<Scalar>>(
        &self, dim: i64, index: &Tensor, value: S
    ) -> Tensor {
        self.f_scatter_1(dim, index, value).unwrap()
    }

    pub fn scatter_add_(
        &self, dim: i64, index: &Tensor, src: &Tensor
    ) -> Tensor {
        self.f_scatter_add_(dim, index, src).unwrap()
    }

    pub fn select(
        &self, dim: i64, index: i64
    ) -> Tensor {
        self.f_select(dim, index).unwrap()
    }

    pub fn selu(
        &self, 
    ) -> Tensor {
        self.f_selu().unwrap()
    }

    pub fn selu_(
        &self, 
    ) -> Tensor {
        self.f_selu_().unwrap()
    }

    pub fn set_(
        &self, 
    ) -> Tensor {
        self.f_set_().unwrap()
    }

    pub fn set_1(
        &self, source: &Tensor
    ) -> Tensor {
        self.f_set_1(source).unwrap()
    }

    pub fn set_requires_grad(
        &self, r: bool
    ) -> Tensor {
        self.f_set_requires_grad(r).unwrap()
    }

    pub fn sigmoid(
        &self, 
    ) -> Tensor {
        self.f_sigmoid().unwrap()
    }

    pub fn sigmoid_(
        &self, 
    ) -> Tensor {
        self.f_sigmoid_().unwrap()
    }

    pub fn sigmoid_out(
        &self, result: &Tensor
    ) -> Tensor {
        self.f_sigmoid_out(result).unwrap()
    }

    pub fn sign(
        &self, 
    ) -> Tensor {
        self.f_sign().unwrap()
    }

    pub fn sign_(
        &self, 
    ) -> Tensor {
        self.f_sign_().unwrap()
    }

    pub fn sign_out(
        &self, result: &Tensor
    ) -> Tensor {
        self.f_sign_out(result).unwrap()
    }

    pub fn sin(
        &self, 
    ) -> Tensor {
        self.f_sin().unwrap()
    }

    pub fn sin_(
        &self, 
    ) -> Tensor {
        self.f_sin_().unwrap()
    }

    pub fn sin_out(
        &self, result: &Tensor
    ) -> Tensor {
        self.f_sin_out(result).unwrap()
    }

    pub fn sinh(
        &self, 
    ) -> Tensor {
        self.f_sinh().unwrap()
    }

    pub fn sinh_(
        &self, 
    ) -> Tensor {
        self.f_sinh_().unwrap()
    }

    pub fn sinh_out(
        &self, result: &Tensor
    ) -> Tensor {
        self.f_sinh_out(result).unwrap()
    }

    pub fn slice(
        &self, dim: i64, start: i64, end: i64, step: i64
    ) -> Tensor {
        self.f_slice(dim, start, end, step).unwrap()
    }

    pub fn slogdet(
        &self, 
    ) -> (Tensor, Tensor) {
        self.f_slogdet().unwrap()
    }

    pub fn smm(
        &self, mat2: &Tensor
    ) -> Tensor {
        self.f_smm(mat2).unwrap()
    }

    pub fn smooth_l1_loss(
        &self, target: &Tensor, reduction: i64
    ) -> Tensor {
        self.f_smooth_l1_loss(target, reduction).unwrap()
    }

    pub fn smooth_l1_loss_backward(
        &self, grad_output: &Tensor, target: &Tensor, reduction: i64
    ) -> Tensor {
        self.f_smooth_l1_loss_backward(grad_output, target, reduction).unwrap()
    }

    pub fn smooth_l1_loss_backward_out(
        &self, grad_input: &Tensor, grad_output: &Tensor, target: &Tensor, reduction: i64
    ) -> Tensor {
        self.f_smooth_l1_loss_backward_out(grad_input, grad_output, target, reduction).unwrap()
    }

    pub fn smooth_l1_loss_out(
        &self, output: &Tensor, target: &Tensor, reduction: i64
    ) -> Tensor {
        self.f_smooth_l1_loss_out(output, target, reduction).unwrap()
    }

    pub fn soft_margin_loss(
        &self, target: &Tensor, reduction: i64
    ) -> Tensor {
        self.f_soft_margin_loss(target, reduction).unwrap()
    }

    pub fn soft_margin_loss_backward(
        &self, grad_output: &Tensor, target: &Tensor, reduction: i64
    ) -> Tensor {
        self.f_soft_margin_loss_backward(grad_output, target, reduction).unwrap()
    }

    pub fn soft_margin_loss_backward_out(
        &self, grad_input: &Tensor, grad_output: &Tensor, target: &Tensor, reduction: i64
    ) -> Tensor {
        self.f_soft_margin_loss_backward_out(grad_input, grad_output, target, reduction).unwrap()
    }

    pub fn soft_margin_loss_out(
        &self, output: &Tensor, target: &Tensor, reduction: i64
    ) -> Tensor {
        self.f_soft_margin_loss_out(output, target, reduction).unwrap()
    }

    pub fn softmax(
        &self, dim: i64
    ) -> Tensor {
        self.f_softmax(dim).unwrap()
    }

    pub fn softmax1(
        &self, dim: i64, dtype: Kind
    ) -> Tensor {
        self.f_softmax1(dim, dtype).unwrap()
    }

    pub fn softplus(
        &self, 
    ) -> Tensor {
        self.f_softplus().unwrap()
    }

    pub fn softplus_out(
        &self, output: &Tensor
    ) -> Tensor {
        self.f_softplus_out(output).unwrap()
    }

    pub fn softshrink(
        &self, 
    ) -> Tensor {
        self.f_softshrink().unwrap()
    }

    pub fn softshrink_out(
        &self, output: &Tensor
    ) -> Tensor {
        self.f_softshrink_out(output).unwrap()
    }

    pub fn sort(
        &self, dim: i64, descending: bool
    ) -> (Tensor, Tensor) {
        self.f_sort(dim, descending).unwrap()
    }

    pub fn sort_out(
        &self, values: &Tensor, indices: &Tensor, dim: i64, descending: bool
    ) -> (Tensor, Tensor) {
        self.f_sort_out(values, indices, dim, descending).unwrap()
    }

    pub fn sparse_coo_tensor(
        size: &[i64], options: (Kind, Device)
    ) -> Tensor {
        Tensor::f_sparse_coo_tensor(size, options).unwrap()
    }

    pub fn sparse_coo_tensor1(
        indices: &Tensor, values: &Tensor, options: (Kind, Device)
    ) -> Tensor {
        Tensor::f_sparse_coo_tensor1(indices, values, options).unwrap()
    }

    pub fn sparse_coo_tensor2(
        indices: &Tensor, values: &Tensor, size: &[i64], options: (Kind, Device)
    ) -> Tensor {
        Tensor::f_sparse_coo_tensor2(indices, values, size, options).unwrap()
    }

    pub fn sparse_resize_(
        &self, size: &[i64], sparse_dim: i64, dense_dim: i64
    ) -> Tensor {
        self.f_sparse_resize_(size, sparse_dim, dense_dim).unwrap()
    }

    pub fn sparse_resize_and_clear_(
        &self, size: &[i64], sparse_dim: i64, dense_dim: i64
    ) -> Tensor {
        self.f_sparse_resize_and_clear_(size, sparse_dim, dense_dim).unwrap()
    }

    pub fn split(
        &self, split_size: i64, dim: i64
    ) -> Vec<Tensor> {
        self.f_split(split_size, dim).unwrap()
    }

    pub fn split_with_sizes(
        &self, split_sizes: &[i64], dim: i64
    ) -> Vec<Tensor> {
        self.f_split_with_sizes(split_sizes, dim).unwrap()
    }

    pub fn sqrt(
        &self, 
    ) -> Tensor {
        self.f_sqrt().unwrap()
    }

    pub fn sqrt_(
        &self, 
    ) -> Tensor {
        self.f_sqrt_().unwrap()
    }

    pub fn sqrt_out(
        &self, result: &Tensor
    ) -> Tensor {
        self.f_sqrt_out(result).unwrap()
    }

    pub fn squeeze(
        &self, 
    ) -> Tensor {
        self.f_squeeze().unwrap()
    }

    pub fn squeeze1(
        &self, dim: i64
    ) -> Tensor {
        self.f_squeeze1(dim).unwrap()
    }

    pub fn squeeze_(
        &self, 
    ) -> Tensor {
        self.f_squeeze_().unwrap()
    }

    pub fn squeeze_1(
        &self, dim: i64
    ) -> Tensor {
        self.f_squeeze_1(dim).unwrap()
    }

    pub fn sspaddmm(
        &self, mat1: &Tensor, mat2: &Tensor
    ) -> Tensor {
        self.f_sspaddmm(mat1, mat2).unwrap()
    }

    pub fn sspaddmm_out(
        &self, result: &Tensor, mat1: &Tensor, mat2: &Tensor
    ) -> Tensor {
        self.f_sspaddmm_out(result, mat1, mat2).unwrap()
    }

    pub fn stack<T: Borrow<Tensor>>(
        tensors: &[T], dim: i64
    ) -> Tensor {
        Tensor::f_stack(tensors, dim).unwrap()
    }

    pub fn stack_out<T: Borrow<Tensor>>(
        result: &Tensor, tensors: &[T], dim: i64
    ) -> Tensor {
        Tensor::f_stack_out(result, tensors, dim).unwrap()
    }

    pub fn std(
        &self, unbiased: bool
    ) -> Tensor {
        self.f_std(unbiased).unwrap()
    }

    pub fn std1(
        &self, dim: i64, unbiased: bool, keepdim: bool
    ) -> Tensor {
        self.f_std1(dim, unbiased, keepdim).unwrap()
    }

    pub fn std_out(
        &self, result: &Tensor, dim: i64, unbiased: bool, keepdim: bool
    ) -> Tensor {
        self.f_std_out(result, dim, unbiased, keepdim).unwrap()
    }

    pub fn stft<T: Borrow<Tensor>>(
        &self, n_fft: i64, hop_length: i64, win_length: i64, window: Option<T>, normalized: bool, onesided: bool
    ) -> Tensor {
        self.f_stft(n_fft, hop_length, win_length, window, normalized, onesided).unwrap()
    }

    pub fn g_sub(
        &self, other: &Tensor
    ) -> Tensor {
        self.f_sub(other).unwrap()
    }

    pub fn g_sub1<S: Into<Scalar>>(
        &self, other: S
    ) -> Tensor {
        self.f_sub1(other).unwrap()
    }

    pub fn g_sub_(
        &self, other: &Tensor
    ) -> Tensor {
        self.f_sub_(other).unwrap()
    }

    pub fn g_sub_1<S: Into<Scalar>>(
        &self, other: S
    ) -> Tensor {
        self.f_sub_1(other).unwrap()
    }

    pub fn sub_out(
        &self, result: &Tensor, other: &Tensor
    ) -> Tensor {
        self.f_sub_out(result, other).unwrap()
    }

    pub fn sum(
        &self, 
    ) -> Tensor {
        self.f_sum().unwrap()
    }

    pub fn sum1(
        &self, dtype: Kind
    ) -> Tensor {
        self.f_sum1(dtype).unwrap()
    }

    pub fn sum2(
        &self, dim: &[i64], keepdim: bool
    ) -> Tensor {
        self.f_sum2(dim, keepdim).unwrap()
    }

    pub fn sum3(
        &self, dim: &[i64], dtype: Kind
    ) -> Tensor {
        self.f_sum3(dim, dtype).unwrap()
    }

    pub fn sum4(
        &self, dim: &[i64], keepdim: bool, dtype: Kind
    ) -> Tensor {
        self.f_sum4(dim, keepdim, dtype).unwrap()
    }

    pub fn sum_out(
        &self, result: &Tensor, dim: &[i64], keepdim: bool
    ) -> Tensor {
        self.f_sum_out(result, dim, keepdim).unwrap()
    }

    pub fn sum_out1(
        &self, result: &Tensor, dim: &[i64], dtype: Kind
    ) -> Tensor {
        self.f_sum_out1(result, dim, dtype).unwrap()
    }

    pub fn sum_out2(
        &self, result: &Tensor, dim: &[i64], keepdim: bool, dtype: Kind
    ) -> Tensor {
        self.f_sum_out2(result, dim, keepdim, dtype).unwrap()
    }

    pub fn svd(
        &self, some: bool, compute_uv: bool
    ) -> (Tensor, Tensor, Tensor) {
        self.f_svd(some, compute_uv).unwrap()
    }

    pub fn svd_out(
        &self, u: &Tensor, s: &Tensor, v: &Tensor, some: bool, compute_uv: bool
    ) -> (Tensor, Tensor, Tensor) {
        self.f_svd_out(u, s, v, some, compute_uv).unwrap()
    }

    pub fn symeig(
        &self, eigenvectors: bool, upper: bool
    ) -> (Tensor, Tensor) {
        self.f_symeig(eigenvectors, upper).unwrap()
    }

    pub fn symeig_out(
        &self, e: &Tensor, v: &Tensor, eigenvectors: bool, upper: bool
    ) -> (Tensor, Tensor) {
        self.f_symeig_out(e, v, eigenvectors, upper).unwrap()
    }

    pub fn tr(
        &self, 
    ) -> Tensor {
        self.f_tr().unwrap()
    }

    pub fn t_(
        &self, 
    ) -> Tensor {
        self.f_t_().unwrap()
    }

    pub fn take(
        &self, index: &Tensor
    ) -> Tensor {
        self.f_take(index).unwrap()
    }

    pub fn take_out(
        &self, result: &Tensor, index: &Tensor
    ) -> Tensor {
        self.f_take_out(result, index).unwrap()
    }

    pub fn tan(
        &self, 
    ) -> Tensor {
        self.f_tan().unwrap()
    }

    pub fn tan_(
        &self, 
    ) -> Tensor {
        self.f_tan_().unwrap()
    }

    pub fn tan_out(
        &self, result: &Tensor
    ) -> Tensor {
        self.f_tan_out(result).unwrap()
    }

    pub fn tanh(
        &self, 
    ) -> Tensor {
        self.f_tanh().unwrap()
    }

    pub fn tanh_(
        &self, 
    ) -> Tensor {
        self.f_tanh_().unwrap()
    }

    pub fn tanh_out(
        &self, result: &Tensor
    ) -> Tensor {
        self.f_tanh_out(result).unwrap()
    }

    pub fn tensordot(
        &self, other: &Tensor, dims_self: &[i64], dims_other: &[i64]
    ) -> Tensor {
        self.f_tensordot(other, dims_self, dims_other).unwrap()
    }

    pub fn threshold<S: Into<Scalar>>(
        &self, threshold: S, value: S
    ) -> Tensor {
        self.f_threshold(threshold, value).unwrap()
    }

    pub fn threshold_<S: Into<Scalar>>(
        &self, threshold: S, value: S
    ) -> Tensor {
        self.f_threshold_(threshold, value).unwrap()
    }

    pub fn threshold_backward<S: Into<Scalar>>(
        &self, grad_output: &Tensor, threshold: S
    ) -> Tensor {
        self.f_threshold_backward(grad_output, threshold).unwrap()
    }

    pub fn threshold_out<S: Into<Scalar>>(
        &self, result: &Tensor, threshold: S, value: S
    ) -> Tensor {
        self.f_threshold_out(result, threshold, value).unwrap()
    }

    pub fn to(
        &self, device: Device
    ) -> Tensor {
        self.f_to(device).unwrap()
    }

    pub fn to1(
        &self, options: (Kind, Device), non_blocking: bool, copy: bool
    ) -> Tensor {
        self.f_to1(options, non_blocking, copy).unwrap()
    }

    pub fn to2(
        &self, dtype: Kind, non_blocking: bool, copy: bool
    ) -> Tensor {
        self.f_to2(dtype, non_blocking, copy).unwrap()
    }

    pub fn to3(
        &self, other: &Tensor, non_blocking: bool, copy: bool
    ) -> Tensor {
        self.f_to3(other, non_blocking, copy).unwrap()
    }

    pub fn to4(
        &self, device: Device, dtype: Kind, non_blocking: bool, copy: bool
    ) -> Tensor {
        self.f_to4(device, dtype, non_blocking, copy).unwrap()
    }

    pub fn to_dense(
        &self, 
    ) -> Tensor {
        self.f_to_dense().unwrap()
    }

    pub fn to_sparse(
        &self, 
    ) -> Tensor {
        self.f_to_sparse().unwrap()
    }

    pub fn to_sparse1(
        &self, sparse_dim: i64
    ) -> Tensor {
        self.f_to_sparse1(sparse_dim).unwrap()
    }

    pub fn topk(
        &self, k: i64, dim: i64, largest: bool, sorted: bool
    ) -> (Tensor, Tensor) {
        self.f_topk(k, dim, largest, sorted).unwrap()
    }

    pub fn topk_out(
        &self, values: &Tensor, indices: &Tensor, k: i64, dim: i64, largest: bool, sorted: bool
    ) -> (Tensor, Tensor) {
        self.f_topk_out(values, indices, k, dim, largest, sorted).unwrap()
    }

    pub fn totype(
        &self, scalar_type: Kind
    ) -> Tensor {
        self.f_totype(scalar_type).unwrap()
    }

    pub fn trace(
        &self, 
    ) -> Tensor {
        self.f_trace().unwrap()
    }

    pub fn transpose(
        &self, dim0: i64, dim1: i64
    ) -> Tensor {
        self.f_transpose(dim0, dim1).unwrap()
    }

    pub fn transpose_(
        &self, dim0: i64, dim1: i64
    ) -> Tensor {
        self.f_transpose_(dim0, dim1).unwrap()
    }

    pub fn tril(
        &self, diagonal: i64
    ) -> Tensor {
        self.f_tril(diagonal).unwrap()
    }

    pub fn tril_(
        &self, diagonal: i64
    ) -> Tensor {
        self.f_tril_(diagonal).unwrap()
    }

    pub fn tril_out(
        &self, result: &Tensor, diagonal: i64
    ) -> Tensor {
        self.f_tril_out(result, diagonal).unwrap()
    }

    pub fn triplet_margin_loss(
        anchor: &Tensor, positive: &Tensor, negative: &Tensor, margin: f64, p: f64, eps: f64, swap: bool, reduction: i64
    ) -> Tensor {
        Tensor::f_triplet_margin_loss(anchor, positive, negative, margin, p, eps, swap, reduction).unwrap()
    }

    pub fn triu(
        &self, diagonal: i64
    ) -> Tensor {
        self.f_triu(diagonal).unwrap()
    }

    pub fn triu_(
        &self, diagonal: i64
    ) -> Tensor {
        self.f_triu_(diagonal).unwrap()
    }

    pub fn triu_out(
        &self, result: &Tensor, diagonal: i64
    ) -> Tensor {
        self.f_triu_out(result, diagonal).unwrap()
    }

    pub fn trtrs(
        &self, a: &Tensor, upper: bool, transpose: bool, unitriangular: bool
    ) -> (Tensor, Tensor) {
        self.f_trtrs(a, upper, transpose, unitriangular).unwrap()
    }

    pub fn trtrs_out(
        &self, x: &Tensor, m: &Tensor, a: &Tensor, upper: bool, transpose: bool, unitriangular: bool
    ) -> (Tensor, Tensor) {
        self.f_trtrs_out(x, m, a, upper, transpose, unitriangular).unwrap()
    }

    pub fn trunc(
        &self, 
    ) -> Tensor {
        self.f_trunc().unwrap()
    }

    pub fn trunc_(
        &self, 
    ) -> Tensor {
        self.f_trunc_().unwrap()
    }

    pub fn trunc_out(
        &self, result: &Tensor
    ) -> Tensor {
        self.f_trunc_out(result).unwrap()
    }

    pub fn type_as(
        &self, other: &Tensor
    ) -> Tensor {
        self.f_type_as(other).unwrap()
    }

    pub fn unbind(
        &self, dim: i64
    ) -> Vec<Tensor> {
        self.f_unbind(dim).unwrap()
    }

    pub fn unfold(
        &self, dimension: i64, size: i64, step: i64
    ) -> Tensor {
        self.f_unfold(dimension, size, step).unwrap()
    }

    pub fn uniform_(
        &self, from: f64, to: f64
    ) -> Tensor {
        self.f_uniform_(from, to).unwrap()
    }

    pub fn unsqueeze(
        &self, dim: i64
    ) -> Tensor {
        self.f_unsqueeze(dim).unwrap()
    }

    pub fn unsqueeze_(
        &self, dim: i64
    ) -> Tensor {
        self.f_unsqueeze_(dim).unwrap()
    }

    pub fn upsample_bilinear2d(
        &self, output_size: &[i64], align_corners: bool
    ) -> Tensor {
        self.f_upsample_bilinear2d(output_size, align_corners).unwrap()
    }

    pub fn upsample_bilinear2d_backward(
        grad_output: &Tensor, output_size: &[i64], input_size: &[i64], align_corners: bool
    ) -> Tensor {
        Tensor::f_upsample_bilinear2d_backward(grad_output, output_size, input_size, align_corners).unwrap()
    }

    pub fn upsample_bilinear2d_backward_out(
        grad_input: &Tensor, grad_output: &Tensor, output_size: &[i64], input_size: &[i64], align_corners: bool
    ) -> Tensor {
        Tensor::f_upsample_bilinear2d_backward_out(grad_input, grad_output, output_size, input_size, align_corners).unwrap()
    }

    pub fn upsample_bilinear2d_out(
        &self, output: &Tensor, output_size: &[i64], align_corners: bool
    ) -> Tensor {
        self.f_upsample_bilinear2d_out(output, output_size, align_corners).unwrap()
    }

    pub fn upsample_linear1d(
        &self, output_size: &[i64], align_corners: bool
    ) -> Tensor {
        self.f_upsample_linear1d(output_size, align_corners).unwrap()
    }

    pub fn upsample_linear1d_backward(
        grad_output: &Tensor, output_size: &[i64], input_size: &[i64], align_corners: bool
    ) -> Tensor {
        Tensor::f_upsample_linear1d_backward(grad_output, output_size, input_size, align_corners).unwrap()
    }

    pub fn upsample_linear1d_backward_out(
        grad_input: &Tensor, grad_output: &Tensor, output_size: &[i64], input_size: &[i64], align_corners: bool
    ) -> Tensor {
        Tensor::f_upsample_linear1d_backward_out(grad_input, grad_output, output_size, input_size, align_corners).unwrap()
    }

    pub fn upsample_linear1d_out(
        &self, output: &Tensor, output_size: &[i64], align_corners: bool
    ) -> Tensor {
        self.f_upsample_linear1d_out(output, output_size, align_corners).unwrap()
    }

    pub fn upsample_nearest1d(
        &self, output_size: &[i64]
    ) -> Tensor {
        self.f_upsample_nearest1d(output_size).unwrap()
    }

    pub fn upsample_nearest1d_backward(
        grad_output: &Tensor, output_size: &[i64], input_size: &[i64]
    ) -> Tensor {
        Tensor::f_upsample_nearest1d_backward(grad_output, output_size, input_size).unwrap()
    }

    pub fn upsample_nearest1d_backward_out(
        grad_input: &Tensor, grad_output: &Tensor, output_size: &[i64], input_size: &[i64]
    ) -> Tensor {
        Tensor::f_upsample_nearest1d_backward_out(grad_input, grad_output, output_size, input_size).unwrap()
    }

    pub fn upsample_nearest1d_out(
        &self, output: &Tensor, output_size: &[i64]
    ) -> Tensor {
        self.f_upsample_nearest1d_out(output, output_size).unwrap()
    }

    pub fn upsample_nearest2d(
        &self, output_size: &[i64]
    ) -> Tensor {
        self.f_upsample_nearest2d(output_size).unwrap()
    }

    pub fn upsample_nearest2d_backward(
        grad_output: &Tensor, output_size: &[i64], input_size: &[i64]
    ) -> Tensor {
        Tensor::f_upsample_nearest2d_backward(grad_output, output_size, input_size).unwrap()
    }

    pub fn upsample_nearest2d_backward_out(
        grad_input: &Tensor, grad_output: &Tensor, output_size: &[i64], input_size: &[i64]
    ) -> Tensor {
        Tensor::f_upsample_nearest2d_backward_out(grad_input, grad_output, output_size, input_size).unwrap()
    }

    pub fn upsample_nearest2d_out(
        &self, output: &Tensor, output_size: &[i64]
    ) -> Tensor {
        self.f_upsample_nearest2d_out(output, output_size).unwrap()
    }

    pub fn upsample_nearest3d(
        &self, output_size: &[i64]
    ) -> Tensor {
        self.f_upsample_nearest3d(output_size).unwrap()
    }

    pub fn upsample_nearest3d_backward(
        grad_output: &Tensor, output_size: &[i64], input_size: &[i64]
    ) -> Tensor {
        Tensor::f_upsample_nearest3d_backward(grad_output, output_size, input_size).unwrap()
    }

    pub fn upsample_nearest3d_backward_out(
        grad_input: &Tensor, grad_output: &Tensor, output_size: &[i64], input_size: &[i64]
    ) -> Tensor {
        Tensor::f_upsample_nearest3d_backward_out(grad_input, grad_output, output_size, input_size).unwrap()
    }

    pub fn upsample_nearest3d_out(
        &self, output: &Tensor, output_size: &[i64]
    ) -> Tensor {
        self.f_upsample_nearest3d_out(output, output_size).unwrap()
    }

    pub fn upsample_trilinear3d(
        &self, output_size: &[i64], align_corners: bool
    ) -> Tensor {
        self.f_upsample_trilinear3d(output_size, align_corners).unwrap()
    }

    pub fn upsample_trilinear3d_backward(
        grad_output: &Tensor, output_size: &[i64], input_size: &[i64], align_corners: bool
    ) -> Tensor {
        Tensor::f_upsample_trilinear3d_backward(grad_output, output_size, input_size, align_corners).unwrap()
    }

    pub fn upsample_trilinear3d_backward_out(
        grad_input: &Tensor, grad_output: &Tensor, output_size: &[i64], input_size: &[i64], align_corners: bool
    ) -> Tensor {
        Tensor::f_upsample_trilinear3d_backward_out(grad_input, grad_output, output_size, input_size, align_corners).unwrap()
    }

    pub fn upsample_trilinear3d_out(
        &self, output: &Tensor, output_size: &[i64], align_corners: bool
    ) -> Tensor {
        self.f_upsample_trilinear3d_out(output, output_size, align_corners).unwrap()
    }

    pub fn values(
        &self, 
    ) -> Tensor {
        self.f_values().unwrap()
    }

    pub fn var(
        &self, unbiased: bool
    ) -> Tensor {
        self.f_var(unbiased).unwrap()
    }

    pub fn var1(
        &self, dim: i64, unbiased: bool, keepdim: bool
    ) -> Tensor {
        self.f_var1(dim, unbiased, keepdim).unwrap()
    }

    pub fn var_out(
        &self, result: &Tensor, dim: i64, unbiased: bool, keepdim: bool
    ) -> Tensor {
        self.f_var_out(result, dim, unbiased, keepdim).unwrap()
    }

    pub fn view(
        &self, size: &[i64]
    ) -> Tensor {
        self.f_view(size).unwrap()
    }

    pub fn view_as(
        &self, other: &Tensor
    ) -> Tensor {
        self.f_view_as(other).unwrap()
    }

    pub fn where_(
        &self, condition: &Tensor, other: &Tensor
    ) -> Tensor {
        self.f_where_(condition, other).unwrap()
    }

    pub fn zero_(
        &self, 
    ) -> Tensor {
        self.f_zero_().unwrap()
    }

    pub fn zeros(
        size: &[i64], options: (Kind, Device)
    ) -> Tensor {
        Tensor::f_zeros(size, options).unwrap()
    }

    pub fn zeros_like(
        &self, 
    ) -> Tensor {
        self.f_zeros_like().unwrap()
    }

    pub fn zeros_like1(
        &self, options: (Kind, Device)
    ) -> Tensor {
        self.f_zeros_like1(options).unwrap()
    }

    pub fn zeros_out(
        result: &Tensor, size: &[i64]
    ) -> Tensor {
        Tensor::f_zeros_out(result, size).unwrap()
    }
}
